# ç¬¬ä¸€é˜¶æ®µï¼šæœç´¢å·¥å…·é›†æˆ - å®æ–½æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›å°† Tavily æœç´¢å·¥å…·é›†æˆåˆ°æ·±åº¦åˆ†ææµç¨‹çš„åˆ†æ­¥å®æ–½æŒ‡å—ã€‚è¿™æ˜¯å¤šå·¥å…·é›†æˆæ–¹æ¡ˆçš„ç¬¬ä¸€é˜¶æ®µï¼Œé‡ç‚¹æ˜¯æ„å»º LangGraph åŸºç¡€æ¡†æ¶å¹¶æ·»åŠ æ–°é—»æœç´¢éªŒè¯èƒ½åŠ›ã€‚

**æ—¶é—´çº¿**: 1-2 å‘¨
**ç›®æ ‡**: æ­å»º LangGraph å­å›¾éª¨æ¶ï¼Œå®ç° Tavily æœç´¢å·¥å…·ç”¨äºæ–°é—»éªŒè¯
**çŠ¶æ€**: æœªå¼€å§‹

---

## ä¸ºä»€ä¹ˆé€‰æ‹© Tavily æœç´¢ï¼Ÿ

- **Google Custom Search** å…è´¹é…é¢ï¼šä»… 100 æ¬¡æŸ¥è¯¢/å¤©ï¼ˆç”Ÿäº§ç¯å¢ƒä¸å¤Ÿç”¨ï¼‰
- **Tavily ä¼˜åŠ¿**ï¼š
  - ä¸“ä¸º AI åº”ç”¨ä¼˜åŒ–ï¼Œè¿”å›ç»“æ„åŒ–å“åº”
  - å…è´¹å±‚ï¼š1,000 æ¬¡æŸ¥è¯¢/æœˆ
  - Pro å±‚ï¼š$20/æœˆ æ— é™é‡
  - è¿”å›æ ‡é¢˜ã€æ‘˜è¦ã€ç›¸å…³æ€§è¯„åˆ†å’Œæ¥æºå¯ä¿¡åº¦
  - å¹³å‡å»¶è¿Ÿï¼š1-2 ç§’
  - ç®€å• APIï¼Œå•æ¬¡è°ƒç”¨è¿”å›å¤šæºç»“æœ

---

## æ¶æ„å˜æ›´

### æ”¹åŠ¨èŒƒå›´

**éœ€ä¿®æ”¹çš„æ–‡ä»¶**ï¼š
- `src/ai/deep_analysis/gemini.py`: åœ¨ `analyse()` æ–¹æ³•ä¸­æ·»åŠ  LangGraph å­å›¾
- `src/ai/tools/` ç›®å½•ä¸‹çš„æ–°æ–‡ä»¶ï¼šå·¥å…·å®ç°

**ä¸æ”¹åŠ¨çš„æ–‡ä»¶**ï¼š
- ä¸»æµç¨‹ï¼ˆ`src/listener.py`, `src/pipeline/langgraph_pipeline.py`, `src/ai/signal_engine.py`ï¼‰
- æ‰€æœ‰ç°æœ‰æµç¨‹ä¿æŒä¸å˜

### è§¦å‘æ¡ä»¶

ä¸ç°æœ‰æ·±åº¦åˆ†æé€»è¾‘ç›¸åŒï¼š
- Gemini Flash åˆæ­¥åˆ†æ `confidence >= HIGH_VALUE_CONFIDENCE_THRESHOLD`ï¼ˆé»˜è®¤ 0.75ï¼‰
- æˆ– `event_type` å±äºé«˜ä»·å€¼ç±»å‹ï¼ˆdepegã€liquidationã€hackï¼‰
- æ’é™¤ä½ä»·å€¼ç±»å‹ï¼ˆmacroã€otherã€airdropã€governanceã€celebrityã€scam_alertï¼‰

### æµç¨‹å›¾

```
ç°æœ‰æµç¨‹ï¼ˆä¸å˜ï¼‰:
listener â†’ langgraph_pipeline â†’ _node_ai_signal â†’ AiSignalEngine.analyse()
                                                          â†“
                                              Gemini Flash åˆæ­¥åˆ†æ
                                                          â†“
                                    æ£€æŸ¥ is_high_value_signal() (signal_engine.py:528-540)
                                                          â†“
                                          [æ–°å¢] DeepAnalysisGraph å­å›¾
                                                          â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“                                                     â†“
        Context Gather (è®°å¿†) â†’ Tool Planner (AIå†³ç­–) â†’ Tool Executor (è°ƒAPI)
                    â†‘                                                     â†“
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ è·¯ç”±å™¨ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â†“ (æœ€å¤š 3 è½®)
                                    Synthesis (æœ€ç»ˆæ¨ç†)
                                          â†“
                                    è¾“å‡ºæœ€ç»ˆä¿¡å·
```

---

## LangGraph çŠ¶æ€è®¾è®¡

### çŠ¶æ€å¯¹è±¡ (DeepAnalysisState)

```python
from typing import TypedDict, Optional
from src.db.models import EventPayload
from src.ai.signal_engine import SignalResult

class DeepAnalysisState(TypedDict, total=False):
    # è¾“å…¥
    payload: EventPayload                # åŸå§‹æ¶ˆæ¯è½½è·
    preliminary: SignalResult             # Gemini Flash åˆæ­¥ç»“æœ

    # è¯æ®æ§½ä½ï¼ˆç¬¬ä¸€é˜¶æ®µï¼šä»…æœç´¢ + è®°å¿†ï¼‰
    search_evidence: Optional[dict]       # æœç´¢ç»“æœ
    memory_evidence: Optional[dict]       # å†å²ç›¸ä¼¼äº‹ä»¶

    # æ§åˆ¶æµ
    next_tools: list[str]                 # ["search"] æˆ– []
    tool_call_count: int                  # 0-3
    max_tool_calls: int                   # å›ºå®šä¸º 3

    # è¾“å‡º
    final_response: str                   # JSON å­—ç¬¦ä¸²ï¼ˆæœ€ç»ˆä¿¡å·ï¼‰
```

---

## âš ï¸ é‡è¦ä¿®æ”¹å»ºè®®ï¼ˆå®æ–½å‰å¿…è¯»ï¼‰

åŸºäºç°æœ‰ä»£ç å®¡æŸ¥ï¼Œä»¥ä¸‹ä¿®æ”¹å»ºè®®åº”åœ¨å®æ–½å‰çº³å…¥è€ƒè™‘ï¼š

### ğŸ”´ å¿…é¡»ä¿®æ”¹ï¼ˆé¿å…æŠ€æœ¯å€ºï¼‰

#### 1. è®°å¿†æ£€ç´¢é€»è¾‘é‡æ„

**é—®é¢˜**: `_node_context_gather` å®ç°ä¼šä¸ç°æœ‰ `_tool_fetch_memories` (gemini.py:122-193) é€»è¾‘é‡å¤

**è§£å†³æ–¹æ¡ˆ**: å°†è®°å¿†æ£€ç´¢é‡æ„ä¸ºç‹¬ç«‹çš„å¼‚æ­¥ Helper æ–¹æ³•

```python
async def _fetch_memory_entries(
    self,
    *,
    payload: "EventPayload",
    preliminary: "SignalResult",
    limit: int | None = None,
) -> list[dict]:
    """ç‹¬ç«‹çš„è®°å¿†æ£€ç´¢ Helperï¼Œåœ¨ä¸¤å¤„å¤ç”¨ï¼š
    1. _tool_fetch_memories (Function Calling å·¥å…·)
    2. _node_context_gather (LangGraph èŠ‚ç‚¹)
    """
    # æå–ç°æœ‰ _tool_fetch_memories çš„æ ¸å¿ƒé€»è¾‘
    # è¿”å›æ ¼å¼åŒ–çš„ prompt_entries
```

**å®æ–½ä½ç½®**: ä»»åŠ¡ 3.2 - Context Gather èŠ‚ç‚¹å®ç°æ—¶

---

#### 2. Tool Planner ä½¿ç”¨ Function Calling

**é—®é¢˜**: åŸæ–¹æ¡ˆç¬¬ 122 è¡Œæ˜ç¡®"ä¸ä½¿ç”¨ Function Calling,é‡‡ç”¨æ–‡æœ¬ JSON è¿”å›"ï¼Œå®¹æ˜“å‡ºç°è§£æå¤±è´¥

**è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ Gemini Function Calling å®šä¹‰ä¸“ç”¨å·¥å…·å†³ç­–å‡½æ•°

```python
# åœ¨ _build_tools() ä¸­æ·»åŠ 
{
    "name": "decide_next_tools",
    "description": "æ ¹æ®å·²æœ‰è¯æ®å†³å®šä¸‹ä¸€æ­¥éœ€è¦è°ƒç”¨çš„å·¥å…·",
    "parameters": {
        "type": "OBJECT",
        "properties": {
            "tools": {
                "type": "ARRAY",
                "items": {"type": "STRING"},
                "description": "éœ€è¦è°ƒç”¨çš„å·¥å…·åˆ—è¡¨,å¯é€‰å€¼: search"
            },
            "reason": {
                "type": "STRING",
                "description": "å†³ç­–ç†ç”±"
            }
        },
        "required": ["tools", "reason"]
    }
}
```

**ä¼˜åŠ¿**:
- ä¿è¯è¾“å‡ºç»“æ„ä¸€è‡´æ€§ï¼Œå‡å°‘ JSON è§£æå¤±è´¥é£é™©
- å¤ç”¨å·²éªŒè¯å¯é çš„ Gemini Function Calling èƒ½åŠ›
- ä¾¿äºåç»­æ‰©å±•å¤šå·¥å…·å†³ç­–ï¼ˆPhase 2ï¼‰

**å®æ–½ä½ç½®**: ä»»åŠ¡ 3.3 - Tool Planner èŠ‚ç‚¹å®ç°æ—¶

---

### ğŸŸ¡ å¼ºçƒˆå»ºè®®ï¼ˆæå‡è´¨é‡ï¼‰

#### 3. æœç´¢å…³é”®è¯ç”Ÿæˆä¼˜åŒ–

**é—®é¢˜**: å½“å‰è®¾è®¡ (ç¬¬ 749 è¡Œ) ç›´æ¥æ‹¼æ¥ `asset + event_type`ï¼Œå¯¹ä¸­æ–‡æ¶ˆæ¯ä¸å¤Ÿå‹å¥½

**æ”¹è¿›æ–¹æ¡ˆ**: æ·»åŠ è¯­è¨€æ£€æµ‹é€»è¾‘

```python
def _build_search_keyword(self, payload: EventPayload, preliminary: SignalResult) -> str:
    """æ ¹æ®æ¶ˆæ¯è¯­è¨€ç”Ÿæˆä¼˜åŒ–çš„æœç´¢å…³é”®è¯"""
    base = f"{preliminary.asset} {preliminary.event_type}"

    # ä¸­æ–‡ç¯å¢ƒæ·»åŠ ä¸­æ–‡å…³é”®è¯
    if payload.language in ["zh", "zh-CN", "zh-TW"]:
        event_cn_map = {
            "hack": "é»‘å®¢æ”»å‡»",
            "regulation": "ç›‘ç®¡æ”¿ç­–",
            "partnership": "åˆä½œä¼™ä¼´",
            "listing": "ä¸Šçº¿",
            "delisting": "ä¸‹æ¶",
            # ... å…¶ä»–æ˜ å°„
        }
        event_cn = event_cn_map.get(preliminary.event_type, preliminary.event_type)
        base = f"{preliminary.asset} {event_cn} æ–°é—»"

    # é«˜ä¼˜å…ˆçº§äº‹ä»¶æ·»åŠ  official å…³é”®è¯
    if preliminary.event_type in ["hack", "regulation", "partnership"]:
        base += " official statement" if payload.language == "en" else " å®˜æ–¹å£°æ˜"

    return base
```

**å®æ–½ä½ç½®**: ä»»åŠ¡ 3.4 - Tool Executor èŠ‚ç‚¹ä¸­çš„ `_execute_search_tool` æ–¹æ³•

---

#### 4. Synthesis Prompt é‡åŒ–è§„åˆ™

**é—®é¢˜**: åŸæ–¹æ¡ˆç¬¬ 991 è¡Œçš„ç½®ä¿¡åº¦è°ƒæ•´è§„åˆ™è¿‡äºç¬¼ç»Ÿï¼ˆ"æå‡ç½®ä¿¡åº¦"ã€"é™ä½ç½®ä¿¡åº¦"ï¼‰

**æ”¹è¿›æ–¹æ¡ˆ**: æ·»åŠ æ˜ç¡®çš„é‡åŒ–è§„åˆ™

```python
ã€ç½®ä¿¡åº¦è°ƒæ•´è§„åˆ™ã€‘
- åŸºå‡†: Gemini Flash åˆåˆ¤ç½®ä¿¡åº¦ = {preliminary.confidence}

- æœç´¢å¤šæºç¡®è®¤ (multi_source=true) AND å®˜æ–¹ç¡®è®¤ (official_confirmed=true):
  â†’ æå‡ +0.15 to +0.20

- æœç´¢å¤šæºç¡®è®¤ä½†æ— å®˜æ–¹ç¡®è®¤:
  â†’ æå‡ +0.05 to +0.10

- æœç´¢ç»“æœ < 3 æ¡æˆ–æ— å®˜æ–¹ç¡®è®¤:
  â†’ é™ä½ -0.10 to -0.20

- æœç´¢ç»“æœå†²çª (ä¸åŒæ¥æºè¯´æ³•çŸ›ç›¾):
  â†’ é™ä½ -0.20 å¹¶æ ‡è®° data_incomplete

- å†å²è®°å¿†å­˜åœ¨é«˜ç›¸ä¼¼åº¦æ¡ˆä¾‹ (similarity > 0.8):
  â†’ å‚è€ƒå†å²æ¡ˆä¾‹æœ€ç»ˆç½®ä¿¡åº¦,è°ƒæ•´ Â±0.10

ã€æœ€ç»ˆçº¦æŸã€‘
- ç½®ä¿¡åº¦èŒƒå›´: 0.0 - 1.0
- å¦‚æœæœ€ç»ˆç½®ä¿¡åº¦ < 0.4, å¿…é¡»æ·»åŠ  confidence_low é£é™©æ ‡å¿—
- åœ¨ notes ä¸­è¯´æ˜: "åˆåˆ¤ {preliminary.confidence:.2f} â†’ æœ€ç»ˆ {final_confidence:.2f}, ä¾æ®: [æœç´¢/è®°å¿†/å†²çª]"
```

**å®æ–½ä½ç½®**: ä»»åŠ¡ 3.5 - Synthesis èŠ‚ç‚¹çš„ `_build_synthesis_prompt` æ–¹æ³•

---

### ğŸŸ¢ å¯é€‰ä¼˜åŒ–ï¼ˆåç»­è¿­ä»£ï¼‰

#### 5. å·¥å…·è°ƒç”¨æ¯æ—¥é…é¢é™åˆ¶

æ·»åŠ æˆæœ¬æ§åˆ¶æœºåˆ¶:

```python
# åœ¨ __init__ ä¸­æ·»åŠ 
self._tool_call_daily_limit = getattr(config, "DEEP_ANALYSIS_TOOL_DAILY_LIMIT", 500)
self._tool_call_count_today = 0
self._tool_call_reset_date = datetime.now(timezone.utc).date()

# åœ¨ _node_tool_executor ä¸­æ·»åŠ æ£€æŸ¥
def _check_tool_quota(self) -> bool:
    today = datetime.now(timezone.utc).date()
    if today != self._tool_call_reset_date:
        self._tool_call_count_today = 0
        self._tool_call_reset_date = today

    if self._tool_call_count_today >= self._tool_call_daily_limit:
        logger.warning("âš ï¸ ä»Šæ—¥å·¥å…·è°ƒç”¨é…é¢å·²ç”¨å°½ (%d/%d)",
                      self._tool_call_count_today, self._tool_call_daily_limit)
        return False

    self._tool_call_count_today += 1
    return True
```

**å®æ–½ä½ç½®**: ä»»åŠ¡ 6.3 - æˆæœ¬æµ‹è¯•å®Œæˆåæ·»åŠ 

---

#### 6. å•å…ƒæµ‹è¯• Mock/é›†æˆæµ‹è¯•åˆ†ç¦»

**é—®é¢˜**: ç¬¬ 615 è¡Œ "æµ‹è¯•çœŸå® API è°ƒç”¨ (éœ€è¦ API Key)" ä¼šå¯¼è‡´ CI/CD ä¾èµ–å¤–éƒ¨æœåŠ¡

**æ”¹è¿›æ–¹æ¡ˆ**:
- ä¿ç•™ 1 ä¸ªçœŸå® API é›†æˆæµ‹è¯• (æ ‡è®° `@pytest.mark.integration`)
- æ·»åŠ å®Œæ•´çš„ Mock æµ‹è¯•è¦†ç›–æ‰€æœ‰åˆ†æ”¯

```python
@pytest.mark.integration
async def test_tavily_real_api(provider):
    """çœŸå® API é›†æˆæµ‹è¯•ï¼ˆéœ€è¦ TAVILY_API_KEYï¼‰"""
    result = await provider.search(keyword="Bitcoin", max_results=5)
    assert result.success is True

@pytest.mark.asyncio
async def test_tavily_mock_success(provider):
    """Mock æµ‹è¯• - æˆåŠŸåœºæ™¯"""
    with patch("httpx.AsyncClient.post", new_callable=AsyncMock) as mock_post:
        mock_post.return_value.status_code = 200
        mock_post.return_value.json.return_value = {"results": [...]}
        result = await provider.search(keyword="test", max_results=5)
        assert result.success is True
```

**å®æ–½ä½ç½®**: ä»»åŠ¡ 1.4 - å•å…ƒæµ‹è¯•å®ç°æ—¶

---

## å®æ–½ä»»åŠ¡

### ç¬¬ 1 å¤©ï¼šå·¥å…·åŸºç¡€æ¶æ„

#### ä»»åŠ¡ 1.1ï¼šåˆ›å»ºå·¥å…·ç›®å½•ç»“æ„

```bash
mkdir -p src/ai/tools/search/providers
touch src/ai/tools/__init__.py
touch src/ai/tools/base.py
touch src/ai/tools/search/__init__.py
touch src/ai/tools/search/fetcher.py
touch src/ai/tools/search/providers/__init__.py
touch src/ai/tools/search/providers/base.py
touch src/ai/tools/search/providers/tavily.py
touch src/ai/tools/exceptions.py
```

> **é¢„ç•™æ‰©å±•æ€§**ï¼š`providers/` ç›®å½•ç”¨äºå­˜æ”¾ä¸åŒæœç´¢ API çš„å®ç°ï¼ˆå¦‚ Tavilyã€Googleã€SerpAPI ç­‰ï¼‰ï¼Œ`fetcher.py` è´Ÿè´£æ ¹æ®é…ç½®é€‰æ‹©å…·ä½“ Providerã€‚å› æ­¤åç»­æ›´æ¢æˆ–æ–°å¢æœç´¢æœåŠ¡åªéœ€æ–°å¢ä¸€ä¸ª Provider ç±»å¹¶åœ¨å…¥å£æ³¨å†Œå³å¯ã€‚

#### ä»»åŠ¡ 1.2ï¼šå®ç°å·¥å…·åŸºç±»

**æ–‡ä»¶**: `src/ai/tools/base.py`

```python
from dataclasses import dataclass
from typing import Optional
from abc import ABC, abstractmethod
from datetime import datetime, timezone

@dataclass
class ToolResult:
    """æ ‡å‡†åŒ–å·¥å…·ç»“æœæ ¼å¼"""
    source: str              # å·¥å…·æ¥æºï¼ˆå¦‚ "Tavily"ï¼‰
    timestamp: str           # ISO 8601 æ—¶é—´æˆ³
    success: bool            # è°ƒç”¨æ˜¯å¦æˆåŠŸ
    data: dict              # ç»“æ„åŒ–æ•°æ®
    triggered: bool          # æ˜¯å¦è§¦å‘å¼‚å¸¸é˜ˆå€¼
    confidence: float        # ç»“æœå¯ä¿¡åº¦ï¼ˆ0.0-1.0ï¼‰
    error: Optional[str] = None  # é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœå¤±è´¥ï¼‰

    @staticmethod
    def _format_timestamp() -> str:
        """è¿”å›å½“å‰ UTC æ—¶é—´æˆ³ï¼ˆISO 8601 æ ¼å¼ï¼‰"""
        return datetime.now(timezone.utc).isoformat()

class BaseTool(ABC):
    """æ‰€æœ‰å·¥å…·çš„æŠ½è±¡åŸºç±»"""

    def __init__(self, config):
        self._config = config
        self._timeout = getattr(config, 'DEEP_ANALYSIS_TOOL_TIMEOUT', 10)

    @abstractmethod
    async def fetch(self, **kwargs) -> ToolResult:
        """ä»å·¥å…· API è·å–æ•°æ®"""
        pass

    def _handle_timeout(self, error: Exception) -> ToolResult:
        """æ ‡å‡†è¶…æ—¶é”™è¯¯å¤„ç†"""
        return ToolResult(
            source=self.__class__.__name__,
            timestamp=ToolResult._format_timestamp(),
            success=False,
            data={},
            triggered=False,
            confidence=0.0,
            error=f"timeout: {str(error)}"
        )
```

> `ToolResult` å°†åœ¨æ‰€æœ‰å¤–éƒ¨å·¥å…·ä¹‹é—´å¤ç”¨ï¼Œä¿è¯ LangGraph çŠ¶æ€æœºæ¥æ”¶ç»Ÿä¸€çš„è¯æ®ç»“æ„ï¼Œåç»­æ‰©å±•ä»·æ ¼/å®è§‚/é“¾ä¸Šå·¥å…·æ—¶æ— éœ€å†å®šä¹‰ä¸€æ¬¡ã€‚

**æ–‡ä»¶**: `src/ai/tools/search/providers/base.py`

```python
from __future__ import annotations

from abc import abstractmethod
from typing import Dict, Type

from ..base import BaseTool, ToolResult


class SearchProvider(BaseTool):
    """æœç´¢ API Provider æŠ½è±¡åŸºç±»ï¼Œç»§æ‰¿ BaseTool å¤ç”¨è¶…æ—¶å¤„ç†"""

    def __init__(self, config) -> None:
        super().__init__(config)

    @abstractmethod
    async def search(self, *, keyword: str, max_results: int) -> ToolResult:
        """æ‰§è¡Œæœç´¢å¹¶è¿”å›æ ‡å‡†åŒ–ç»“æœ"""


ProviderRegistry = Dict[str, Type['SearchProvider']]
```

> `ProviderRegistry` ä»…ä½œä¸ºç±»å‹æç¤ºï¼Œå®é™…æ³¨å†Œé€»è¾‘æ”¾åœ¨ `search/__init__.py` æˆ– `search/fetcher.py` ä¸­ï¼Œä¾¿äºæŒ‰å­—ç¬¦ä¸² key åŠ¨æ€åˆ›å»º Providerã€‚

**æ–‡ä»¶**: `src/ai/tools/exceptions.py`

```python
class ToolFetchError(Exception):
    """å·¥å…·è·å–é”™è¯¯çš„åŸºç±»"""
    pass

class ToolTimeoutError(ToolFetchError):
    """å·¥å…· API è¶…æ—¶"""
    pass

class ToolRateLimitError(ToolFetchError):
    """å·¥å…· API è¶…å‡ºé€Ÿç‡é™åˆ¶"""
    pass
```

#### ä»»åŠ¡ 1.3ï¼šå®ç° Tavily æœç´¢ Provider ä¸ Fetcherï¼ˆæ”¯æŒåç»­åˆ‡æ¢ï¼‰

**æ–‡ä»¶**: `src/ai/tools/search/providers/tavily.py`

```python
import httpx
from urllib.parse import urlparse

from ...base import ToolResult
from ...exceptions import ToolRateLimitError
from .base import SearchProvider


class TavilySearchProvider(SearchProvider):
    """Tavily æœç´¢ API å®ç°"""

    API_ENDPOINT = "https://api.tavily.com/search"

    def __init__(self, config) -> None:
        super().__init__(config)
        self._api_key = getattr(config, "TAVILY_API_KEY", None)
        self._multi_source_threshold = getattr(config, "SEARCH_MULTI_SOURCE_THRESHOLD", 3)
        self._include_domains = self._parse_domains(
            getattr(config, "SEARCH_INCLUDE_DOMAINS", "")
        )

        if not self._api_key:
            raise ValueError("TAVILY_API_KEY æœªé…ç½®")

    @staticmethod
    def _parse_domains(domains_str: str) -> list[str]:
        if not domains_str:
            return ["coindesk.com", "theblock.co", "cointelegraph.com"]
        return [domain.strip() for domain in domains_str.split(",") if domain.strip()]

    async def search(self, *, keyword: str, max_results: int) -> ToolResult:
        payload = {
            "api_key": self._api_key,
            "query": keyword,
            "max_results": max_results,
            "search_depth": "basic",
            "include_domains": self._include_domains,
            "include_answer": False,
        }

        try:
            async with httpx.AsyncClient(timeout=self._timeout) as client:
                response = await client.post(self.API_ENDPOINT, json=payload)

            if response.status_code == 429:
                raise ToolRateLimitError("Tavily API è¶…å‡ºé€Ÿç‡é™åˆ¶")

            response.raise_for_status()
            data = response.json()
        except httpx.TimeoutException as exc:
            return self._handle_timeout(exc)
        except ToolRateLimitError:
            return ToolResult(
                source="Tavily",
                timestamp=ToolResult._format_timestamp(),
                success=False,
                data={},
                triggered=False,
                confidence=0.0,
                error="rate_limit",
            )
        except Exception as exc:
            return ToolResult(
                source="Tavily",
                timestamp=ToolResult._format_timestamp(),
                success=False,
                data={},
                triggered=False,
                confidence=0.0,
                error=str(exc),
            )

        return self._parse_response(data, keyword)

    def _parse_response(self, data: dict, keyword: str) -> ToolResult:
        results = data.get("results", [])
        multi_source = len(results) >= self._multi_source_threshold
        official_confirmed = self._check_official_confirmation(results)
        sentiment = self._analyze_sentiment(results)

        formatted_results = [
            {
                "title": item.get("title", ""),
                "source": urlparse(item.get("url", "")).netloc,
                "url": item.get("url", ""),
                "score": item.get("score", 0.0),
            }
            for item in results
        ]

        tool_data = {
            "keyword": keyword,
            "results": formatted_results,
            "multi_source": multi_source,
            "official_confirmed": official_confirmed,
            "sentiment": sentiment,
            "source_count": len(results),
        }

        triggered = multi_source and official_confirmed
        confidence = self._calculate_confidence(results, multi_source, official_confirmed)

        return ToolResult(
            source="Tavily",
            timestamp=ToolResult._format_timestamp(),
            success=True,
            data=tool_data,
            triggered=triggered,
            confidence=confidence,
        )

    def _check_official_confirmation(self, results: list[dict]) -> bool:
        official_keywords = [
            "å®˜æ–¹", "å£°æ˜", "å…¬å‘Š", "official", "statement",
            "announcement", "confirmed", "press release",
        ]

        for item in results:
            title = item.get("title", "").lower()
            content = item.get("content", "").lower()
            if any(keyword in title or keyword in content for keyword in official_keywords):
                return True
        return False

    def _analyze_sentiment(self, results: list[dict]) -> dict:
        panic_keywords = ["æš´è·Œ", "å´©ç›˜", "ææ…Œ", "hack", "exploit", "crash", "dump"]
        neutral_keywords = ["è§‚å¯Ÿ", "ç­‰å¾…", "ç›‘æ§", "watch", "monitor", "observe"]
        optimistic_keywords = ["æ¢å¤", "ç¨³å®š", "åå¼¹", "recovery", "stable", "bounce"]

        panic = neutral = optimistic = 0
        for item in results:
            text = (item.get("title", "") + " " + item.get("content", "")).lower()
            if any(word in text for word in panic_keywords):
                panic += 1
            if any(word in text for word in neutral_keywords):
                neutral += 1
            if any(word in text for word in optimistic_keywords):
                optimistic += 1

        total = panic + neutral + optimistic
        if total == 0:
            return {"panic": 0.33, "neutral": 0.34, "optimistic": 0.33}

        return {
            "panic": round(panic / total, 2),
            "neutral": round(neutral / total, 2),
            "optimistic": round(optimistic / total, 2),
        }

    def _calculate_confidence(
        self,
        results: list[dict],
        multi_source: bool,
        official_confirmed: bool,
    ) -> float:
        if not results:
            return 0.0

        avg_score = sum(item.get("score", 0.0) for item in results) / len(results)
        confidence = avg_score
        if multi_source:
            confidence = min(1.0, confidence + 0.1)
        if official_confirmed:
            confidence = min(1.0, confidence + 0.15)
        return round(confidence, 2)
```

**æ–‡ä»¶**: `src/ai/tools/search/__init__.py`

```python
from __future__ import annotations

from typing import Type

from .providers.base import ProviderRegistry, SearchProvider
from .providers.tavily import TavilySearchProvider

REGISTRY: ProviderRegistry = {
    "tavily": TavilySearchProvider,
}


def create_search_provider(config) -> SearchProvider:
    provider_key = getattr(config, "DEEP_ANALYSIS_SEARCH_PROVIDER", "tavily").lower()
    provider_cls: Type[SearchProvider] | None = REGISTRY.get(provider_key)

    if provider_cls is None:
        raise ValueError(f"æœªçŸ¥æœç´¢ Provider: {provider_key}")

    return provider_cls(config)


__all__ = [
    "create_search_provider",
    "SearchProvider",
]
```

**æ–‡ä»¶**: `src/ai/tools/search/fetcher.py`

```python
from __future__ import annotations

from typing import Optional

from ..base import ToolResult
from . import create_search_provider


class SearchTool:
    """å°è£…æœç´¢ Providerï¼Œæ”¯æŒæœªæ¥çƒ­æ’æ‹”"""

    def __init__(self, config) -> None:
        self._config = config
        self._provider = create_search_provider(config)
        self._max_results = getattr(config, "SEARCH_MAX_RESULTS", 5)

    async def fetch(self, *, keyword: str, max_results: Optional[int] = None) -> ToolResult:
        target = max_results or self._max_results
        return await self._provider.search(keyword=keyword, max_results=target)

    def refresh_provider(self) -> None:
        """å…è®¸åœ¨è¿è¡Œæ—¶æ›´æ–°é…ç½®åé‡æ–°åŠ è½½ Provider"""
        self._provider = create_search_provider(self._config)
```

> `SearchTool` ä½œä¸º LangGraph èŠ‚ç‚¹ä½¿ç”¨çš„ç»Ÿä¸€å…¥å£ï¼Œå†…éƒ¨å¯æŒ‰éœ€æ›´æ¢ Providerï¼›`refresh_provider()` ä¸ºåç»­åŠ¨æ€åˆ‡æ¢ï¼ˆå¦‚çƒ­æ›´æ–°é…ç½®ï¼‰é¢„ç•™æ‰©å±•ç‚¹ã€‚

**æ–‡ä»¶**: `src/ai/tools/__init__.py`

```python
from .base import BaseTool, ToolResult
from .exceptions import ToolFetchError, ToolTimeoutError, ToolRateLimitError
from .search.fetcher import SearchTool

__all__ = [
    "BaseTool",
    "ToolResult",
    "SearchTool",
    "ToolFetchError",
    "ToolTimeoutError",
    "ToolRateLimitError",
]
```

#### ä»»åŠ¡ 1.4ï¼šå•å…ƒæµ‹è¯•

**æ–‡ä»¶**: `tests/ai/tools/test_search_fetcher.py`

```python
import httpx
import pytest
from unittest.mock import AsyncMock, Mock, patch

from src.ai.tools.search.fetcher import SearchTool
from src.ai.tools.search.providers.tavily import TavilySearchProvider


@pytest.fixture
def mock_config():
    config = Mock()
    config.TAVILY_API_KEY = "test-api-key"
    config.SEARCH_MAX_RESULTS = 5
    config.SEARCH_MULTI_SOURCE_THRESHOLD = 3
    config.SEARCH_INCLUDE_DOMAINS = "coindesk.com,theblock.co"
    config.DEEP_ANALYSIS_TOOL_TIMEOUT = 10
    config.DEEP_ANALYSIS_SEARCH_PROVIDER = "tavily"
    return config


@pytest.fixture
def provider(mock_config):
    return TavilySearchProvider(mock_config)


@pytest.fixture
def search_tool(mock_config):
    return SearchTool(mock_config)


@pytest.mark.asyncio
async def test_successful_search(provider):
    mock_response = {
        "results": [
            {
                "title": "USDC è„±é”šè‡³ $0.98",
                "url": "https://coindesk.com/test",
                "content": "Circle å®˜æ–¹å£°æ˜ç¡®è®¤...",
                "score": 0.95,
            },
            {
                "title": "å¸‚åœºææ…Œï¼ŒUSDC å¤±å»é”šå®š",
                "url": "https://theblock.co/test",
                "content": "æš´è·ŒæŒç»­...",
                "score": 0.88,
            },
        ]
    }

    with patch("httpx.AsyncClient.post", new_callable=AsyncMock) as mock_post:
        mock_post.return_value.status_code = 200
        mock_post.return_value.json.return_value = mock_response

        result = await provider.search(keyword="USDC depeg", max_results=5)

    assert result.success is True
    assert result.source == "Tavily"
    assert result.data["keyword"] == "USDC depeg"
    assert result.data["source_count"] == 2
    assert result.data["official_confirmed"] is True
    assert result.confidence > 0.8


@pytest.mark.asyncio
async def test_rate_limit_error(provider):
    with patch("httpx.AsyncClient.post", new_callable=AsyncMock) as mock_post:
        mock_post.return_value.status_code = 429

        result = await provider.search(keyword="test query", max_results=5)

    assert result.success is False
    assert result.error == "rate_limit"


@pytest.mark.asyncio
async def test_timeout_error(provider):
    with patch("httpx.AsyncClient.post", new_callable=AsyncMock) as mock_post:
        mock_post.side_effect = httpx.TimeoutException("è¿æ¥è¶…æ—¶")

        result = await provider.search(keyword="test query", max_results=5)

    assert result.success is False
    assert "timeout" in result.error


@pytest.mark.asyncio
async def test_multi_source_detection(provider):
    mock_response = {
        "results": [
            {
                "title": f"æ–°é—» {i}",
                "url": f"https://source{i}.com/test",
                "score": 0.9,
            }
            for i in range(5)
        ]
    }

    with patch("httpx.AsyncClient.post", new_callable=AsyncMock) as mock_post:
        mock_post.return_value.status_code = 200
        mock_post.return_value.json.return_value = mock_response

        result = await provider.search(keyword="test", max_results=5)

    assert result.data["multi_source"] is True


@pytest.mark.asyncio
async def test_official_confirmation_detection(provider):
    mock_response = {
        "results": [
            {
                "title": "Circle å®˜æ–¹å…¬å‘Š",
                "url": "https://coindesk.com/test",
                "content": "Circle å®˜æ–¹å£°æ˜...",
                "score": 0.95,
            }
        ]
    }

    with patch("httpx.AsyncClient.post", new_callable=AsyncMock) as mock_post:
        mock_post.return_value.status_code = 200
        mock_post.return_value.json.return_value = mock_response

        result = await provider.search(keyword="test", max_results=5)

    assert result.data["official_confirmed"] is True


@pytest.mark.asyncio
async def test_search_tool_respects_max_results(search_tool):
    with patch("httpx.AsyncClient.post", new_callable=AsyncMock) as mock_post:
        mock_post.return_value.status_code = 200
        mock_post.return_value.json.return_value = {"results": []}

        await search_tool.fetch(keyword="anything")

    mock_post.assert_awaited()
    _, kwargs = mock_post.call_args
    assert kwargs["json"]["max_results"] == 5
```

---

### ç¬¬ 2 å¤©ï¼šLangGraph çŠ¶æ€å¯¹è±¡ä¸èŠ‚ç‚¹éª¨æ¶

#### ä»»åŠ¡ 2.1ï¼šå®šä¹‰çŠ¶æ€å¯¹è±¡

åœ¨ `src/ai/deep_analysis/gemini.py` æ–‡ä»¶é¡¶éƒ¨æ·»åŠ ï¼š

```python
from typing import TypedDict, Optional

class DeepAnalysisState(TypedDict, total=False):
    # è¾“å…¥
    payload: 'EventPayload'
    preliminary: 'SignalResult'

    # è¯æ®æ§½ä½ï¼ˆç¬¬ä¸€é˜¶æ®µï¼šä»…æœç´¢ + è®°å¿†ï¼‰
    search_evidence: Optional[dict]
    memory_evidence: Optional[dict]

    # æ§åˆ¶æµ
    next_tools: list[str]        # ["search"] æˆ– []
    tool_call_count: int         # 0-3
    max_tool_calls: int          # å›ºå®šä¸º 3

    # è¾“å‡º
    final_response: str          # JSON å­—ç¬¦ä¸²
```

#### ä»»åŠ¡ 2.2ï¼šå®ç°èŠ‚ç‚¹æ–¹æ³•éª¨æ¶

æ·»åŠ åˆ° `GeminiDeepAnalysisEngine` ç±»ï¼š

```python
async def _node_context_gather(self, state: DeepAnalysisState) -> dict:
    """èŠ‚ç‚¹ 1ï¼šæ”¶é›†å†å²è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆå¼‚æ­¥ï¼‰"""
    # TODO: åœ¨ç¬¬ 3 å¤©å®ç°
    return {}


async def _node_tool_planner(self, state: DeepAnalysisState) -> dict:
    """èŠ‚ç‚¹ 2ï¼šAI å†³å®šæ¥ä¸‹æ¥è°ƒç”¨å“ªäº›å·¥å…·ï¼ˆå¼‚æ­¥ï¼‰"""
    # TODO: åœ¨ç¬¬ 3 å¤©å®ç°
    return {}


async def _node_tool_executor(self, state: DeepAnalysisState) -> dict:
    """èŠ‚ç‚¹ 3ï¼šæ‰§è¡Œ planner å†³å®šçš„å·¥å…·ï¼ˆå¼‚æ­¥ï¼Œç¬¬ä¸€é˜¶æ®µä»…æœç´¢ï¼‰"""
    # TODO: åœ¨ç¬¬ 3 å¤©å®ç°
    return {}


async def _node_synthesis(self, state: DeepAnalysisState) -> dict:
    """èŠ‚ç‚¹ 4ï¼šç»¼åˆæ‰€æœ‰è¯æ®ç”Ÿæˆæœ€ç»ˆä¿¡å·ï¼ˆå¼‚æ­¥ï¼‰"""
    # TODO: åœ¨ç¬¬ 3 å¤©å®ç°
    return {}
```

#### ä»»åŠ¡ 2.3ï¼šå®ç°è·¯ç”±å™¨æ–¹æ³•

```python
def _route_after_planner(self, state: DeepAnalysisState) -> str:
    """
    Tool Planner ä¹‹åçš„è·¯ç”±ï¼š
    - å¦‚æœ next_tools ä¸ºç©º â†’ "synthesis"
    - å¦åˆ™ â†’ "executor"
    """
    if not state.get("next_tools"):
        return "synthesis"
    return "executor"

def _route_after_executor(self, state: DeepAnalysisState) -> str:
    """
    Tool Executor ä¹‹åçš„è·¯ç”±ï¼š
    - å¦‚æœ tool_call_count >= max_tool_calls â†’ "synthesis"
    - å¦åˆ™ â†’ "planner"ï¼ˆä¸‹ä¸€è½®ï¼‰
    """
    if state["tool_call_count"] >= state["max_tool_calls"]:
        logger.info("è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•° (3)ï¼Œè¿›å…¥æœ€ç»ˆæ¨ç†")
        return "synthesis"
    return "planner"
```

---

### ç¬¬ 3-4 å¤©ï¼šå®ç° LangGraph å­å›¾

#### ä»»åŠ¡ 3.1ï¼šæ„å»ºå›¾ç»“æ„

æ·»åŠ åˆ° `GeminiDeepAnalysisEngine`ï¼š

```python
def _build_deep_graph(self):
    """æ„å»ºç”¨äºå·¥å…·å¢å¼ºæ·±åº¦åˆ†æçš„ LangGraph"""
    from langgraph.graph import StateGraph, END

    graph = StateGraph(DeepAnalysisState)

    # æ·»åŠ èŠ‚ç‚¹
    graph.add_node("context_gather", self._node_context_gather)
    graph.add_node("planner", self._node_tool_planner)
    graph.add_node("executor", self._node_tool_executor)
    graph.add_node("synthesis", self._node_synthesis)

    # å®šä¹‰è¾¹
    graph.set_entry_point("context_gather")
    graph.add_edge("context_gather", "planner")

    # æ¡ä»¶è·¯ç”±
    graph.add_conditional_edges(
        "planner",
        self._route_after_planner,
        {
            "executor": "executor",
            "synthesis": "synthesis"
        }
    )

    graph.add_conditional_edges(
        "executor",
        self._route_after_executor,
        {
            "planner": "planner",
            "synthesis": "synthesis"
        }
    )

    graph.add_edge("synthesis", END)

    return graph.compile()
```

#### ä»»åŠ¡ 3.2ï¼šå®ç° Context Gather èŠ‚ç‚¹

```python
async def _node_context_gather(self, state: DeepAnalysisState) -> dict:
    """æ”¶é›†å†å²è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆå¼‚æ­¥ Helper ç‰ˆæœ¬ï¼‰"""
    logger.info("ğŸ§  Context Gather: è·å–å†å²è®°å¿†")

    entries = await self._fetch_memory_entries(
        payload=state["payload"],
        preliminary=state["preliminary"],
    )

    memory_text = self._format_memory_evidence(entries)
    logger.info("ğŸ§  Context Gather: æ‰¾åˆ° %d æ¡å†å²äº‹ä»¶", len(entries))

    return {
        "memory_evidence": {
            "entries": entries,
            "formatted": memory_text,
            "count": len(entries),
        }
    }


def _format_memory_evidence(self, entries: list) -> str:
    """æ ¼å¼åŒ–è®°å¿†æ¡ç›®ä¾› AI ä½¿ç”¨"""
    if not entries:
        return "æ— å†å²ç›¸ä¼¼äº‹ä»¶"

    lines = []
    for i, entry in enumerate(entries, 1):
        confidence = getattr(entry, 'confidence', 'N/A')
        similarity = getattr(entry, 'similarity', 'N/A')
        summary = getattr(entry, 'summary', 'N/A')
        lines.append(f"{i}. {summary} (ç½®ä¿¡åº¦: {confidence}, ç›¸ä¼¼åº¦: {similarity})")

    return "\n".join(lines)


async def _fetch_memory_entries(
    self,
    *,
    payload: "EventPayload",
    preliminary: "SignalResult",
    limit: int | None = None,
) -> list[dict]:
    """ç‹¬ç«‹çš„è®°å¿†æ£€ç´¢ Helperï¼Œå¤ç”¨ç°æœ‰ä»“å‚¨é€»è¾‘"""

    if not self._memory or not self._memory.enabled:
        return []

    limit = limit or self._memory_limit
    keywords = list(payload.keywords_hit or [])
    asset_codes = _normalise_asset_codes(preliminary.asset)

    repo = self._memory.repository
    if repo is None:
        return []

    entries: list = []

    if hasattr(repo, "fetch_memories") and inspect.iscoroutinefunction(repo.fetch_memories):
        entries = await repo.fetch_memories(
            embedding=None,
            asset_codes=asset_codes,
            keywords=keywords,
        )
    elif hasattr(repo, "fetch_memories"):
        result = repo.fetch_memories(
            embedding=None,
            asset_codes=asset_codes,
            keywords=keywords,
        )
        if inspect.isawaitable(result):
            result = await result
        if isinstance(result, MemoryContext):
            entries = list(result.entries)
        elif isinstance(result, Iterable):
            entries = list(result)
    elif hasattr(repo, "load_entries"):
        entries = repo.load_entries(
            keywords=keywords,
            limit=limit,
            min_confidence=self._memory_min_confidence,
        )

    prompt_entries = _memory_entries_to_prompt(entries)[:limit] if entries else []
    return prompt_entries
```

#### ä»»åŠ¡ 3.3ï¼šå®ç° Tool Planner èŠ‚ç‚¹

```python
async def _node_tool_planner(self, state: DeepAnalysisState) -> dict:
    """AI å†³å®šæ˜¯å¦è°ƒç”¨æœç´¢å·¥å…·ï¼ˆå¼‚æ­¥ï¼‰"""
    logger.info("ğŸ¤– Tool Planner: å†³ç­–ä¸‹ä¸€æ­¥å·¥å…·")

    prompt = self._build_planner_prompt(state)
    decision_text = await self._invoke_text_model(prompt)

    import json
    try:
        decision = json.loads(decision_text)
        tools = decision.get("tools", [])
        reason = decision.get("reason", "")
        logger.info("ğŸ¤– Tool Planner å†³ç­–: %s, ç†ç”±: %s", tools, reason)
        return {"next_tools": tools}
    except json.JSONDecodeError:
        logger.warning("æ— æ³•è§£æ planner å†³ç­–: %s", decision_text)
        return {"next_tools": []}

def _build_planner_prompt(self, state: DeepAnalysisState) -> str:
    """æ„å»ºå·¥å…·è§„åˆ’ prompt"""
    payload = state["payload"]
    preliminary = state["preliminary"]
    memory_ev = state.get("memory_evidence", {})
    search_ev = state.get("search_evidence", {})

    return f"""ä½ æ˜¯å·¥å…·è°ƒåº¦ä¸“å®¶,åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢æ–°é—»éªŒè¯ã€‚

ã€æ¶ˆæ¯å†…å®¹ã€‘{payload.text}
ã€äº‹ä»¶ç±»å‹ã€‘{preliminary.event_type}
ã€èµ„äº§ã€‘{preliminary.asset}
ã€åˆæ­¥ç½®ä¿¡åº¦ã€‘{preliminary.confidence}

ã€å·²æœ‰è¯æ®ã€‘
- å†å²è®°å¿†: {memory_ev.get('formatted', 'æ— ')}
- æœç´¢ç»“æœ: {self._format_search_evidence(search_ev)}

ã€å†³ç­–è§„åˆ™ã€‘
1. å¦‚æœäº‹ä»¶ç±»å‹æ˜¯ hack/regulation/partnership/celebrity â†’ éœ€è¦æœç´¢éªŒè¯
2. å¦‚æœå·²æœ‰æœç´¢ç»“æœä¸” multi_source=true â†’ è¯æ®å……åˆ†,æ— éœ€å†æœç´¢
3. å¦‚æœ tool_call_count >= 2 â†’ è¯æ®å……åˆ†,æ— éœ€å†æœç´¢
4. å¦‚æœæ˜¯æ•°å€¼ç±»äº‹ä»¶ (depeg/liquidation) â†’ æš‚ä¸éœ€è¦æœç´¢ï¼ˆç¬¬ä¸€é˜¶æ®µé™åˆ¶ï¼‰

ã€å½“å‰çŠ¶æ€ã€‘
- å·²è°ƒç”¨å·¥å…·æ¬¡æ•°: {state['tool_call_count']}
- æœ€å¤§è°ƒç”¨æ¬¡æ•°: {state['max_tool_calls']}

è¿”å› JSON:
- éœ€è¦æœç´¢: {{"tools": ["search"], "reason": "ä¼ é—»ç±»äº‹ä»¶éœ€å¤šæºéªŒè¯"}}
- æ— éœ€æœç´¢: {{"tools": [], "reason": "å·²æœ‰å……åˆ†è¯æ®"}}

åªè¿”å› JSON,ä¸è¦å…¶ä»–æ–‡å­—ã€‚"""


async def _invoke_text_model(self, prompt: str) -> str:
    """ç»Ÿä¸€çš„æ–‡æœ¬ç”Ÿæˆè°ƒç”¨ï¼Œå¤ç”¨ Function Calling å®¢æˆ·ç«¯"""
    messages = [{"role": "user", "content": prompt}]
    response = await self._client.generate_content_with_tools(messages, tools=None)

    if not response or not response.text:
        raise DeepAnalysisError("Gemini è¿”å›ç©ºå“åº”")

    return response.text.strip()

def _format_search_evidence(self, search_ev: dict) -> str:
    """æ ¼å¼åŒ–æœç´¢è¯æ®ç”¨äºæ˜¾ç¤º"""
    if not search_ev:
        return "æ— "

    data = search_ev.get("data", {})
    return f"æ‰¾åˆ° {data.get('source_count', 0)} æ¡ç»“æœ, å¤šæºç¡®è®¤={data.get('multi_source', False)}, å®˜æ–¹ç¡®è®¤={data.get('official_confirmed', False)}"
```

#### ä»»åŠ¡ 3.4ï¼šå®ç° Tool Executor èŠ‚ç‚¹

```python
async def _node_tool_executor(self, state: DeepAnalysisState) -> dict:
    """æ‰§è¡Œ planner å†³å®šçš„å·¥å…·ï¼ˆå¼‚æ­¥ï¼Œç¬¬ä¸€é˜¶æ®µä»…æœç´¢ï¼‰"""
    tools_to_call = state.get("next_tools", [])
    logger.info("ğŸ”§ Tool Executor: è°ƒç”¨å·¥å…·: %s", tools_to_call)

    updates: dict = {"tool_call_count": state["tool_call_count"] + 1}

    for tool_name in tools_to_call:
        if tool_name != "search":
            logger.warning("æœªçŸ¥å·¥å…·: %s", tool_name)
            continue

        if not self._search_tool:
            logger.warning("æœç´¢å·¥å…·æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ‰§è¡Œ")
            continue

        result = await self._execute_search_tool(state)
        if result:
            updates["search_evidence"] = result

    return updates


async def _execute_search_tool(self, state: DeepAnalysisState) -> Optional[dict]:
    """æ‰§è¡Œ SearchTool å¹¶è½¬æ¢ä¸º LangGraph çŠ¶æ€æ ¼å¼"""
    preliminary = state["preliminary"]

    keyword = f"{preliminary.asset} {preliminary.event_type}"
    if preliminary.event_type in ["hack", "regulation"]:
        keyword += " news official"

    logger.info("ğŸ”§ è°ƒç”¨æœç´¢å·¥å…·: keyword='%s'", keyword)

    try:
        result = await self._search_tool.fetch(keyword=keyword, max_results=5)
    except Exception as exc:  # pragma: no cover - defensive
        logger.error("æœç´¢å·¥å…·æ‰§è¡Œå¤±è´¥: %s", exc)
        return None

    if not result.success:
        logger.warning("ğŸ”§ æœç´¢å·¥å…·è°ƒç”¨å¤±è´¥: %s", result.error)
        return None

    logger.info(
        "ğŸ”§ æœç´¢è¿”å› %d æ¡ç»“æœ (multi_source=%s, official=%s)",
        result.data.get("source_count", 0),
        result.data.get("multi_source"),
        result.data.get("official_confirmed"),
    )

    return {
        "success": True,
        "data": result.data,
        "triggered": result.triggered,
        "confidence": result.confidence,
    }
```

#### ä»»åŠ¡ 3.5ï¼šå®ç° Synthesis èŠ‚ç‚¹

```python
async def _node_synthesis(self, state: DeepAnalysisState) -> dict:
    """ç»¼åˆæ‰€æœ‰è¯æ®ç”Ÿæˆæœ€ç»ˆä¿¡å·ï¼ˆå¼‚æ­¥ï¼‰"""
    logger.info("ğŸ“Š Synthesis: ç”Ÿæˆæœ€ç»ˆåˆ†æ")

    prompt = self._build_synthesis_prompt(state)
    final_json = await self._invoke_text_model(prompt)

    try:
        import json
        parsed = json.loads(final_json)
        final_conf = parsed.get("confidence", 0.0)
        prelim_conf = state["preliminary"].confidence
        logger.info("ğŸ“Š Synthesis: æœ€ç»ˆç½®ä¿¡åº¦ %.2f (åˆæ­¥ %.2f)", final_conf, prelim_conf)
    except Exception:  # pragma: no cover - å®¹å¿è§£æå¤±è´¥
        logger.warning("ğŸ“Š Synthesis: æ— æ³•è§£ææœ€ç»ˆ JSON")

    return {"final_response": final_json}

def _build_synthesis_prompt(self, state: DeepAnalysisState) -> str:
    """æ„å»ºæœ€ç»ˆç»¼åˆæ¨ç† prompt"""
    payload = state["payload"]
    preliminary = state["preliminary"]
    memory_ev = state.get("memory_evidence", {})
    search_ev = state.get("search_evidence", {})

    return f"""ä½ æ˜¯åŠ å¯†äº¤æ˜“å°èµ„æ·±åˆ†æå¸ˆ,å·²æŒæ¡å®Œæ•´è¯æ®,è¯·ç»™å‡ºæœ€ç»ˆåˆ¤æ–­ã€‚

ã€åŸå§‹æ¶ˆæ¯ã€‘
{payload.text}

ã€Gemini Flash åˆæ­¥åˆ¤æ–­ã€‘
- äº‹ä»¶ç±»å‹: {preliminary.event_type}
- èµ„äº§: {preliminary.asset}
- æ“ä½œ: {preliminary.action}
- ç½®ä¿¡åº¦: {preliminary.confidence}
- æ‘˜è¦: {preliminary.summary}

ã€å†å²è®°å¿†ã€‘
{memory_ev.get('formatted', 'æ— å†å²ç›¸ä¼¼äº‹ä»¶')}

ã€æœç´¢éªŒè¯ã€‘
{self._format_search_detail(search_ev)}

è¯·ç»¼åˆåˆ¤æ–­:
1. æœç´¢ç»“æœæ˜¯å¦ç¡®è®¤äº‹ä»¶çœŸå®æ€§ï¼ˆmulti_source + official_confirmedï¼‰
2. ç»“åˆå†å²æ¡ˆä¾‹è°ƒæ•´ç½®ä¿¡åº¦
3. å¦‚æœæœç´¢ç»“æœå†²çªæˆ–ä¸è¶³,é™ä½ç½®ä¿¡åº¦å¹¶æ ‡è®° data_incomplete

è¿”å› JSONï¼ˆä¸ SignalResult æ ¼å¼ä¸€è‡´ï¼‰:
{{
  "summary": "ä¸­æ–‡æ‘˜è¦",
  "event_type": "{preliminary.event_type}",
  "asset": "{preliminary.asset}",
  "asset_name": "{getattr(preliminary, 'asset_name', '')}",
  "action": "buy|sell|observe",
  "direction": "long|short|neutral",
  "confidence": 0.0-1.0,
  "strength": "low|medium|high",
  "timeframe": "short|medium|long",
  "risk_flags": [],
  "notes": "æ¨ç†ä¾æ®,å¼•ç”¨æœç´¢æ¥æºå’Œå…³é”®è¯æ®",
  "links": []
}}

ã€å…³é”®è¦æ±‚ã€‘
- æœç´¢å¤šæºç¡®è®¤ + å®˜æ–¹ç¡®è®¤ â†’ æå‡ç½®ä¿¡åº¦ (+0.1 to +0.2)
- æœç´¢ç»“æœå†²çªæˆ–æ— å®˜æ–¹ç¡®è®¤ â†’ é™ä½ç½®ä¿¡åº¦ (-0.1 to -0.2)
- è¯æ®ä¸è¶³ â†’ æ ‡è®° data_incomplete é£é™©
- åœ¨ notes ä¸­æ˜ç¡®è¯´æ˜ä½¿ç”¨äº†å“ªäº›è¯æ®åŠå…³é”®å‘ç°

åªè¿”å› JSON,ä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

def _format_search_detail(self, search_ev: dict) -> str:
    """è¯¦ç»†æ ¼å¼åŒ–æœç´¢è¯æ®"""
    if not search_ev or not search_ev.get("success"):
        return "æ— æœç´¢ç»“æœæˆ–æœç´¢å¤±è´¥"

    data = search_ev.get("data", {})
    results = data.get("results", [])

    lines = [
        f"å…³é”®è¯: {data.get('keyword', 'N/A')}",
        f"ç»“æœæ•°: {data.get('source_count', 0)}",
        f"å¤šæºç¡®è®¤: {data.get('multi_source', False)}",
        f"å®˜æ–¹ç¡®è®¤: {data.get('official_confirmed', False)}",
        f"æƒ…ç»ªåˆ†æ: {data.get('sentiment', {})}",
        "",
        "æœç´¢ç»“æœ:"
    ]

    for i, result in enumerate(results[:3], 1):  # æ˜¾ç¤ºå‰ 3 æ¡
        lines.append(f"{i}. {result.get('title', 'N/A')} (æ¥æº: {result.get('source', 'N/A')}, è¯„åˆ†: {result.get('score', 0.0)})")

    return "\n".join(lines)
```

---

### ç¬¬ 5 å¤©ï¼šé›†æˆåˆ° analyse() æ–¹æ³•

#### ä»»åŠ¡ 5.1ï¼šä¿®æ”¹ analyse() æ–¹æ³•

åœ¨ `src/ai/deep_analysis/gemini.py` ä¸­ä¿®æ”¹ `analyse()` æ–¹æ³•ï¼š

```python
async def analyse(self, payload, preliminary):
    """å¸¦å¯é€‰å·¥å…·é›†æˆçš„æ·±åº¦åˆ†æ"""
    if not getattr(self._config, "DEEP_ANALYSIS_TOOLS_ENABLED", False):
        # é™çº§ï¼šä½¿ç”¨ç°æœ‰ Function Calling æµç¨‹
        logger.info("å·¥å…·æœªå¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿ Function Calling æµç¨‹")
        return await self._analyse_with_function_calling(payload, preliminary)

    max_calls = getattr(self._config, "DEEP_ANALYSIS_MAX_TOOL_CALLS", 3)

    try:
        logger.info("=== å¯åŠ¨ LangGraph å·¥å…·å¢å¼ºæ·±åº¦åˆ†æ ===")
        graph = self._build_deep_graph()

        initial_state = DeepAnalysisState(
            payload=payload,
            preliminary=preliminary,
            search_evidence=None,
            memory_evidence=None,
            next_tools=[],
            tool_call_count=0,
            max_tool_calls=max_calls,
            final_response="",
        )

        final_state = await graph.ainvoke(initial_state)
        final_payload = final_state.get("final_response")
        if not final_payload:
            raise DeepAnalysisError("LangGraph æœªè¿”å›æœ€ç»ˆç»“æœ")

        result = self._parse_json(final_payload)
        logger.info("=== LangGraph æ·±åº¦åˆ†æå®Œæˆ ===")
        return result

    except Exception as exc:
        logger.error("LangGraph å·¥å…·ç¼–æ’å¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿæµç¨‹: %s", exc, exc_info=True)
        return await self._analyse_with_function_calling(payload, preliminary)

async def _analyse_with_function_calling(self, payload, preliminary):
    """
    ä¼ ç»Ÿ Function Calling å®ç°ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
    è¿™æ˜¯ç°æœ‰ analyse() é€»è¾‘ï¼Œé‡æ„ä¸ºå•ç‹¬æ–¹æ³•
    """
    # å°†æ‰€æœ‰ç°æœ‰ analyse() ä»£ç ç§»åˆ°è¿™é‡Œ
    # ... (ç°æœ‰å®ç°)
    pass
```

#### ä»»åŠ¡ 5.2ï¼šåœ¨ __init__ ä¸­åˆå§‹åŒ–å·¥å…·

```python
def __init__(self, *, client, memory_bundle, parse_json_callback, config=None):
    # ... ç°æœ‰åˆå§‹åŒ–

    self._config = config or SimpleNamespace()
    self._search_tool = None

    if config and getattr(config, "TOOL_SEARCH_ENABLED", False):
        from src.ai.tools import SearchTool

        try:
            self._search_tool = SearchTool(config)
            provider = getattr(config, "DEEP_ANALYSIS_SEARCH_PROVIDER", "tavily")
            logger.info("æœç´¢å·¥å…·å·²åˆå§‹åŒ–ï¼ŒProvider=%s", provider)
        except ValueError as exc:
            logger.warning("æœç´¢å·¥å…·åˆå§‹åŒ–å¤±è´¥: %s", exc)
            self._search_tool = None
```

> å…¼å®¹æ€§æç¤ºï¼š`config` å‚æ•°ä¿æŒå¯é€‰ï¼Œæ—§çš„å·¥å‚è°ƒç”¨æ— éœ€ç«‹å³ä¿®æ”¹ï¼›ä½†éœ€è¦åœ¨æ–‡ä»¶é¡¶éƒ¨è¡¥å…… `from types import SimpleNamespace`ã€‚å¦‚æœæœªæ¥åˆ‡æ¢ Providerï¼Œåªéœ€è°ƒæ•´é…ç½®å¹¶ç¡®ä¿å¯¹åº” API Key å·²è®¾ç½®å³å¯ã€‚

- åœ¨ `src/ai/deep_analysis/factory.py` ä¸­è°ƒç”¨ `GeminiDeepAnalysisEngine` æ—¶ï¼Œè®°å¾—ä¼ å…¥åŒä¸€ä»½ `config` å®ä¾‹ï¼Œç¡®ä¿å¼€å…³ä¸ API key ç”Ÿæ•ˆã€‚
- å¦‚éœ€æ”¯æŒè¿è¡Œæ—¶åˆ·æ–° Providerï¼Œå¯åœ¨é…ç½®å˜æ›´åè°ƒç”¨ `self._search_tool.refresh_provider()`ï¼ˆå·²é¢„ç•™æ¥å£ï¼‰ã€‚

---

### ç¬¬ 6-7 å¤©ï¼šæµ‹è¯•ä¸è°ƒä¼˜

#### ä»»åŠ¡ 6.1ï¼šåŠŸèƒ½æµ‹è¯•

**æµ‹è¯•æ¶ˆæ¯**ï¼š

```python
# ä¼ é—»ç±»
test_rumor = "Coinbase å³å°†ä¸Šçº¿ XYZ ä»£å¸,å†…éƒ¨äººå£«é€éœ²ä¸‹å‘¨å…¬å¸ƒ"

# æ”¿ç­–ç±»
test_policy = "SEC æ‰¹å‡†æ¯”ç‰¹å¸ç°è´§ ETF,å°†äºä¸‹å‘¨å¼€å§‹äº¤æ˜“"

# é»‘å®¢ç±»
test_hack = "XXX DeFi åè®®é­å—é—ªç”µè´·æ”»å‡»,æŸå¤±è¶…è¿‡ $100M USDC"
```

**éªŒè¯æ­¥éª¤**ï¼š
1. æ¶ˆæ¯è§¦å‘æ·±åº¦åˆ†æ
2. Context Gather è·å–è®°å¿†
3. Tool Planner å†³å®šè°ƒç”¨æœç´¢
4. Tool Executor è°ƒç”¨ Tavily API
5. Synthesis ç»¼åˆè¯æ® â†’ æœ€ç»ˆä¿¡å·

**æ—¥å¿—æ£€æŸ¥ç‚¹**ï¼š
```
[INFO] ğŸ§  Context Gather: æ‰¾åˆ° 2 æ¡å†å²äº‹ä»¶
[INFO] ğŸ¤– Tool Planner å†³ç­–: ['search'], ç†ç”±: ä¼ é—»ç±»äº‹ä»¶éœ€å¤šæºéªŒè¯
[INFO] ğŸ”§ SearchTool(provider=tavily) è¯·æ±‚: keyword='XYZ listing'
[INFO] ğŸ”§ SearchTool è¿”å› 4 æ¡ç»“æœ
[INFO] ğŸ“Š Synthesis: æœ€ç»ˆç½®ä¿¡åº¦ 0.65 (åˆæ­¥ 0.80)
```

#### ä»»åŠ¡ 6.2ï¼šè¾¹ç•Œæµ‹è¯•

**æµ‹è¯•åœºæ™¯**ï¼š
- [ ] Tavily API è¶…æ—¶ â†’ éªŒè¯é™çº§åˆ°ä¼ ç»Ÿæµç¨‹
- [ ] Tavily é€Ÿç‡é™åˆ¶ï¼ˆ429ï¼‰â†’ éªŒè¯é”™è¯¯å¤„ç†
- [ ] æ— æœç´¢ç»“æœ â†’ éªŒè¯ Synthesis èƒ½å¤„ç†ç©ºè¯æ®
- [ ] ç»“æœå†²çªï¼ˆä¸åŒæ¥æºçŸ›ç›¾ï¼‰â†’ éªŒè¯ç½®ä¿¡åº¦ä¸‹è°ƒ

#### ä»»åŠ¡ 6.3ï¼šæˆæœ¬å’Œå»¶è¿Ÿæµ‹è¯•

**æ”¶é›†æŒ‡æ ‡**ï¼ˆ10 æ¡æµ‹è¯•æ¶ˆæ¯ï¼‰ï¼š
- [ ] å¹³å‡æ€»å»¶è¿Ÿ
- [ ] Context Gather å»¶è¿Ÿ
- [ ] Tool Planner å»¶è¿Ÿ
- [ ] Tool Executor å»¶è¿Ÿ
- [ ] Synthesis å»¶è¿Ÿ
- [ ] æ¯æ¡æ¶ˆæ¯çš„ Tavily API è°ƒç”¨æ¬¡æ•°
- [ ] æ¯æ¡æ¶ˆæ¯çš„å¹³å‡æˆæœ¬ï¼š
  - Tool Planner (Gemini): ~$0.01
  - Tavily API: ~$0.002ï¼ˆæŒ‰ $20/æœˆæ— é™é‡è®¡ç®—ï¼‰
  - Synthesis (Gemini): ~$0.02
  - **æ€»è®¡**: ~$0.032/æ¡

**æ€§èƒ½ç›®æ ‡**ï¼š
- å¹³å‡å»¶è¿Ÿ < 8sï¼ˆContext 1s + Planner 2s + Executor 2s + Synthesis 3sï¼‰
- Tavily æˆåŠŸç‡ > 95%
- æ¯æ¡æ¶ˆæ¯å·¥å…·è°ƒç”¨ â‰¤ 1 æ¬¡ï¼ˆç¬¬ä¸€é˜¶æ®µç®€å•åœºæ™¯ï¼‰

#### ä»»åŠ¡ 6.4ï¼šPrompt è°ƒä¼˜

æ ¹æ®æµ‹è¯•ç»“æœï¼š
- [ ] å¦‚æœ Tool Planner è¿‡åº¦è°ƒç”¨æœç´¢ â†’ åŠ å¼º prompt çº¦æŸ
- [ ] å¦‚æœ Synthesis ç½®ä¿¡åº¦è°ƒæ•´ä¸åˆç† â†’ ä¼˜åŒ–è¯æ®æƒé‡
- [ ] å¦‚æœæœç´¢ç»“æœè´¨é‡ä½ â†’ è°ƒæ•´ Tavily å‚æ•°ï¼ˆinclude_domains, search_depthï¼‰

#### ä»»åŠ¡ 6.5ï¼šå¯è§‚æµ‹æ€§å¢å¼º

ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ è¯¦ç»†æ—¥å¿—ï¼š

```python
# åœ¨ _node_context_gather ä¸­
logger.info(f"ğŸ§  Context Gather: æ‰¾åˆ° {len(memory_entries)} æ¡è®°å¿†, æœ€é«˜ç›¸ä¼¼åº¦: {top_similarity:.2f}")

# åœ¨ _node_tool_planner ä¸­
logger.info(f"ğŸ¤– Tool Planner: å†³ç­–={next_tools}, ç†ç”±={reason}, è½®æ¬¡={tool_call_count+1}/3")

# åœ¨ _node_tool_executor ä¸­
logger.info(f"ğŸ”§ Tool Executor: Tavily keyword='{keyword}', ç»“æœæ•°={len(results)}, è§¦å‘={triggered}")

# åœ¨ _node_synthesis ä¸­
logger.info(f"ğŸ“Š Synthesis: ç½®ä¿¡åº¦ {final_conf:.2f} (Î” {final_conf - prelim_conf:+.2f}), è¯æ®: è®°å¿†={mem_count}, æœç´¢={search_count}")
```

å¯é€‰ï¼šå°†å·¥å…·è°ƒç”¨è®°å½•åˆ°æ•°æ®åº“ï¼š

```sql
CREATE TABLE deep_analysis_tool_calls (
    id BIGSERIAL PRIMARY KEY,
    event_id BIGINT REFERENCES news_events(id),
    tool_name TEXT NOT NULL,
    request_params JSONB,
    response_data JSONB,
    latency_ms INTEGER,
    success BOOLEAN,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

---

## é…ç½®

### ç¯å¢ƒå˜é‡ï¼ˆ.envï¼‰

æ·»åŠ åˆ° `.env`ï¼š

```bash
# ==================== æ·±åº¦åˆ†æå·¥å…·ï¼ˆç¬¬ä¸€é˜¶æ®µï¼‰====================

# å·¥å…·ç‰¹æ€§å¼€å…³
DEEP_ANALYSIS_TOOLS_ENABLED=false        # é»˜è®¤å…³é—­ï¼Œæµ‹è¯•é€šè¿‡åå¯ç”¨

# å·¥å…·è°ƒç”¨é™åˆ¶
DEEP_ANALYSIS_MAX_TOOL_CALLS=3           # æœ€å¤§å·¥å…·è½®æ¬¡
DEEP_ANALYSIS_TOOL_TIMEOUT=10            # æ¯ä¸ªå·¥å…·è¶…æ—¶ï¼ˆç§’ï¼‰

# æœç´¢å·¥å…·é…ç½®
TOOL_SEARCH_ENABLED=true                 # æœç´¢å·¥å…·å¼€å…³
DEEP_ANALYSIS_SEARCH_PROVIDER=tavily     # é»˜è®¤ Providerï¼Œå¯åˆ‡æ¢ä¸ºåç»­æ‰©å±•
TAVILY_API_KEY=                          # Provider=tavily æ—¶å¿…å¡«
SEARCH_MAX_RESULTS=5                     # æœ€å¤§æœç´¢ç»“æœæ•°
SEARCH_MULTI_SOURCE_THRESHOLD=3          # å¤šæºä¸€è‡´æ€§é˜ˆå€¼ï¼ˆæ¥æºæ•°ï¼‰
SEARCH_INCLUDE_DOMAINS=coindesk.com,theblock.co,cointelegraph.com  # ä¼˜å…ˆåŸŸåï¼ˆé€—å·åˆ†éš”ï¼‰

# ç¬¬äºŒé˜¶æ®µ+ å·¥å…·ï¼ˆæš‚æ—¶ç¦ç”¨ï¼‰
TOOL_PRICE_ENABLED=false
TOOL_MACRO_ENABLED=false
TOOL_ONCHAIN_ENABLED=false
```

### Config ç±»æ›´æ–°

æ·»åŠ åˆ° `src/config.py`ï¼š

```python
# æ·±åº¦åˆ†æå·¥å…·ï¼ˆç¬¬ä¸€é˜¶æ®µï¼‰
DEEP_ANALYSIS_TOOLS_ENABLED: bool = False
DEEP_ANALYSIS_MAX_TOOL_CALLS: int = 3
DEEP_ANALYSIS_TOOL_TIMEOUT: int = 10

# æœç´¢å·¥å…·
TOOL_SEARCH_ENABLED: bool = True
DEEP_ANALYSIS_SEARCH_PROVIDER: str = "tavily"
TAVILY_API_KEY: str = ""
SEARCH_MAX_RESULTS: int = 5
SEARCH_MULTI_SOURCE_THRESHOLD: int = 3
SEARCH_INCLUDE_DOMAINS: str = "coindesk.com,theblock.co,cointelegraph.com"

# æœªæ¥çš„å·¥å…·
TOOL_PRICE_ENABLED: bool = False
TOOL_MACRO_ENABLED: bool = False
TOOL_ONCHAIN_ENABLED: bool = False
```

---

## éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½æ€§
- [ ] ä¼ é—»/æ”¿ç­–/é»‘å®¢æ¶ˆæ¯è§¦å‘æœç´¢å·¥å…·ï¼ˆé»˜è®¤ Provider=tavilyï¼‰
- [ ] æœç´¢ç»“æœå¡«å…… `search_evidence`ï¼ˆmulti_sourceã€official_confirmedã€sentimentï¼‰
- [ ] Synthesis ç»“åˆæœç´¢ + è®°å¿†è¯æ®è°ƒæ•´ç½®ä¿¡åº¦
- [ ] æœç´¢å¤±è´¥æ—¶é™çº§åˆ°ä¼ ç»Ÿæµç¨‹ï¼Œä¸é˜»å¡æ¶ˆæ¯å¤„ç†

### æ€§èƒ½
- [ ] å¹³å‡å»¶è¿Ÿ < 8s
- [ ] Tavily API æˆåŠŸç‡ > 95%
- [ ] æ¯æ¡æ¶ˆæ¯å·¥å…·è°ƒç”¨ â‰¤ 1 æ¬¡ï¼ˆç¬¬ä¸€é˜¶æ®µç®€åŒ–åœºæ™¯ï¼‰

### æˆæœ¬
- [ ] å¹³å‡æˆæœ¬ < $0.05/æ¡
- [ ] Tavily æœˆåº¦é…é¢åœ¨é™åˆ¶å†…ï¼ˆ1,000 å…è´¹æˆ– $20 æ— é™é‡ï¼‰

### è´¨é‡
- [ ] ä¼ é—»æ¶ˆæ¯ç½®ä¿¡åº¦å‡†ç¡®æ€§æå‡ï¼ˆå¯¹æ¯”äººå·¥æ ‡æ³¨ï¼‰
- [ ] è¯¯æŠ¥ç‡é™ä½ï¼ˆå¤šæºéªŒè¯è¿‡æ»¤è™šå‡ä¼ é—»ï¼‰
- [ ] Synthesis çš„ `notes` å­—æ®µåŒ…å«æœç´¢æ¥æºå¼•ç”¨

### å¯ç»´æŠ¤æ€§
- [ ] ä»£ç æœ‰å®Œæ•´æ³¨é‡Šå’Œç±»å‹æç¤º
- [ ] å·¥å…·é€»è¾‘ä¸ LangGraph é€»è¾‘è§£è€¦ï¼ˆä¾¿äºæ·»åŠ æ›´å¤šå·¥å…·ï¼‰
- [ ] `DEEP_ANALYSIS_TOOLS_ENABLED` å¼€å…³å¯éšæ—¶ç¦ç”¨åŠŸèƒ½

---

## Tavily API å‚è€ƒ

### ç«¯ç‚¹

```
POST https://api.tavily.com/search
```

### è¯·æ±‚ç¤ºä¾‹

```bash
curl -X POST https://api.tavily.com/search \
  -H "Content-Type: application/json" \
  -d '{
    "api_key": "tvly-xxxxx",
    "query": "USDC depeg Circle official statement",
    "max_results": 5,
    "search_depth": "basic",
    "include_domains": ["coindesk.com", "theblock.co"],
    "include_answer": false
  }'
```

### å“åº”ç¤ºä¾‹

```json
{
  "query": "USDC depeg Circle official statement",
  "results": [
    {
      "title": "Circle: USDC reserves are safe amid depeg concerns",
      "url": "https://coindesk.com/...",
      "content": "Circle CEO Jeremy Allaire stated that all USDC reserves...",
      "score": 0.95,
      "published_date": "2025-10-11"
    },
    {
      "title": "USDC briefly depegs to $0.98 on Binance",
      "url": "https://theblock.co/...",
      "content": "The USD Coin (USDC) stablecoin briefly lost its peg...",
      "score": 0.89,
      "published_date": "2025-10-11"
    }
  ]
}
```

### å®šä»·

- **å…è´¹å±‚**: 1,000 æ¬¡è¯·æ±‚/æœˆ
- **Pro å±‚**: $20/æœˆï¼Œæ— é™è¯·æ±‚ï¼Œæ›´å¿«å“åº”
- **å¹³å‡å»¶è¿Ÿ**: 1-2 ç§’

### é”™è¯¯å¤„ç†

- **401**: API key æ— æ•ˆ â†’ æ£€æŸ¥é…ç½®
- **429**: è¶…å‡ºé…é¢ â†’ ç­‰å¾…æœˆåº¦é‡ç½®æˆ–å‡çº§åˆ° Pro
- **503**: æœåŠ¡ä¸å¯ç”¨ â†’ é‡è¯• 3 æ¬¡åé™çº§

---

## ğŸ’° æˆæœ¬ä¸é£é™©æ§åˆ¶

### æˆæœ¬é¢„ä¼°ï¼ˆåŸºäº Gemini 2.5 Flashï¼‰

#### å•æ¡æ¶ˆæ¯æˆæœ¬æ‹†è§£

**ç°æœ‰æ·±åº¦åˆ†ææµç¨‹** (ä½¿ç”¨ Gemini 2.5 Flash Function Calling):
```
Gemini Flash åˆæ­¥åˆ†æ (AiSignalEngine): $0.0015 (è¾“å…¥ 1K tokens Ã— $0.00001875 + è¾“å‡º 500 tokens Ã— $0.000075)
+ Gemini 2.5 Flash æ·±åº¦åˆ†æ (Function Calling): $0.003 (è¾“å…¥ 2K tokens + è¾“å‡º 800 tokens)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ç°æœ‰æˆæœ¬: ~$0.0045/æ¡ (è§¦å‘æ·±åº¦åˆ†æçš„é«˜ä»·å€¼æ¶ˆæ¯)
```

**Phase 1 æ–°å¢æˆæœ¬** (è§¦å‘ LangGraph å·¥å…·æµç¨‹):
```
Context Gather (è®°å¿†æ£€ç´¢): $0 (ä¸è°ƒç”¨ AIï¼Œä»…æ•°æ®åº“æŸ¥è¯¢)

+ Tool Planner (Gemini 2.5 Flash å†³ç­–):
  è¾“å…¥: ~1.5K tokens (æ¶ˆæ¯å†…å®¹ + åˆæ­¥ç»“æœ + è¯æ®æ§½ä½ + å†³ç­–è§„åˆ™)
  è¾“å‡º: ~100 tokens (JSON: {"tools": ["search"], "reason": "..."})
  æˆæœ¬: $0.00003 (è¾“å…¥) + $0.00001 (è¾“å‡º) = $0.00004

+ Tavily API:
  - å…è´¹å±‚: $0 (1000 æ¬¡/æœˆ)
  - Pro å±‚å‡æ‘Š: $0.02 (å‡è®¾ 1000 æ¬¡/æœˆï¼Œ$20/æœˆ)

+ Synthesis (Gemini 2.5 Flash ç»¼åˆæ¨ç†):
  è¾“å…¥: ~2.5K tokens (æ¶ˆæ¯ + åˆåˆ¤ + è®°å¿† + æœç´¢ç»“æœ + è¯¦ç»†è§„åˆ™)
  è¾“å‡º: ~600 tokens (å®Œæ•´ JSON ä¿¡å·)
  æˆæœ¬: $0.00005 (è¾“å…¥) + $0.00005 (è¾“å‡º) = $0.0001

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Phase 1 æ–°å¢æˆæœ¬: $0.00014 (ä¸å« Tavily) æˆ– $0.02014 (å« Tavily Pro å‡æ‘Š)
```

**æ€»æˆæœ¬**:
```
åœºæ™¯ 1: Tool Planner å†³å®šä¸æœç´¢ (é¢„è®¡ 60% æ¦‚ç‡)
  = ç°æœ‰æˆæœ¬ + Planner æˆæœ¬
  = $0.0045 + $0.00004
  = $0.00454/æ¡ (+1%)

åœºæ™¯ 2: Tool Planner å†³å®šæœç´¢ (é¢„è®¡ 40% æ¦‚ç‡)
  = ç°æœ‰æˆæœ¬ + Planner + Tavily + Synthesis
  = $0.0045 + $0.00004 + $0 + $0.0001  (ä½¿ç”¨ Tavily å…è´¹å±‚)
  = $0.00564/æ¡ (+25%)

  æˆ– $0.0045 + $0.00004 + $0.02 + $0.0001 = $0.02464/æ¡ (+448%)  (Pro å±‚å‡æ‘Š)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
åŠ æƒå¹³å‡æˆæœ¬ (ä½¿ç”¨ Tavily å…è´¹å±‚):
  $0.00454 Ã— 60% + $0.00564 Ã— 40% = $0.00498/æ¡ (+11%)

åŠ æƒå¹³å‡æˆæœ¬ (Tavily Pro å±‚):
  $0.00454 Ã— 60% + $0.02464 Ã— 40% = $0.01258/æ¡ (+180%)
```

**æˆæœ¬å¢å¹…æ€»ç»“**:
- **æœ€ä¼˜åœºæ™¯** (ä½¿ç”¨ Tavily å…è´¹å±‚): **+11%** ($0.0045 â†’ $0.00498)
- **Pro åœºæ™¯** (è¶…å‡ºå…è´¹å±‚): **+180%** ($0.0045 â†’ $0.01258)

---

#### æœˆåº¦æˆæœ¬é¢„ä¼°

å‡è®¾æ¯å¤© **50 æ¡**é«˜ä»·å€¼æ¶ˆæ¯è§¦å‘æ·±åº¦åˆ†æ:

| åœºæ™¯ | ç°æœ‰æˆæœ¬/æœˆ | Phase 1 æˆæœ¬/æœˆ (å…è´¹å±‚) | Phase 1 æˆæœ¬/æœˆ (Pro) | å¢é‡ |
|------|------------|------------------------|---------------------|------|
| ä¿å®ˆä¼°ç®— (30% æœç´¢ç‡) | **$6.75** | **$7.31** | **$17.01** | +$0.56 / +$10.26 |
| ä¸­ç­‰ä¼°ç®— (40% æœç´¢ç‡) | **$6.75** | **$7.47** | **$18.87** | +$0.72 / +$12.12 |
| æ¿€è¿›ä¼°ç®— (60% æœç´¢ç‡) | **$6.75** | **$7.79** | **$22.59** | +$1.04 / +$15.84 |

**Tavily API é¢å¤–æˆæœ¬**:
- å¦‚æœæœˆæœç´¢æ¬¡æ•° â‰¤ 1000: **$0/æœˆ** (å…è´¹å±‚)
- å¦‚æœæœˆæœç´¢æ¬¡æ•° > 1000: **$20/æœˆ** (Pro å±‚)

**å…³é”®ç»“è®º**:
- âœ… **å¦‚æœä½¿ç”¨å…è´¹å±‚ (æœˆæœç´¢ < 1000 æ¬¡)**: æˆæœ¬å¢é‡ä»… **$0.72 - $1.04/æœˆ** (+11%)
- âš ï¸ **å¦‚æœéœ€è¦ Pro å±‚ (æœˆæœç´¢ > 1000 æ¬¡)**: æˆæœ¬å¢é‡ä¸º **$12.12 - $15.84/æœˆ** (+180%)

**æœˆæœç´¢æ¬¡æ•°é¢„ä¼°**:
```
æ¯å¤© 50 æ¡æ·±åº¦åˆ†ææ¶ˆæ¯ Ã— 40% æœç´¢ç‡ Ã— 30 å¤© = 600 æ¬¡/æœˆ
```
â†’ **ä¸éœ€è¦ Pro å±‚**ï¼Œå¯å®Œå…¨ä½¿ç”¨å…è´¹å±‚

---

### æˆæœ¬ä¼˜åŒ–ç­–ç•¥

#### ğŸ”´ å¿…é¡»å®æ–½ï¼ˆé¿å…è¶…å‡ºå…è´¹é…é¢ï¼‰

##### 1. æœç´¢ç»“æœç¼“å­˜

**ç›®æ ‡**: ç›¸åŒå…³é”®è¯ 10 åˆ†é’Ÿå†…å¤ç”¨ç»“æœï¼Œå‡å°‘ Tavily API è°ƒç”¨

**å®ç°** (åœ¨ `SearchTool` æ·»åŠ ):
```python
import hashlib
import time
from typing import Optional, Dict

class SearchTool:
    def __init__(self, config) -> None:
        # ... ç°æœ‰ä»£ç 
        self._cache: Dict[str, tuple[ToolResult, float]] = {}  # {keyword_hash: (result, timestamp)}
        self._cache_ttl = getattr(config, "SEARCH_CACHE_TTL_SECONDS", 600)  # 10 åˆ†é’Ÿ

    async def fetch(self, *, keyword: str, max_results: Optional[int] = None) -> ToolResult:
        # æ£€æŸ¥ç¼“å­˜
        cache_key = hashlib.md5(f"{keyword}:{max_results}".encode()).hexdigest()
        if cache_key in self._cache:
            cached_result, cached_time = self._cache[cache_key]
            if time.time() - cached_time < self._cache_ttl:
                logger.info("ğŸ”§ ä½¿ç”¨ç¼“å­˜çš„æœç´¢ç»“æœ: keyword='%s'", keyword)
                return cached_result
            else:
                del self._cache[cache_key]  # æ¸…ç†è¿‡æœŸç¼“å­˜

        # è°ƒç”¨ Provider
        result = await self._provider.search(keyword=keyword, max_results=max_results or self._max_results)

        # å­˜å…¥ç¼“å­˜
        if result.success:
            self._cache[cache_key] = (result, time.time())

        return result
```

**æ”¶ç›Š**: å‡è®¾ç¼“å­˜å‘½ä¸­ç‡ 30%ï¼Œå¯èŠ‚çœ **180 æ¬¡ Tavily è°ƒç”¨/æœˆ**

---

##### 2. æ¯æ—¥è°ƒç”¨é…é¢é™åˆ¶

**ç›®æ ‡**: é™åˆ¶æ¯å¤©æœ€å¤š 50 æ¬¡å·¥å…·è°ƒç”¨ï¼Œé˜²æ­¢æ„å¤–è¶…é™

**å®ç°** (åœ¨ `GeminiDeepAnalysisEngine.__init__` å’Œ `_node_tool_executor` æ·»åŠ ):

```python
from datetime import datetime, timezone

def __init__(self, ...):
    # ... ç°æœ‰ä»£ç 
    self._tool_call_daily_limit = getattr(config, "DEEP_ANALYSIS_TOOL_DAILY_LIMIT", 50)
    self._tool_call_count_today = 0
    self._tool_call_reset_date = datetime.now(timezone.utc).date()

def _check_tool_quota(self) -> bool:
    """æ£€æŸ¥æ˜¯å¦è¶…å‡ºæ¯æ—¥é…é¢"""
    today = datetime.now(timezone.utc).date()

    # è·¨å¤©é‡ç½®è®¡æ•°å™¨
    if today != self._tool_call_reset_date:
        self._tool_call_count_today = 0
        self._tool_call_reset_date = today

    # æ£€æŸ¥é…é¢
    if self._tool_call_count_today >= self._tool_call_daily_limit:
        logger.warning(
            "âš ï¸ ä»Šæ—¥å·¥å…·è°ƒç”¨é…é¢å·²ç”¨å°½ (%d/%d)ï¼Œè·³è¿‡æœ¬æ¬¡è°ƒç”¨",
            self._tool_call_count_today,
            self._tool_call_daily_limit
        )
        return False

    self._tool_call_count_today += 1
    return True

async def _node_tool_executor(self, state: DeepAnalysisState) -> dict:
    """æ‰§è¡Œ planner å†³å®šçš„å·¥å…·ï¼ˆå¼‚æ­¥ï¼Œç¬¬ä¸€é˜¶æ®µä»…æœç´¢ï¼‰"""
    tools_to_call = state.get("next_tools", [])
    logger.info("ğŸ”§ Tool Executor: è°ƒç”¨å·¥å…·: %s", tools_to_call)

    # æ£€æŸ¥é…é¢ï¼ˆæ–°å¢ï¼‰
    if not self._check_tool_quota():
        logger.warning("âš ï¸ è¶…å‡ºæ¯æ—¥é…é¢ï¼Œè·³è¿‡å·¥å…·è°ƒç”¨")
        return {"tool_call_count": state["tool_call_count"] + 1}  # ç›´æ¥è·³è¿‡

    # ... åŸæœ‰é€»è¾‘
```

**é…ç½®**:
```bash
# åœ¨ .env æ·»åŠ 
DEEP_ANALYSIS_TOOL_DAILY_LIMIT=50  # æ¯å¤©æœ€å¤š 50 æ¬¡å·¥å…·è°ƒç”¨
```

**æ”¶ç›Š**: ç¡®ä¿æœˆæœç´¢æ¬¡æ•° â‰¤ 1500 (50/å¤© Ã— 30å¤©)ï¼Œç•™æœ‰å®‰å…¨ä½™é‡

---

##### 3. ç™½åå• + é»‘åå•ç­–ç•¥

**ç›®æ ‡**: ä»…å¯¹å¿…éœ€çš„äº‹ä»¶ç±»å‹è§¦å‘æœç´¢ï¼Œé¿å…è¿‡åº¦è°ƒç”¨

**å®ç°** (åœ¨ `_node_tool_planner` æ·»åŠ ):
```python
# åœ¨ _node_tool_planner æ–¹æ³•å¼€å¤´æ·»åŠ 
FORCE_SEARCH_EVENT_TYPES = {"hack", "regulation", "partnership"}  # å¼ºåˆ¶æœç´¢
NEVER_SEARCH_EVENT_TYPES = {"macro", "governance", "airdrop", "celebrity"}  # æ°¸ä¸æœç´¢

async def _node_tool_planner(self, state: DeepAnalysisState) -> dict:
    """AI å†³å®šæ˜¯å¦è°ƒç”¨æœç´¢å·¥å…·ï¼ˆå¼‚æ­¥ï¼‰"""
    logger.info("ğŸ¤– Tool Planner: å†³ç­–ä¸‹ä¸€æ­¥å·¥å…·")

    preliminary = state["preliminary"]

    # é»‘åå•ï¼šç›´æ¥è·³è¿‡
    if preliminary.event_type in NEVER_SEARCH_EVENT_TYPES:
        logger.info("ğŸ¤– Tool Planner: äº‹ä»¶ç±»å‹ '%s' åœ¨é»‘åå•ï¼Œè·³è¿‡æœç´¢", preliminary.event_type)
        return {"next_tools": []}

    # ç™½åå•ï¼šå¼ºåˆ¶æœç´¢ï¼ˆä»…é¦–è½®ï¼‰
    if preliminary.event_type in FORCE_SEARCH_EVENT_TYPES and state["tool_call_count"] == 0:
        logger.info("ğŸ¤– Tool Planner: äº‹ä»¶ç±»å‹ '%s' åœ¨ç™½åå•ï¼Œå¼ºåˆ¶æœç´¢", preliminary.event_type)
        return {"next_tools": ["search"]}

    # å·²æœ‰æœç´¢ç»“æœï¼šä¸å†é‡å¤æœç´¢
    if state.get("search_evidence"):
        logger.info("ğŸ¤– Tool Planner: å·²æœ‰æœç´¢ç»“æœï¼Œæ— éœ€å†æœç´¢")
        return {"next_tools": []}

    # å…¶ä»–æƒ…å†µï¼šè®© AI å†³ç­–ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
    # ...
```

**æ”¶ç›Š**: å‡å°‘çº¦ 40% ä¸å¿…è¦çš„æœç´¢è°ƒç”¨

---

#### ğŸŸ¡ å»ºè®®å®æ–½ï¼ˆæå‡æ€§ä»·æ¯”ï¼‰

##### 4. Tool Planner Prompt æ·»åŠ æˆæœ¬æ„è¯†

åœ¨ `_build_planner_prompt` çš„å†³ç­–è§„åˆ™ä¸­æ·»åŠ :
```python
ã€å†³ç­–è§„åˆ™ã€‘ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
0. âš ï¸ æˆæœ¬æ„è¯†ï¼šæ¯æ¬¡æœç´¢æ¶ˆè€—é…é¢ï¼Œè¯·è°¨æ…å†³ç­–
1. å¦‚æœå·²æœ‰æœç´¢ç»“æœ â†’ è¯æ®å……åˆ†ï¼Œæ— éœ€å†æœç´¢
2. å¦‚æœäº‹ä»¶ç±»å‹æ˜¯ hack/regulation/partnership â†’ éœ€è¦æœç´¢éªŒè¯
3. å¦‚æœ tool_call_count >= 2 â†’ é¿å…è¿‡åº¦æœç´¢
4. å¦‚æœæ˜¯æ•°å€¼ç±»äº‹ä»¶ (depeg/liquidation) â†’ æš‚ä¸éœ€è¦æœç´¢
5. å¦‚æœè®°å¿†ä¸­å·²æœ‰é«˜ç›¸ä¼¼åº¦æ¡ˆä¾‹ (similarity > 0.8) â†’ ä¼˜å…ˆä½¿ç”¨è®°å¿†ï¼Œå‡å°‘æœç´¢
```

---

##### 5. æ¸è¿›å¼ Rollout

**Week 1-2**: 5% æµé‡
```python
# åœ¨ analyse() æ–¹æ³•æ·»åŠ 
import random
if random.random() > 0.05:  # 95% æµé‡èµ°åŸæµç¨‹
    return await self._analyse_with_function_calling(payload, preliminary)
```

**Week 3-4**: ç›‘æ§æŒ‡æ ‡
- æ¯æ—¥ Tavily è°ƒç”¨æ¬¡æ•°
- å¹³å‡æˆæœ¬
- ç½®ä¿¡åº¦æ”¹å–„å¹…åº¦
- è¯¯æŠ¥ç‡å˜åŒ–

**Month 2**: å¦‚æœæŒ‡æ ‡è‰¯å¥½ï¼Œæ‰©å±•åˆ° 100%

---

### æˆæœ¬ç›‘æ§ä¸é¢„è­¦

#### å®æ—¶æˆæœ¬è¿½è¸ª

åœ¨ `GeminiDeepAnalysisEngine.__init__` æ·»åŠ :
```python
from collections import defaultdict

def __init__(self, ...):
    # ... ç°æœ‰ä»£ç 
    self._cost_tracker = {
        "daily_calls": 0,
        "daily_cost_usd": 0.0,
        "monthly_budget_usd": 30.0,  # $30/æœˆé¢„ç®—
        "cost_by_tool": defaultdict(float),  # {"search": 0.02, "planner": 0.0001, ...}
    }

def _track_cost(self, tool_name: str, cost_usd: float):
    """è®°å½•å·¥å…·è°ƒç”¨æˆæœ¬"""
    self._cost_tracker["daily_calls"] += 1
    self._cost_tracker["daily_cost_usd"] += cost_usd
    self._cost_tracker["cost_by_tool"][tool_name] += cost_usd

    # é¢„è­¦ï¼šæ¯æ—¥æˆæœ¬è¶…è¿‡æœˆé¢„ç®— 1/30
    daily_budget = self._cost_tracker["monthly_budget_usd"] / 30
    if self._cost_tracker["daily_cost_usd"] > daily_budget:
        logger.warning(
            "ğŸ’° ä»Šæ—¥æˆæœ¬è¶…é¢„ç®—: $%.4f (é¢„ç®—: $%.4f/å¤©)",
            self._cost_tracker["daily_cost_usd"],
            daily_budget
        )
```

åœ¨ `_node_tool_executor` ä¸­è°ƒç”¨:
```python
async def _node_tool_executor(self, state: DeepAnalysisState) -> dict:
    # ... è°ƒç”¨å·¥å…·å
    if result.success:
        self._track_cost("search", 0.0 if self._is_free_tier() else 0.02)
    # ...
```

---

### é£é™©ç¼“è§£

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ | çŠ¶æ€ |
|------|------|---------|------|
| **Tavily å…è´¹é…é¢è€—å°½** | éœ€è¦ $20/æœˆå‡çº§ Pro | æœç´¢ç¼“å­˜ + æ¯æ—¥é™é¢ + ç™½åå• | âœ… å·²è®¾è®¡ |
| **Tool Planner è¿‡åº¦è°ƒç”¨** | ä¸å¿…è¦çš„æœç´¢æˆæœ¬ | Prompt ä¼˜åŒ– + ç™½åå•å¼ºåˆ¶è§„åˆ™ | âœ… å·²è®¾è®¡ |
| **æˆæœ¬å¤±æ§** | æœˆæˆæœ¬è¶…é¢„ç®— | å®æ—¶æˆæœ¬è¿½è¸ª + é¢„è­¦ + é™çº§å¼€å…³ | âœ… å·²è®¾è®¡ |
| **Tavily API æ•…éšœ** | æ— æ³•æœç´¢ | å®Œæ•´é™çº§æœºåˆ¶ + é”™è¯¯å¤„ç† | âœ… å·²å®ç° |
| **Tool Planner é”™è¯¯å†³ç­–** | é”™è¿‡å…³é”®æœç´¢ | ç™½åå•å¼ºåˆ¶æœç´¢ + å†³ç­–æ—¥å¿—å®¡è®¡ | âœ… å·²è®¾è®¡ |

---

### ROI åˆ†æ

#### ä»·å€¼é‡åŒ–

**æå‡çš„èƒ½åŠ›**:
1. **ä¼ é—»è¿‡æ»¤**: å¤šæºéªŒè¯å‡å°‘è™šå‡ä¼ é—»
   - å‡è®¾æ¯æœˆé¿å… 1 æ¬¡é”™è¯¯äº¤æ˜“ â†’ èŠ‚çœæŸå¤± $500+
   - Phase 1 æœˆæˆæœ¬: ~$1 (å…è´¹å±‚) æˆ– ~$12 (Pro)
   - **ROI: 50x - 500x**

2. **é»‘å®¢äº‹ä»¶å¿«é€Ÿç¡®è®¤**: å®˜æ–¹å£°æ˜ + æƒ…ç»ªåˆ†æ
   - æå‰ 5-10 åˆ†é’Ÿç¡®è®¤ â†’ æŠ¢å…ˆåšç©º
   - æ½œåœ¨æ”¶ç›Š: å•æ¬¡ > $200
   - **ROI: 16x - 200x**

3. **ç›‘ç®¡æ”¿ç­–å®æ—¶æ€§**: æœç´¢æœ€æ–°æ”¿ç­–
   - é¿å…ä¿¡æ¯æ»å â†’ éš¾ä»¥é‡åŒ–ï¼Œä½†é‡è¦

**è´¨é‡æŒ‡æ ‡æ”¹å–„** (é¢„æœŸ):
- ä¼ é—»æ¶ˆæ¯å‡†ç¡®æ€§: **+15-20%**
- è¯¯æŠ¥ç‡: **-20-30%**
- ä¿¡å·ç½®ä¿¡åº¦: **+10%**

**ç»“è®º**: **ä»·å€¼è¿œå¤§äºæˆæœ¬ï¼ŒROI > 50x**

---

### é…ç½®æ›´æ–°

åœ¨ `.env` æ·»åŠ æˆæœ¬æ§åˆ¶é…ç½®:
```bash
# ==================== æˆæœ¬æ§åˆ¶ ====================

# æ¯æ—¥å·¥å…·è°ƒç”¨é…é¢
DEEP_ANALYSIS_TOOL_DAILY_LIMIT=50

# æœç´¢ç¼“å­˜ TTLï¼ˆç§’ï¼‰
SEARCH_CACHE_TTL_SECONDS=600  # 10 åˆ†é’Ÿ

# æœˆåº¦é¢„ç®—ï¼ˆç¾å…ƒï¼‰
DEEP_ANALYSIS_MONTHLY_BUDGET=30.0

# æ¸è¿›å¼ Rollout æ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰
PHASE1_ROLLOUT_PERCENTAGE=0.05  # 5% æµé‡
```

---

## ç¬¬ä¸€é˜¶æ®µåçš„ä¸‹ä¸€æ­¥

å®Œæˆç¬¬ä¸€é˜¶æ®µåï¼Œæ ¹æ®ç”Ÿäº§æ•°æ®è¯„ä¼°ï¼š

### 1. æœç´¢è´¨é‡ä¼˜åŒ–
- å¦‚æœ Tavily ç»“æœè´¨é‡å·® â†’ è°ƒæ•´ `include_domains` æˆ– `search_depth`
- å¦‚æœè¯¯æŠ¥ç‡é«˜ â†’ å¢å¼ºå¤šæºä¸€è‡´æ€§é€»è¾‘ï¼ˆæ£€æŸ¥æ¥æºæƒå¨æ€§ï¼‰

### 2. æˆæœ¬ä¼˜åŒ–
- å¦‚æœ Tavily é…é¢è¶…é™ â†’ å®ç°ç»“æœç¼“å­˜ï¼ˆç›¸åŒå…³é”®è¯ 10 åˆ†é’Ÿå†…å¤ç”¨ï¼‰
- å¦‚æœ Planner è¿‡åº¦è°ƒç”¨ â†’ ä¼˜åŒ– prompt æˆ–æ·»åŠ äº‹ä»¶ç±»å‹ç™½åå•

### 3. æ‰©å±•åˆ°ç¬¬äºŒé˜¶æ®µ
- å¦‚æœæœç´¢å·¥å…·æ•ˆæœæ˜¾è‘— â†’ ä¼˜å…ˆå®ç°ä»·æ ¼å·¥å…·ï¼ˆè„±é”šåœºæ™¯ï¼‰
- å¦‚æœä¼ é—»éªŒè¯éœ€æ±‚ä¸é«˜ â†’ è·³è¿‡ç¬¬äºŒé˜¶æ®µï¼Œä¸“æ³¨ä¼˜åŒ–ç°æœ‰æµç¨‹

---

## å‚è€ƒèµ„æ–™

### API æ–‡æ¡£
- [Tavily API](https://docs.tavily.com/)
- [Google Custom Search API](https://developers.google.com/custom-search/v1/overview)ï¼ˆå¤‡é€‰ï¼‰

### ç›¸å…³æ–‡æ¡£
- ä¸»æ–¹æ¡ˆï¼š`docs/deep_analysis_tools_integration_plan.md`
- æ¶æ„ï¼š`docs/memory_architecture.md`
- AI ä¿¡å·å¼•æ“ï¼š`docs/aisignalengine_implementation.md`

---

## å˜æ›´æ—¥å¿—

- **2025-10-11**: åˆ›å»ºç¬¬ä¸€é˜¶æ®µå®æ–½æŒ‡å—ï¼ˆä¸­æ–‡ç‰ˆï¼‰
- **2025-10-11**: æ·»åŠ "âš ï¸ é‡è¦ä¿®æ”¹å»ºè®®"ç« èŠ‚ï¼ŒåŸºäºç°æœ‰ä»£ç å®¡æŸ¥æå‡º 6 é¡¹æ”¹è¿›:
  - ğŸ”´ å¿…é¡»ä¿®æ”¹: è®°å¿†æ£€ç´¢é€»è¾‘é‡æ„ã€Tool Planner ä½¿ç”¨ Function Calling
  - ğŸŸ¡ å¼ºçƒˆå»ºè®®: æœç´¢å…³é”®è¯ä¼˜åŒ–ã€Synthesis Prompt é‡åŒ–è§„åˆ™
  - ğŸŸ¢ å¯é€‰ä¼˜åŒ–: æ¯æ—¥é…é¢é™åˆ¶ã€Mock/é›†æˆæµ‹è¯•åˆ†ç¦»
