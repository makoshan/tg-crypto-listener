# æ·±åº¦åˆ†æå¼•æ“å®æ–½ç»†èŠ‚

æœ¬æ–‡æ¡£é’ˆå¯¹åŸæ–¹æ¡ˆçš„å…³é”®å®æ–½é£é™©ï¼Œç»™å‡ºå…·ä½“è§£å†³æ–¹æ¡ˆã€‚

## é—®é¢˜ 1: GeminiClient æ‰©å±• - æ”¯æŒ Function Calling

### ç°çŠ¶
`src/ai/gemini_client.py` åªæ”¯æŒçº¯æ–‡æœ¬è¾“å‡ºï¼Œæ²¡æœ‰ `tools` å‚æ•°å’Œ `function_call` å¤„ç†é€»è¾‘ã€‚

### è§£å†³æ–¹æ¡ˆï¼šåˆ›å»º GeminiFunctionCallingClient

```python
# src/ai/gemini_function_client.py

"""Gemini Function Calling client (google-genai SDK)"""

import asyncio
import json
import logging
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

try:
    from google import genai
    from google.genai.types import Tool, FunctionDeclaration, Content, Part
except ImportError:
    genai = None

from .gemini_client import AiServiceError

logger = logging.getLogger(__name__)


@dataclass
class FunctionCall:
    """Gemini function call representation"""
    name: str
    args: Dict[str, Any]
    id: Optional[str] = None


@dataclass
class GeminiFunctionResponse:
    """Gemini function calling response"""
    text: Optional[str] = None
    function_calls: List[FunctionCall] = None

    def __post_init__(self):
        if self.function_calls is None:
            self.function_calls = []


class GeminiFunctionCallingClient:
    """
    Gemini client with Function Calling support

    æ‰©å±•è‡ª GeminiClientï¼Œå¢åŠ  tools å’Œ function response å¤„ç†
    """

    def __init__(
        self,
        api_key: str,
        model_name: str,
        timeout: float,
        max_retries: int = 1,
        retry_backoff_seconds: float = 1.5,
    ):
        if not api_key:
            raise AiServiceError("Gemini API key is required")
        if genai is None:
            raise AiServiceError("google-genai æœªå®‰è£…")

        self._client = genai.Client(api_key=api_key)
        self._model_name = model_name
        self._timeout = timeout
        self._max_retries = max(0, int(max_retries))
        self._retry_backoff = max(0.0, float(retry_backoff_seconds))

        logger.info(f"GeminiFunctionCallingClient åˆå§‹åŒ–: model={model_name}")

    async def generate_content_with_tools(
        self,
        contents: str | List[Dict],
        tools: Optional[List[Tool]] = None
    ) -> GeminiFunctionResponse:
        """
        è°ƒç”¨ Gemini API with tools support

        Args:
            contents: å¯¹è¯å†…å®¹ï¼ˆå­—ç¬¦ä¸²æˆ–æ¶ˆæ¯åˆ—è¡¨ï¼‰
            tools: Function declarations

        Returns:
            GeminiFunctionResponse (åŒ…å«æ–‡æœ¬æˆ– function calls)
        """
        last_exc: Exception | None = None
        last_error_message = "Gemini è°ƒç”¨å¤±è´¥"
        last_error_temporary = False

        for attempt in range(self._max_retries + 1):
            try:
                response = await asyncio.wait_for(
                    asyncio.to_thread(
                        self._call_model_with_tools,
                        contents,
                        tools
                    ),
                    timeout=self._timeout,
                )
                return response

            except asyncio.TimeoutError as exc:
                last_exc = exc
                last_error_message = "Gemini è¯·æ±‚è¶…æ—¶"
                last_error_temporary = True
                logger.warning(f"Gemini è¶…æ—¶ (å°è¯• {attempt + 1}/{self._max_retries + 1})")

            except Exception as exc:
                last_exc = exc
                last_error_message = str(exc)
                last_error_temporary = self._is_temporary_error(exc)
                logger.warning(
                    f"Gemini å¼‚å¸¸ (å°è¯• {attempt + 1}/{self._max_retries + 1}): {exc}"
                )

            if attempt < self._max_retries:
                backoff = self._retry_backoff * (2 ** attempt)
                await asyncio.sleep(backoff)

        raise AiServiceError(last_error_message, temporary=last_error_temporary) from last_exc

    def _call_model_with_tools(
        self,
        contents: str | List[Dict],
        tools: Optional[List[Tool]]
    ) -> GeminiFunctionResponse:
        """
        åŒæ­¥è°ƒç”¨ Gemini API

        Returns:
            GeminiFunctionResponse
        """
        # æ„å»ºè¯·æ±‚å‚æ•°
        config = {}
        if tools:
            config["tools"] = tools

        # è°ƒç”¨ API
        response = self._client.models.generate_content(
            model=self._model_name,
            contents=contents,
            config=config
        )

        # è§£æå“åº”
        return self._parse_response(response)

    def _parse_response(self, response) -> GeminiFunctionResponse:
        """
        è§£æ Gemini å“åº”ï¼Œæå–æ–‡æœ¬æˆ– function calls

        Response structure:
        - response.candidates[0].content.parts[0].text (æ–‡æœ¬å“åº”)
        - response.candidates[0].content.parts[0].function_call (å‡½æ•°è°ƒç”¨)
        """
        if not hasattr(response, "candidates") or not response.candidates:
            raise AiServiceError("Gemini è¿”å›æ— å€™é€‰ç»“æœ")

        candidate = response.candidates[0]
        if not hasattr(candidate, "content") or not candidate.content:
            raise AiServiceError("Gemini å€™é€‰ç»“æœæ— å†…å®¹")

        content = candidate.content
        parts = getattr(content, "parts", [])

        # æå–æ–‡æœ¬å’Œ function calls
        text_chunks = []
        function_calls = []

        for part in parts:
            # æ–‡æœ¬éƒ¨åˆ†
            if hasattr(part, "text") and part.text:
                text_chunks.append(part.text)

            # Function call éƒ¨åˆ†
            if hasattr(part, "function_call") and part.function_call:
                fc = part.function_call
                function_calls.append(FunctionCall(
                    name=fc.name,
                    args=dict(fc.args),  # è½¬æ¢ protobuf Struct ä¸º dict
                    id=getattr(fc, "id", None)
                ))

        # ä¼˜å…ˆè¿”å› function calls
        if function_calls:
            logger.debug(f"Gemini è¿”å› {len(function_calls)} ä¸ª function calls")
            return GeminiFunctionResponse(
                text=None,
                function_calls=function_calls
            )

        # å¦åˆ™è¿”å›æ–‡æœ¬
        final_text = "".join(text_chunks).strip()
        if not final_text:
            raise AiServiceError("Gemini è¿”å›ç©ºå†…å®¹")

        return GeminiFunctionResponse(text=final_text)

    def build_function_response_content(
        self,
        function_name: str,
        response_data: Dict[str, Any]
    ) -> Content:
        """
        æ„å»º function response å†…å®¹ï¼ˆç”¨äºå›å¡«ï¼‰

        Args:
            function_name: å‡½æ•°å
            response_data: å‡½æ•°æ‰§è¡Œç»“æœ

        Returns:
            Gemini Content å¯¹è±¡
        """
        from google.genai.types import FunctionResponse

        return Content(
            parts=[
                Part(
                    function_response=FunctionResponse(
                        name=function_name,
                        response=response_data
                    )
                )
            ]
        )

    def _is_temporary_error(self, exc: Exception) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæš‚æ—¶æ€§é”™è¯¯"""
        if isinstance(exc, (ConnectionError, OSError, asyncio.TimeoutError)):
            return True

        error_msg = str(exc).upper()
        temporary_keywords = ["TIMEOUT", "UNAVAILABLE", "503", "429", "RESOURCE_EXHAUSTED"]
        return any(kw in error_msg for kw in temporary_keywords)
```

### ä½¿ç”¨ç¤ºä¾‹

```python
# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = GeminiFunctionCallingClient(
    api_key="xxx",
    model_name="gemini-2.5-pro",
    timeout=30.0
)

# å®šä¹‰å·¥å…·
from google.genai.types import Tool, FunctionDeclaration, Schema

memory_tool = Tool(
    function_declarations=[
        FunctionDeclaration(
            name="memory",
            description="Memory management",
            parameters=Schema(
                type="object",
                properties={
                    "command": Schema(type="string"),
                    "path": Schema(type="string"),
                },
                required=["command"]
            )
        )
    ]
)

# è°ƒç”¨
response = await client.generate_content_with_tools(
    contents="è¯·æŸ¥çœ‹ memories/btc_analysis.json",
    tools=[memory_tool]
)

# æ£€æŸ¥å“åº”
if response.function_calls:
    for fc in response.function_calls:
        print(f"Function: {fc.name}, Args: {fc.args}")
else:
    print(f"Text: {response.text}")
```

---

## é—®é¢˜ 2: Gemini ä¸Šä¸‹æ–‡ç®¡ç† - é˜²æ­¢ Token çˆ†ç‚¸

### é—®é¢˜
Gemini æ²¡æœ‰ Claude çš„ `context_management` è‡ªåŠ¨æ¸…ç†æœºåˆ¶ï¼Œå¤šè½® function calling ä¼šå¯¼è‡´ï¼š
- `contents` åˆ—è¡¨æ— é™å¢é•¿
- Token æ•°è¶…è¿‡é™åˆ¶ï¼ˆGemini 2.5 Pro ä¸Šé™ 2M tokensï¼Œä½†å®é™… 32K åæ€§èƒ½ä¸‹é™ï¼‰

### è§£å†³æ–¹æ¡ˆï¼šæ‰‹åŠ¨ä¸Šä¸‹æ–‡çª—å£ç®¡ç†

```python
# src/ai/gemini_deep_engine.py (ä¼˜åŒ–ç‰ˆ)

class GeminiDeepAnalysisEngine(DeepAnalysisEngine):
    def __init__(
        self,
        client: GeminiFunctionCallingClient,
        memory_handler: MemoryToolHandler,
        max_function_turns: int = 5,
        context_window_tokens: int = 16000,  # ä¿ç•™ 16K token çª—å£
        context_keep_turns: int = 2,         # ä¿ç•™æœ€è¿‘ 2 è½®å¯¹è¯
    ):
        self._client = client
        self._memory_handler = memory_handler
        self._max_turns = max_function_turns
        self._context_window = context_window_tokens
        self._keep_turns = context_keep_turns

    async def _run_function_calling_loop(
        self,
        initial_messages: List[Dict],
        tools: List[Tool]
    ) -> str:
        """
        Function calling å¾ªç¯ with context pruning
        """
        # åˆå§‹æ¶ˆæ¯
        conversation = initial_messages.copy()
        turn_count = 0

        while turn_count < self._max_turns:
            # æ­¥éª¤ 1: ä¸Šä¸‹æ–‡è£å‰ªï¼ˆåœ¨æ¯æ¬¡è°ƒç”¨å‰ï¼‰
            pruned_conversation = self._prune_context(conversation)

            logger.debug(
                f"ğŸ”§ Function calling è½®æ¬¡ {turn_count + 1}: "
                f"åŸå§‹æ¶ˆæ¯æ•°={len(conversation)}, è£å‰ªå={len(pruned_conversation)}"
            )

            # æ­¥éª¤ 2: è°ƒç”¨ Gemini
            response = await self._client.generate_content_with_tools(
                contents=pruned_conversation,
                tools=tools
            )

            # æ­¥éª¤ 3: æ£€æŸ¥æ˜¯å¦æœ‰ function calls
            if not response.function_calls:
                # æ²¡æœ‰ function callï¼Œè¿”å›æœ€ç»ˆæ–‡æœ¬
                return response.text or ""

            # æ­¥éª¤ 4: æ‰§è¡Œæ‰€æœ‰ function calls
            for fc in response.function_calls:
                logger.info(f"ğŸ”§ æ‰§è¡Œå‡½æ•°: {fc.name}({fc.args})")

                # æ‰§è¡Œå‡½æ•°
                try:
                    result = self._memory_handler.execute_tool_use(fc.args)
                except Exception as e:
                    result = {"success": False, "error": str(e)}
                    logger.error(f"âŒ å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")

                # å°† function call æ·»åŠ åˆ°å¯¹è¯å†å²
                conversation.append({
                    "role": "model",
                    "parts": [{"function_call": {"name": fc.name, "args": fc.args}}]
                })

                # å°† function response æ·»åŠ åˆ°å¯¹è¯å†å²
                conversation.append({
                    "role": "function",
                    "parts": [{
                        "function_response": {
                            "name": fc.name,
                            "response": result
                        }
                    }]
                })

            turn_count += 1

        raise RuntimeError(f"Function calling è¶…è¿‡æœ€å¤§è½®æ•° {self._max_turns}")

    def _prune_context(self, conversation: List[Dict]) -> List[Dict]:
        """
        è£å‰ªä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢ token çˆ†ç‚¸

        ç­–ç•¥ï¼š
        1. å§‹ç»ˆä¿ç•™åˆå§‹ç³»ç»Ÿæç¤º + ç”¨æˆ·æ¶ˆæ¯
        2. ä¿ç•™æœ€è¿‘ N è½®å‡½æ•°è°ƒç”¨ï¼ˆæ¯è½®åŒ…å« function_call + function_responseï¼‰
        3. åˆ é™¤ä¸­é—´çš„å†å²æ¶ˆæ¯

        Example:
        [
            {role: "user", parts: [...]},           # ä¿ç•™ï¼šåˆå§‹æ¶ˆæ¯
            {role: "model", parts: [fc1]},          # åˆ é™¤ï¼šæ—§çš„ function call
            {role: "function", parts: [resp1]},     # åˆ é™¤ï¼šæ—§çš„ function response
            {role: "model", parts: [fc2]},          # ä¿ç•™ï¼šæœ€è¿‘çš„ function call
            {role: "function", parts: [resp2]},     # ä¿ç•™ï¼šæœ€è¿‘çš„ function response
        ]
        """
        if len(conversation) <= 3:
            # æ¶ˆæ¯å¤ªå°‘ï¼Œä¸éœ€è¦è£å‰ª
            return conversation

        # æ­¥éª¤ 1: æå–åˆå§‹æ¶ˆæ¯ï¼ˆç¬¬ä¸€ä¸ª user æ¶ˆæ¯ï¼‰
        initial_messages = []
        for msg in conversation:
            if msg.get("role") == "user":
                initial_messages.append(msg)
                break

        # æ­¥éª¤ 2: æå–æœ€è¿‘çš„ N è½®å‡½æ•°è°ƒç”¨
        # ä»åå¾€å‰æŸ¥æ‰¾ function_call + function_response å¯¹
        recent_turns = []
        turn_pairs = []
        current_pair = []

        for msg in reversed(conversation[len(initial_messages):]):
            role = msg.get("role")

            # function response
            if role == "function":
                current_pair.insert(0, msg)

            # model function call
            elif role == "model":
                current_pair.insert(0, msg)

                # å¦‚æœ pair å®Œæ•´ï¼ˆmodel + functionï¼‰
                if len(current_pair) == 2:
                    turn_pairs.append(current_pair)
                    current_pair = []

                    if len(turn_pairs) >= self._keep_turns:
                        break

        # åè½¬å›æ­£åº
        for pair in reversed(turn_pairs):
            recent_turns.extend(pair)

        # æ­¥éª¤ 3: åˆå¹¶
        pruned = initial_messages + recent_turns

        # æ­¥éª¤ 4: ä¼°ç®— token æ•°ï¼ˆç®€å•ä¼°ç®—ï¼š1 token â‰ˆ 4 charsï¼‰
        estimated_tokens = sum(
            len(json.dumps(msg)) // 4 for msg in pruned
        )

        if estimated_tokens > self._context_window:
            logger.warning(
                f"âš ï¸ ä¸Šä¸‹æ–‡ä»ç„¶è¿‡é•¿ï¼ˆ{estimated_tokens} tokensï¼‰ï¼Œ"
                f"å°†å‡å°‘ä¿ç•™è½®æ•°"
            )
            # è¿›ä¸€æ­¥è£å‰ªï¼šåªä¿ç•™æœ€è¿‘ 1 è½®
            if len(turn_pairs) > 1:
                recent_turns = []
                for pair in turn_pairs[-1:]:
                    recent_turns.extend(pair)
                pruned = initial_messages + recent_turns

        return pruned
```

### ä¸Šä¸‹æ–‡ç®¡ç†å¯¹æ¯”

| ç­–ç•¥ | Claude | Gemini æ–¹æ¡ˆ |
|------|--------|------------|
| æœºåˆ¶ | Context Editing APIï¼ˆè‡ªåŠ¨ï¼‰ | æ‰‹åŠ¨è£å‰ª `_prune_context` |
| è§¦å‘æ¡ä»¶ | `input_tokens > trigger` | æ¯æ¬¡è°ƒç”¨å‰ |
| ä¿ç•™å†…å®¹ | æœ€è¿‘ N ä¸ª tool uses | åˆå§‹æ¶ˆæ¯ + æœ€è¿‘ N è½® |
| Token ä¼°ç®— | ç²¾ç¡®ï¼ˆAPI è¿”å›ï¼‰ | ç®€å•ä¼°ç®—ï¼ˆå­—ç¬¦æ•° / 4ï¼‰ |

---

## é—®é¢˜ 3: Memory Backend ç»Ÿä¸€åˆå§‹åŒ–

### é—®é¢˜
åŸæ–¹æ¡ˆç›´æ¥åˆ›å»º `MemoryToolHandler(base_path="./memories")`ï¼Œåªæ”¯æŒ Local æ¨¡å¼ï¼Œå¿½ç•¥äº†ç°æœ‰çš„ Supabase/Hybrid backend é…ç½®ã€‚

### è§£å†³æ–¹æ¡ˆï¼šç»Ÿä¸€ Backend å·¥å‚

```python
# src/memory/__init__.py (æ–°å¢)

def create_memory_backend(config):
    """
    æ ¹æ®é…ç½®åˆ›å»ºè®°å¿†åç«¯

    Returns:
        LocalMemoryStore | SupabaseMemoryRepository | HybridMemoryRepository
    """
    backend_type = getattr(config, "MEMORY_BACKEND", "local").lower()

    if backend_type == "supabase":
        from .repository import SupabaseMemoryRepository

        supabase_url = getattr(config, "SUPABASE_URL", "")
        supabase_key = getattr(config, "SUPABASE_SERVICE_KEY", "")

        if not supabase_url or not supabase_key:
            logger.warning("Supabase é…ç½®ä¸å®Œæ•´ï¼Œå›é€€åˆ° Local æ¨¡å¼")
            return create_local_backend(config)

        return SupabaseMemoryRepository(
            supabase_url=supabase_url,
            supabase_key=supabase_key,
            timeout=getattr(config, "SUPABASE_TIMEOUT_SECONDS", 8.0)
        )

    elif backend_type == "hybrid":
        from .hybrid_repository import HybridMemoryRepository

        # Hybrid = Supabase (remote) + Local (fallback)
        supabase_repo = create_memory_backend(
            type("Config", (), {"MEMORY_BACKEND": "supabase", **vars(config)})()
        )
        local_store = create_local_backend(config)

        return HybridMemoryRepository(
            remote=supabase_repo,
            local=local_store
        )

    else:  # local
        return create_local_backend(config)


def create_local_backend(config):
    """åˆ›å»º Local å†…å­˜å­˜å‚¨"""
    from .local_memory_store import LocalMemoryStore

    memory_dir = getattr(config, "MEMORY_DIR", "./memories")
    return LocalMemoryStore(base_path=memory_dir)
```

### å·¥å‚ç±»æ›´æ–°

```python
# src/ai/deep_analysis_factory.py (ä¿®æ­£ç‰ˆ)

from ..memory import create_memory_backend, MemoryToolHandler

class DeepAnalysisEngineFactory:
    @staticmethod
    def create(provider: str, config: Any) -> Optional[DeepAnalysisEngine]:
        """
        åˆ›å»ºæ·±åº¦åˆ†æå¼•æ“

        æ³¨æ„ï¼šä¸å†ç›´æ¥ä¼ å…¥ memory_handlerï¼Œè€Œæ˜¯ä» config åˆ›å»º
        """
        # ç»Ÿä¸€åˆ›å»º Memory Backendï¼ˆæ”¯æŒ Local/Supabase/Hybridï¼‰
        memory_backend = create_memory_backend(config)
        memory_handler = MemoryToolHandler(
            base_path=getattr(config, "MEMORY_DIR", "./memories"),
            backend=memory_backend
        )

        provider = provider.lower().strip()

        if provider == "claude":
            return DeepAnalysisEngineFactory._create_claude_engine(
                config, memory_handler
            )
        elif provider == "gemini":
            return DeepAnalysisEngineFactory._create_gemini_engine(
                config, memory_handler
            )
        else:
            logger.warning(f"æœªçŸ¥å¼•æ“: {provider}")
            return None
```

### Backend åˆå§‹åŒ–æµç¨‹

```
Config
  â”œâ”€ MEMORY_BACKEND=local    â†’ LocalMemoryStore
  â”œâ”€ MEMORY_BACKEND=supabase â†’ SupabaseMemoryRepository
  â””â”€ MEMORY_BACKEND=hybrid   â†’ HybridMemoryRepository
         â””â”€ remote: Supabase
         â””â”€ local: LocalMemoryStore (fallback)

MemoryToolHandler(backend=...)
  â””â”€ ç»Ÿä¸€æ¥å£ï¼Œè°ƒç”¨ backend.store() / backend.query()
```

---

## é—®é¢˜ 4: ç»Ÿä¸€ Prompt å’Œè§£æé€»è¾‘

### é—®é¢˜
ä¸¤ç§å¼•æ“å¿…é¡»ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ prompt æ¨¡æ¿å’Œ JSON schemaï¼Œå¦åˆ™è¾“å‡ºæ ¼å¼ä¼šåˆ†å‰ã€‚

### è§£å†³æ–¹æ¡ˆï¼šæŠ½è±¡åŸºç±»æä¾›ç»Ÿä¸€æ–¹æ³•

```python
# src/ai/deep_analysis_engine.py (å®Œæ•´ç‰ˆ)

import json
from abc import ABC, abstractmethod
from typing import Dict, Any
from .signal_engine import EventPayload, SignalResult

class DeepAnalysisEngine(ABC):
    """æ·±åº¦åˆ†æå¼•æ“æŠ½è±¡åŸºç±»"""

    @abstractmethod
    async def analyze(
        self,
        payload: EventPayload,
        initial_result: SignalResult
    ) -> SignalResult:
        """æ‰§è¡Œæ·±åº¦åˆ†æ"""
        pass

    @abstractmethod
    def supports_memory(self) -> bool:
        """æ˜¯å¦æ”¯æŒè®°å¿†ç®¡ç†"""
        pass

    # ========== ç»Ÿä¸€çš„ Prompt æ„å»ºæ–¹æ³• ==========

    def build_deep_analysis_prompt(
        self,
        payload: EventPayload,
        initial_result: SignalResult
    ) -> Dict[str, str]:
        """
        æ„å»ºæ·±åº¦åˆ†æ Promptï¼ˆClaude å’Œ Gemini é€šç”¨ï¼‰

        Returns:
            {"system": "...", "user": "..."}
        """
        # åªä¿ç•™æœ€ç›¸å…³çš„å†å²è®°å½•ï¼ˆå‰2æ¡ï¼‰
        historical_ref = payload.historical_reference or {}
        historical_entries = historical_ref.get("entries", [])
        if len(historical_entries) > 2:
            historical_ref = {"entries": historical_entries[:2]}

        context = {
            "text": payload.translated_text or payload.text,
            "source": payload.source,
            "timestamp": payload.timestamp.isoformat(),
            "historical_reference": historical_ref,
            "initial_analysis": {
                "summary": initial_result.summary,
                "event_type": initial_result.event_type,
                "asset": initial_result.asset,
                "action": initial_result.action,
                "confidence": initial_result.confidence,
                "risk_flags": initial_result.risk_flags,
                "notes": initial_result.notes,
            },
        }

        context_json = json.dumps(context, ensure_ascii=False, indent=2)

        system_prompt = (
            "ä½ æ˜¯åŠ å¯†äº¤æ˜“å°çš„èµ„æ·±åˆ†æå¸ˆï¼Œè´Ÿè´£éªŒè¯å’Œä¼˜åŒ– AI åˆæ­¥åˆ†æç»“æœã€‚\n\n"
            "**ä»»åŠ¡**ï¼š\n"
            "1. éªŒè¯äº‹ä»¶ç±»å‹ã€èµ„äº§è¯†åˆ«ã€ç½®ä¿¡åº¦æ˜¯å¦åˆç†\n"
            "2. ç»“åˆå†å²æ¡ˆä¾‹åˆ¤æ–­å½“å‰äº‹ä»¶çš„ç‹¬ç‰¹æ€§\n"
            "3. è¯„ä¼°é£é™©ç‚¹ï¼ˆæµåŠ¨æ€§ã€ç›‘ç®¡ã€å¸‚åœºæƒ…ç»ªï¼‰\n"
            "4. è°ƒæ•´ç½®ä¿¡åº¦å¹¶ç»™å‡ºæ“ä½œå»ºè®®\n\n"
            "**è¾“å‡ºæ ¼å¼**ï¼šä¸¥æ ¼çš„ JSONï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n"
            "```json\n"
            "{\n"
            '  "summary": "string (ç®€ä½“ä¸­æ–‡ï¼Œ1-2å¥è¯)",\n'
            '  "event_type": "listing|delisting|hack|regulation|funding|whale|liquidation|partnership|product_launch|governance|macro|celebrity|airdrop|other",\n'
            '  "asset": "string (å¤§å†™å¸ç§ä»£ç ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œå¦‚ BTC,ETHï¼›è‹¥æ— æ˜ç¡®åŠ å¯†èµ„äº§åˆ™ä¸º NONE)",\n'
            '  "asset_name": "string (èµ„äº§åç§°ï¼Œç®€ä½“ä¸­æ–‡)",\n'
            '  "action": "buy|sell|observe",\n'
            '  "direction": "long|short|neutral",\n'
            '  "confidence": 0.0-1.0 (ä¿ç•™ä¸¤ä½å°æ•°),\n'
            '  "strength": "low|medium|high",\n'
            '  "risk_flags": ["price_volatility", "liquidity_risk", "regulation_risk", "confidence_low", "data_incomplete"],\n'
            '  "notes": "string (ä¿®æ­£ç†ç”±æˆ–ç–‘ç‚¹ï¼Œç®€ä½“ä¸­æ–‡)",\n'
            '  "links": ["string (ç›¸å…³é“¾æ¥)"]\n'
            "}\n"
            "```\n\n"
            "**é‡è¦**ï¼š\n"
            "- è‹¥åˆæ­¥åˆ†ææœ‰è¯¯ï¼ˆå¦‚è¯¯åˆ¤èµ„äº§ã€ç½®ä¿¡åº¦è¿‡é«˜ï¼‰ï¼Œåœ¨ notes ä¸­è¯´æ˜ä¿®æ­£ç†ç”±\n"
            "- ç¦æ­¢è¿”å› Markdownã€é¢å¤–æ–‡æœ¬æˆ–è§£é‡Šï¼Œä»…è¿”å›çº¯ JSON\n"
            "- æ‰€æœ‰å­—ç¬¦ä¸²å­—æ®µä½¿ç”¨ç®€ä½“ä¸­æ–‡"
        )

        user_prompt = (
            "è¯·åˆ†æä»¥ä¸‹äº‹ä»¶å¹¶è¿”å›ä¼˜åŒ–åçš„ JSONï¼š\n"
            f"```json\n{context_json}\n```"
        )

        return {
            "system": system_prompt,
            "user": user_prompt
        }

    # ========== ç»Ÿä¸€çš„å“åº”è§£ææ–¹æ³• ==========

    def parse_response(self, raw_text: str) -> SignalResult:
        """
        è§£æ AI å“åº”ï¼ˆClaude å’Œ Gemini é€šç”¨ï¼‰

        Args:
            raw_text: AI è¿”å›çš„åŸå§‹æ–‡æœ¬

        Returns:
            SignalResult
        """
        # å¤ç”¨ AiSignalEngine çš„è§£æé€»è¾‘
        from .signal_engine import AiSignalEngine

        # åˆ›å»ºä¸´æ—¶å“åº”å¯¹è±¡
        class TempResponse:
            def __init__(self, text):
                self.text = text

        # ä½¿ç”¨ç°æœ‰çš„ _parse_response æ–¹æ³•
        # æ³¨æ„ï¼šéœ€è¦å°† AiSignalEngine._parse_response æ”¹ä¸º @staticmethod
        # æˆ–è€…ç›´æ¥åœ¨è¿™é‡Œå®ç°å®Œæ•´çš„è§£æé€»è¾‘

        normalized_text = self._prepare_json_text(raw_text)

        try:
            data = json.loads(normalized_text)
        except json.JSONDecodeError:
            logger.warning(f"æ·±åº¦åˆ†æè¿”å›é JSON: {normalized_text[:100]}")
            return SignalResult(
                status="error",
                summary="æ·±åº¦åˆ†æè¿”å›æ ¼å¼å¼‚å¸¸",
                confidence=0.0,
                error="JSON è§£æå¤±è´¥"
            )

        # æå–å­—æ®µï¼ˆä¸ AiSignalEngine._parse_response ä¿æŒä¸€è‡´ï¼‰
        return SignalResult(
            status="success",
            summary=str(data.get("summary", "")).strip(),
            event_type=str(data.get("event_type", "other")).lower(),
            asset=str(data.get("asset", "NONE")).upper().strip(),
            asset_names=str(data.get("asset_name", "")).strip(),
            action=str(data.get("action", "observe")).lower(),
            direction=str(data.get("direction", "neutral")).lower(),
            confidence=max(0.0, min(1.0, float(data.get("confidence", 0.0)))),
            strength=str(data.get("strength", "low")).lower(),
            risk_flags=data.get("risk_flags", []),
            notes=str(data.get("notes", "")).strip(),
            links=data.get("links", []),
            raw_response=raw_text
        )

    @staticmethod
    def _prepare_json_text(text: str) -> str:
        """å»é™¤ Markdown ä»£ç å—æ ‡è®°"""
        candidate = text.strip()
        if candidate.startswith("```") and candidate.endswith("```"):
            candidate = candidate[3:-3].strip()
        if candidate.lower().startswith("json"):
            candidate = candidate[4:].strip()
        return candidate
```

### å­ç±»å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
# src/ai/claude_deep_engine.py

class ClaudeDeepAnalysisEngine(DeepAnalysisEngine):
    async def analyze(self, payload, initial_result):
        # ä½¿ç”¨åŸºç±»çš„ç»Ÿä¸€ prompt
        prompt_dict = self.build_deep_analysis_prompt(payload, initial_result)

        # è½¬æ¢ä¸º Claude messages æ ¼å¼
        messages = [
            {"role": "system", "content": prompt_dict["system"]},
            {"role": "user", "content": prompt_dict["user"]}
        ]

        # è°ƒç”¨ Claudeï¼ˆè‡ªåŠ¨æ‰§è¡Œ Memory Toolï¼‰
        response = await self._client.generate_signal(
            json.dumps(messages),  # Claude æœŸæœ› JSON å­—ç¬¦ä¸²
            max_tokens=4096
        )

        # ä½¿ç”¨åŸºç±»çš„ç»Ÿä¸€è§£æ
        return self.parse_response(response.text)
```

```python
# src/ai/gemini_deep_engine.py

class GeminiDeepAnalysisEngine(DeepAnalysisEngine):
    async def analyze(self, payload, initial_result):
        # ä½¿ç”¨åŸºç±»çš„ç»Ÿä¸€ prompt
        prompt_dict = self.build_deep_analysis_prompt(payload, initial_result)

        # è½¬æ¢ä¸º Gemini contents æ ¼å¼
        contents = [
            {"role": "user", "parts": [{"text": f"{prompt_dict['system']}\n\n{prompt_dict['user']}"}]}
        ]

        # æ‰§è¡Œ Function Calling å¾ªç¯
        tools = [self._build_memory_tool_schema()]
        final_text = await self._run_function_calling_loop(contents, tools)

        # ä½¿ç”¨åŸºç±»çš„ç»Ÿä¸€è§£æ
        return self.parse_response(final_text)
```

---

## é—®é¢˜ 5: é…ç½®å‘åå…¼å®¹å’Œè¿ç§»ç­–ç•¥

### é—®é¢˜
æ–°å¢ `DEEP_ANALYSIS_ENABLED` å’Œ `DEEP_ANALYSIS_PROVIDER`ï¼Œå¦‚ä½•ä¸æ—§çš„ `CLAUDE_ENABLED` å¹¶å­˜ï¼Ÿ

### è§£å†³æ–¹æ¡ˆï¼šåˆ†é˜¶æ®µè¿ç§»ç­–ç•¥

#### Phase 1: å…¼å®¹æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰

```python
# src/config.py

class Config:
    # ========== æ—§é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰==========
    CLAUDE_ENABLED: bool = _as_bool(os.getenv("CLAUDE_ENABLED", "false"))

    # ========== æ–°é…ç½®ï¼ˆæ¨èï¼‰==========
    DEEP_ANALYSIS_ENABLED: bool = _as_bool(
        os.getenv("DEEP_ANALYSIS_ENABLED", os.getenv("CLAUDE_ENABLED", "false"))
        # å¦‚æœæœªè®¾ç½® DEEP_ANALYSIS_ENABLEDï¼Œå›é€€åˆ° CLAUDE_ENABLED
    )

    DEEP_ANALYSIS_PROVIDER: str = os.getenv(
        "DEEP_ANALYSIS_PROVIDER",
        "claude" if _as_bool(os.getenv("CLAUDE_ENABLED", "false")) else "gemini"
        # å¦‚æœ CLAUDE_ENABLED=trueï¼Œé»˜è®¤ç”¨ claudeï¼›å¦åˆ™ç”¨ gemini
    ).lower()

    @classmethod
    def get_deep_analysis_config(cls) -> Dict[str, Any]:
        """
        è·å–æ·±åº¦åˆ†æé…ç½®ï¼ˆç»Ÿä¸€å…¥å£ï¼Œå¤„ç†å…¼å®¹æ€§ï¼‰

        è¿”å›:
            {
                "enabled": bool,
                "provider": "claude" | "gemini"
            }
        """
        # ä¼˜å…ˆçº§ï¼šDEEP_ANALYSIS_ENABLED > CLAUDE_ENABLED
        enabled = cls.DEEP_ANALYSIS_ENABLED
        provider = cls.DEEP_ANALYSIS_PROVIDER

        # å…¼å®¹æ€§è­¦å‘Š
        if cls.CLAUDE_ENABLED and not os.getenv("DEEP_ANALYSIS_ENABLED"):
            logger.warning(
                "âš ï¸ CLAUDE_ENABLED å·²åºŸå¼ƒï¼Œè¯·è¿ç§»åˆ° DEEP_ANALYSIS_ENABLED å’Œ DEEP_ANALYSIS_PROVIDER"
            )

        return {
            "enabled": enabled,
            "provider": provider
        }
```

#### Phase 2: å·¥å‚ç±»ä½¿ç”¨ç»Ÿä¸€é…ç½®

```python
# src/ai/signal_engine.py (from_config æ–¹æ³•)

@classmethod
def from_config(cls, config: Any) -> "AiSignalEngine":
    # ... ä¸» AI åˆå§‹åŒ– ...

    # è·å–æ·±åº¦åˆ†æé…ç½®ï¼ˆè‡ªåŠ¨å¤„ç†å…¼å®¹æ€§ï¼‰
    deep_config = config.get_deep_analysis_config()

    deep_engine = None
    if deep_config["enabled"]:
        deep_engine = DeepAnalysisEngineFactory.create(
            provider=deep_config["provider"],
            config=config
        )

    return cls(
        enabled=True,
        client=client,
        threshold=getattr(config, "AI_SIGNAL_THRESHOLD", 0.0),
        semaphore=asyncio.Semaphore(concurrency),
        provider_label=provider_label,
        deep_analysis_engine=deep_engine,
        high_value_threshold=getattr(config, "HIGH_VALUE_CONFIDENCE_THRESHOLD", 0.75),
    )
```

#### Phase 3: é…ç½®è¿ç§»æŒ‡å—

**æ—§é…ç½®ï¼ˆä»ç„¶æœ‰æ•ˆï¼‰**ï¼š
```bash
# .env (æ—§ç‰ˆ)
CLAUDE_ENABLED=true
CLAUDE_API_KEY=sk-ant-xxx
```

**æ–°é…ç½®ï¼ˆæ¨èï¼‰**ï¼š
```bash
# .env (æ–°ç‰ˆ)
DEEP_ANALYSIS_ENABLED=true
DEEP_ANALYSIS_PROVIDER=gemini  # æˆ– claude
GEMINI_API_KEY=xxx  # å¦‚æœç”¨ gemini
CLAUDE_API_KEY=xxx  # å¦‚æœç”¨ claude
```

**è¿ç§»æ­¥éª¤**ï¼š
1. ç°æœ‰ç”¨æˆ·æ— éœ€ä¿®æ”¹ï¼Œ`CLAUDE_ENABLED=true` ä¼šè‡ªåŠ¨æ˜ å°„ä¸º `DEEP_ANALYSIS_ENABLED=true` + `DEEP_ANALYSIS_PROVIDER=claude`
2. æ–°ç”¨æˆ·ç›´æ¥ä½¿ç”¨æ–°é…ç½®
3. åœ¨ä¸‹ä¸ªå¤§ç‰ˆæœ¬ï¼ˆ2.0ï¼‰åºŸå¼ƒ `CLAUDE_ENABLED`

#### Phase 4: é…ç½®ä¼˜å…ˆçº§è¡¨

| é…ç½®ç»„åˆ | è¡Œä¸º | è­¦å‘Š |
|---------|------|-----|
| `DEEP_ANALYSIS_ENABLED=true` + `DEEP_ANALYSIS_PROVIDER=gemini` | ä½¿ç”¨ Gemini | æ—  |
| `DEEP_ANALYSIS_ENABLED=true` + `DEEP_ANALYSIS_PROVIDER=claude` | ä½¿ç”¨ Claude | æ—  |
| `CLAUDE_ENABLED=true` (æœªè®¾ç½® `DEEP_ANALYSIS_*`) | ä½¿ç”¨ Claude | âš ï¸ æ—§é…ç½®è­¦å‘Š |
| å‡æœªè®¾ç½® | ç¦ç”¨æ·±åº¦åˆ†æ | æ—  |
| `DEEP_ANALYSIS_ENABLED=true` + `CLAUDE_ENABLED=true` (å†²çª) | ä½¿ç”¨ `DEEP_ANALYSIS_*` | âš ï¸ å¿½ç•¥æ—§é…ç½® |

---

## æ€»ç»“ï¼šå…³é”®ä¿®æ”¹æ¸…å•

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ | ä¿®æ”¹æ–‡ä»¶ |
|------|---------|---------|
| 1. GeminiClient æ‰©å±• | åˆ›å»º `GeminiFunctionCallingClient` | `src/ai/gemini_function_client.py` (æ–°å»º) |
| 2. Token çˆ†ç‚¸ | å®ç° `_prune_context` æ–¹æ³• | `src/ai/gemini_deep_engine.py` |
| 3. Backend å…¼å®¹ | åˆ›å»º `create_memory_backend` å·¥å‚ | `src/memory/__init__.py` |
| 4. Prompt ç»Ÿä¸€ | åœ¨åŸºç±»å®ç° `build_deep_analysis_prompt` å’Œ `parse_response` | `src/ai/deep_analysis_engine.py` |
| 5. é…ç½®å…¼å®¹ | æ·»åŠ  `get_deep_analysis_config` æ–¹æ³• | `src/config.py` |

---

## ä¸‹ä¸€æ­¥

å»ºè®®æŒ‰ä»¥ä¸‹é¡ºåºå®æ–½ï¼š
1. âœ… åˆ›å»º `GeminiFunctionCallingClient`ï¼ˆæœ€å…³é”®ï¼‰
2. âœ… å®ç°ç»Ÿä¸€ Prompt å’Œè§£æé€»è¾‘ï¼ˆé¿å…åˆ†å‰ï¼‰
3. âœ… å®ç° Memory Backend ç»Ÿä¸€åˆå§‹åŒ–ï¼ˆä¿è¯ç°æœ‰åŠŸèƒ½ä¸æ–­ï¼‰
4. âœ… å®ç° Gemini ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆé˜²æ­¢ token çˆ†ç‚¸ï¼‰
5. âœ… é…ç½®å…¼å®¹æ€§å¤„ç†ï¼ˆå¹³æ»‘è¿ç§»ï¼‰
