"""Main Telegram listener entrypoint."""

from __future__ import annotations

import asyncio
import base64
import signal
import sys
import re
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

from telethon import TelegramClient, events
from telethon.errors import PhoneCodeInvalidError, SessionPasswordNeededError

from .ai.signal_engine import AiSignalEngine, EventPayload, SignalResult
from .ai.translator import Translator, build_translator_from_config
from .ai.gemini_client import AiServiceError
from .config import Config
from .forwarder import MessageForwarder
from .utils import (
    MessageDeduplicator,
    analyze_event_intensity,
    contains_keywords,
    format_forwarded_message,
    compute_canonical_hash,
    compute_sha256,
    compute_embedding,
    setup_logger,
)
from .db import AiSignalRepository, NewsEventRepository, SupabaseError, get_supabase_client
from .memory import (
    MemoryContext,
    MemoryRepositoryConfig,
    SupabaseMemoryRepository,
    LocalMemoryStore,
    HybridMemoryRepository,
)
from .pipeline import LangGraphMessagePipeline, PipelineDependencies, PipelineResult
from .db.models import AiSignalPayload, NewsEventPayload

logger = setup_logger(__name__)

MAX_INLINE_MEDIA_BYTES = 4_000_000  # 4MB limit for Gemini inline images (20MB total request limit)


class TelegramListener:
    """Set up Telethon client, filter, format, and forward messages."""

    def __init__(self) -> None:
        self.config = Config()
        self.client: TelegramClient | None = None
        self.forwarder: MessageForwarder | None = None
        self.deduplicator = MessageDeduplicator(self.config.DEDUP_WINDOW_HOURS)
        self.translator: Translator | None = None
        self.ai_engine = AiSignalEngine.from_config(self.config)
        self.running = False
        self.pipeline_enabled = getattr(self.config, "USE_LANGGRAPH_PIPELINE", False)
        self.pipeline: LangGraphMessagePipeline | None = None
        self.stats = {
            "total_received": 0,
            "filtered_out": 0,
            "duplicates": 0,
            "dup_memory": 0,
            "dup_hash": 0,
            "dup_semantic": 0,
            "forwarded": 0,
            "errors": 0,
            "ai_processed": 0,
            "ai_actions": 0,
            "ai_errors": 0,
            "ai_skipped": 0,
            "translations": 0,
            "translation_errors": 0,
            "start_time": datetime.now(),
        }
        self.db_enabled = (
            self.config.ENABLE_DB_PERSISTENCE
            and bool(self.config.SUPABASE_URL)
            and bool(self.config.SUPABASE_SERVICE_KEY)
        )
        self._supabase_client = None
        self.news_repository: NewsEventRepository | None = None
        self.signal_repository: AiSignalRepository | None = None
        self.memory_repository: SupabaseMemoryRepository | LocalMemoryStore | HybridMemoryRepository | None = None

    async def initialize(self) -> None:
        """Prepare Telethon client and verify configuration."""
        if not self.config.validate():
            raise ValueError("é…ç½®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")

        Path("./session").mkdir(exist_ok=True)

        self.client = TelegramClient(
            self.config.SESSION_PATH,
            self.config.TG_API_ID,
            self.config.TG_API_HASH,
        )

        self.forwarder = MessageForwarder(
            self.client,
            self.config.TARGET_CHAT_ID,
            self.config.TARGET_CHAT_ID_BACKUP,
            cooldown_seconds=self.config.FORWARD_COOLDOWN_SECONDS,
        )

        if self.db_enabled:
            try:
                self._supabase_client = get_supabase_client(
                    self.config.SUPABASE_URL,
                    self.config.SUPABASE_SERVICE_KEY,
                    timeout=self.config.SUPABASE_TIMEOUT_SECONDS,
                )
                self.news_repository = NewsEventRepository(self._supabase_client)
                self.signal_repository = AiSignalRepository(self._supabase_client)

                # Initialize memory repository based on MEMORY_BACKEND
                if self.config.MEMORY_ENABLED:
                    memory_config = MemoryRepositoryConfig(
                        max_notes=max(1, int(self.config.MEMORY_MAX_NOTES)),
                        similarity_threshold=float(self.config.MEMORY_SIMILARITY_THRESHOLD),
                        lookback_hours=max(1, int(self.config.MEMORY_LOOKBACK_HOURS)),
                        min_confidence=max(0.0, float(self.config.MEMORY_MIN_CONFIDENCE)),
                    )

                    backend = getattr(self.config, "MEMORY_BACKEND", "supabase").lower()
                    memory_dir = getattr(self.config, "MEMORY_DIR", "./memories")

                    if backend == "local":
                        self.memory_repository = LocalMemoryStore(
                            base_path=memory_dir,
                            lookback_hours=memory_config.lookback_hours,
                        )
                        logger.info("ğŸ—„ï¸ Local Memory å·²å¯ç”¨ (å…³é”®è¯åŒ¹é…)")
                    elif backend == "hybrid":
                        supabase_repo = SupabaseMemoryRepository(
                            self._supabase_client,
                            memory_config,
                        )
                        local_store = LocalMemoryStore(
                            base_path=memory_dir,
                            lookback_hours=memory_config.lookback_hours,
                        )
                        self.memory_repository = HybridMemoryRepository(
                            supabase_repo=supabase_repo,
                            local_store=local_store,
                            max_failures=3,
                        )
                        logger.info("ğŸ—„ï¸ Hybrid Memory å·²å¯ç”¨ (Supabase ä¸»åŠ› + Local é™çº§)")
                    else:  # supabase (default)
                        self.memory_repository = SupabaseMemoryRepository(
                            self._supabase_client,
                            memory_config,
                        )
                        logger.info("ğŸ—„ï¸ Supabase Memory å·²å¯ç”¨ (å‘é‡ç›¸ä¼¼åº¦)")

                logger.info("ğŸ—„ï¸ Supabase æŒä¹…åŒ–å·²å¯ç”¨")
            except SupabaseError as exc:
                self.db_enabled = False
                logger.warning("Supabase åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ç¦ç”¨æŒä¹…åŒ–: %s", exc)

        if self.config.TRANSLATION_ENABLED:
            try:
                translator = build_translator_from_config(self.config)
            except AiServiceError as exc:
                logger.warning("ç¿»è¯‘æ¨¡å—åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸæ–‡: %s", exc)
                translator = None

            if translator is not None:
                self.translator = translator
                if not translator.enabled:
                    logger.debug("ç¿»è¯‘æ¨¡å—å·²å¯ç”¨ä½†ç¼ºå°‘æœ‰æ•ˆå‡­æ®ï¼Œæ¶ˆæ¯å°†ä¿æŒåŸæ–‡")
            else:
                self.translator = None

        if self.pipeline_enabled:
            self._initialize_pipeline()

        logger.info("ğŸš€ æ­£åœ¨è¿æ¥åˆ° Telegram...")
        await self.client.start(phone=self.config.TG_PHONE)

        me = await self.client.get_me()
        username = f"@{me.username}" if me.username else me.id
        logger.info(f"âœ… å·²ç™»å½•ä¸º: {me.first_name} ({username})")

        await self._verify_target_channels()
        logger.info(
            "ğŸ“¡ å¼€å§‹ç›‘å¬ %d ä¸ªé¢‘é“...",
            len(self.config.SOURCE_CHANNELS),
        )
        keywords = ", ".join(self.config.FILTER_KEYWORDS) if self.config.FILTER_KEYWORDS else "æ— "
        logger.info("ğŸ¯ è¿‡æ»¤å…³é”®è¯: %s", keywords)

    async def _verify_target_channels(self) -> None:
        if not self.client:
            return

        try:
            target = await self.client.get_entity(self.config.TARGET_CHAT_ID)
            title = getattr(target, "title", None) or getattr(target, "username", "Unknown")
            logger.info(f"âœ… ç›®æ ‡é¢‘é“éªŒè¯æˆåŠŸ: {title}")
        except Exception as exc:  # pylint: disable=broad-except
            logger.warning(f"âš ï¸ ç›®æ ‡é¢‘é“éªŒè¯å¤±è´¥: {exc}")

    def _initialize_pipeline(self) -> None:
        if not self.pipeline_enabled:
            return
        if not self.forwarder:
            logger.warning("ğŸš« æ— æ³•åˆå§‹åŒ– LangGraph ç®¡çº¿ï¼šè½¬å‘å™¨æœªå‡†å¤‡å°±ç»ª")
            return

        dependencies = PipelineDependencies(
            config=self.config,
            deduplicator=self.deduplicator,
            translator=self.translator,
            ai_engine=self.ai_engine,
            forwarder=self.forwarder,
            news_repository=self.news_repository,
            signal_repository=self.signal_repository,
            memory_repository=self.memory_repository,
            db_enabled=self.db_enabled,
            stats=self.stats,
            logger=logger,
            collect_keywords=self._collect_keywords,
            extract_media=self._extract_media,
            build_ai_kwargs=self._build_ai_kwargs,
            should_include_original=self._should_include_original,
            append_links=self._append_links,
            collect_links=self._collect_links,
            persist_event=self._persist_event,
            update_ai_stats=self._update_ai_stats,
        )
        self.pipeline = LangGraphMessagePipeline(dependencies)
        logger.info("ğŸ§­ LangGraph ç®¡çº¿å·²å¯ç”¨")

    async def start_listening(self) -> None:
        """Register handlers and start event loop."""
        if not self.client or not self.forwarder:
            raise RuntimeError("Client not initialized")

        self.running = True

        @self.client.on(events.NewMessage(chats=self.config.SOURCE_CHANNELS))
        async def message_handler(event):  # type: ignore[no-redef]
            await self._handle_new_message(event)

        logger.info("ğŸ§ æ¶ˆæ¯ç›‘å¬å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢...")

        def signal_handler(signum, frame):  # pylint: disable=unused-argument
            logger.info("ğŸ“¡ æ¥æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­...")
            self.running = False
            if self.client:
                client_loop = getattr(self.client, "loop", None)
                try:
                    if client_loop and client_loop.is_running():
                        client_loop.call_soon_threadsafe(self.client.disconnect)
                    else:
                        self.client.disconnect()
                except RuntimeError:
                    logger.debug("Telethon äº‹ä»¶å¾ªç¯å·²å…³é—­ï¼Œè·³è¿‡é¢å¤– disconnect è°ƒç”¨", exc_info=True)

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

        asyncio.create_task(self._stats_reporter())

        try:
            await self.client.run_until_disconnected()
        except KeyboardInterrupt:
            logger.info("ğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        finally:
            await self._cleanup()

    async def _handle_new_message(self, event) -> None:
        if self.pipeline_enabled and self.pipeline:
            try:
                result = await self.pipeline.run(event)
            except Exception as exc:  # pylint: disable=broad-except
                logger.error("LangGraph ç®¡çº¿æ‰§è¡Œå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæµç¨‹: %s", exc, exc_info=True)
                await self._handle_new_message_legacy(event)
                return

            self._log_pipeline_result(result)
            return

        await self._handle_new_message_legacy(event)

    def _log_pipeline_result(self, result: PipelineResult) -> None:
        if result.status == "dropped":
            logger.debug("LangGraph ç®¡çº¿ä¸¢å¼ƒæ¶ˆæ¯: reason=%s", result.drop_reason)
        elif result.status == "forwarded":
            logger.debug("LangGraph ç®¡çº¿è½¬å‘å®Œæˆ")
        elif result.status == "processed":
            logger.debug("LangGraph ç®¡çº¿å¤„ç†å®Œæˆï¼Œæ— éœ€è½¬å‘")
        else:
            logger.debug(
                "LangGraph ç®¡çº¿å®Œæˆ: status=%s forwarded=%s reason=%s",
                result.status,
                result.forwarded,
                result.drop_reason,
            )

    async def _handle_new_message_legacy(self, event) -> None:
        try:
            self.stats["total_received"] += 1

            message_text = event.message.text or ""
            if not message_text.strip():
                return

            source_chat = await event.get_chat()
            source_name = (
                getattr(source_chat, "title", None)
                or getattr(source_chat, "username", None)
                or str(getattr(source_chat, "id", "Unknown"))
            )

            source_message_id = str(getattr(event.message, "id", ""))
            published_at = getattr(event.message, "date", None) or datetime.now(timezone.utc)
            channel_username = getattr(source_chat, "username", None)
            source_url = (
                f"https://t.me/{channel_username}/{source_message_id}"
                if channel_username and source_message_id
                else None
            )

            logger.debug("ğŸ“¨ æ”¶åˆ°æ¶ˆæ¯æ¥è‡ª %s (é•¿åº¦: %d): %.300s...", source_name, len(message_text), message_text)

            if not contains_keywords(message_text, self.config.FILTER_KEYWORDS):
                self.stats["filtered_out"] += 1
                logger.debug("ğŸš« æ¶ˆæ¯è¢«å…³é”®è¯è¿‡æ»¤å™¨æ‹’ç»")
                return

            if self.deduplicator.is_duplicate(message_text):
                self.stats["duplicates"] += 1
                self.stats["dup_memory"] += 1
                logger.debug("ğŸ”„ é‡å¤æ¶ˆæ¯ï¼Œå·²è·³è¿‡")
                return

            if not self.forwarder:
                logger.error("è½¬å‘å™¨æœªåˆå§‹åŒ–ï¼Œæ¶ˆæ¯è¢«ä¸¢å¼ƒ")
                return

            event_time = datetime.now()

            hash_raw = compute_sha256(message_text)
            hash_canonical = compute_canonical_hash(message_text)
            embedding_vector: list[float] | None = None

            if self.db_enabled and self.news_repository and hash_raw:
                try:
                    existing_event_id = await self.news_repository.check_duplicate(hash_raw)
                except Exception as exc:  # pylint: disable=broad-except
                    logger.warning("å“ˆå¸Œå»é‡æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­å¤„ç†: %s", exc)
                else:
                    if existing_event_id:
                        self.stats["duplicates"] += 1
                        self.stats["dup_hash"] += 1
                        logger.debug(
                            "ğŸ” æ•°æ®åº“å“ˆå¸Œå»é‡å‘½ä¸­: event_id=%s", existing_event_id
                        )
                        return

            if (
                self.db_enabled
                and self.news_repository
                and self.config.OPENAI_API_KEY
            ):
                embedding_vector = await compute_embedding(
                    message_text,
                    api_key=self.config.OPENAI_API_KEY,
                    model=self.config.OPENAI_EMBEDDING_MODEL,
                )
                if embedding_vector:
                    intensity = analyze_event_intensity(
                        message_text,
                        translated_text or "",
                    )
                    threshold = self.config.EMBEDDING_SIMILARITY_THRESHOLD
                    time_window_hours = self.config.EMBEDDING_TIME_WINDOW_HOURS
                    if intensity["has_high_impact"]:
                        threshold = max(threshold, 0.95)
                        time_window_hours = min(time_window_hours, 3)
                        logger.debug(
                            "âš ï¸ é«˜å½±å“äº‹ä»¶å¯ç”¨å®½æ¾è¯­ä¹‰å»é‡: threshold=%.2f window=%sh",
                            threshold,
                            time_window_hours,
                        )
                    threshold = max(0.0, min(1.0, threshold))
                    time_window_hours = max(1, int(time_window_hours))
                    try:
                        similar = await self.news_repository.check_duplicate_by_embedding(
                            embedding=embedding_vector,
                            threshold=threshold,
                            time_window_hours=time_window_hours,
                        )
                    except Exception as exc:  # pylint: disable=broad-except
                        logger.warning("è¯­ä¹‰å»é‡æ£€æŸ¥å¤±è´¥ï¼Œç»§ç»­å¤„ç†: %s", exc)
                    else:
                        if similar:
                            self.stats["duplicates"] += 1
                            self.stats["dup_semantic"] += 1
                            logger.info(
                                "ğŸ” è¯­ä¹‰å»é‡å‘½ä¸­: event_id=%s similarity=%.3f",
                                similar["id"],
                                similar["similarity"],
                            )
                            return

            translated_text = None
            language = "unknown"
            translation_confidence = 0.0
            keywords_hit = self._collect_keywords(message_text)
            media_payload: list[dict[str, Any]] = []

            if self.translator:
                try:
                    translation = await self.translator.translate(message_text)
                    language = translation.language or "unknown"
                    translation_confidence = translation.confidence
                    translated_text = translation.text
                    if translation.translated:
                        self.stats["translations"] += 1
                except AiServiceError as exc:
                    self.stats["translation_errors"] += 1
                    logger.warning("ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸæ–‡: %s", exc)

            if translated_text and translated_text != message_text:
                keywords_hit = self._collect_keywords(message_text, translated_text)

            memory_context: MemoryContext | None = None
            signal_result: SignalResult | None = None
            if self.ai_engine:
                media_payload = await self._extract_media(event.message)
                historical_reference_entries: list[dict[str, str | float]]
                if self.config.MEMORY_ENABLED and self.memory_repository:
                    try:
                        # Different backends require different inputs
                        backend_type = type(self.memory_repository).__name__
                        logger.debug(
                            "ğŸ§  è®°å¿†æ£€ç´¢å¼€å§‹: backend=%s keywords=%s",
                            backend_type,
                            keywords_hit,
                        )

                        if isinstance(self.memory_repository, LocalMemoryStore):
                            # Local: keyword-based, no embedding needed
                            memory_entries = self.memory_repository.load_entries(
                                keywords=keywords_hit,
                                limit=self.config.MEMORY_MAX_NOTES,
                                min_confidence=self.config.MEMORY_MIN_CONFIDENCE,
                            )
                            memory_context = MemoryContext(entries=memory_entries)
                            logger.info(
                                "ğŸ§  Local Memory æ£€ç´¢å®Œæˆ: æ‰¾åˆ° %d æ¡è®°å½•",
                                len(memory_entries),
                            )
                        elif isinstance(self.memory_repository, HybridMemoryRepository):
                            # Hybrid: try Supabase (embedding), fallback to Local (keywords)
                            memory_context = await self.memory_repository.fetch_memories(
                                embedding=embedding_vector,
                                asset_codes=None,
                                keywords=keywords_hit,
                            )
                            logger.info(
                                "ğŸ§  Hybrid Memory æ£€ç´¢å®Œæˆ: æ‰¾åˆ° %d æ¡è®°å½•",
                                len(memory_context.entries) if memory_context else 0,
                            )
                        else:
                            # Supabase: vector similarity (requires embedding)
                            if embedding_vector:
                                memory_context = await self.memory_repository.fetch_memories(
                                    embedding=embedding_vector,
                                    asset_codes=None,
                                )
                                logger.info(
                                    "ğŸ§  Supabase Memory æ£€ç´¢å®Œæˆ: æ‰¾åˆ° %d æ¡è®°å½•",
                                    len(memory_context.entries) if memory_context else 0,
                                )
                            else:
                                memory_context = None
                                logger.debug("ğŸ§  æ—  embeddingï¼Œè·³è¿‡ Supabase è®°å¿†æ£€ç´¢")
                    except (SupabaseError, Exception) as exc:
                        logger.warning("è®°å¿†æ£€ç´¢å¤±è´¥ï¼Œè·³è¿‡å†å²å‚è€ƒ: %s", exc)
                        memory_context = None
                if memory_context and not memory_context.is_empty():
                    historical_reference_entries = memory_context.to_prompt_payload()
                    logger.info(
                        "ğŸ§  è®°å¿†æ³¨å…¥ Prompt: %d æ¡å†å²å‚è€ƒ",
                        len(historical_reference_entries),
                    )
                    # è¯¦ç»†æ˜¾ç¤ºæ¯æ¡è®°å¿†çš„å†…å®¹
                    if logger.isEnabledFor(10):  # DEBUG level
                        logger.debug("ğŸ“š è®°å¿†è¯¦æƒ…ï¼ˆå®Œæ•´ï¼‰:")
                        for i, entry in enumerate(memory_context.entries, 1):
                            logger.debug(
                                f"  [{i}] ID={entry.id[:8]}... assets={entry.assets} "
                                f"action={entry.action} confidence={entry.confidence:.2f} "
                                f"similarity={entry.similarity:.2f} time={entry.created_at.strftime('%Y-%m-%d %H:%M:%S')}"
                            )
                            logger.debug(f"      æ‘˜è¦: {entry.summary}")
                    else:
                        # INFO level: åªæ˜¾ç¤ºç®€çŸ­ç»Ÿè®¡
                        for i, entry in enumerate(memory_context.entries, 1):
                            logger.info(
                                f"  [{i}] {entry.assets} {entry.action} "
                                f"(conf={entry.confidence:.2f}, sim={entry.similarity:.2f})"
                            )
                else:
                    historical_reference_entries = []
                    logger.debug("ğŸ§  æ— å†å²è®°å¿†ï¼Œä½¿ç”¨ç©ºä¸Šä¸‹æ–‡")
                payload = EventPayload(
                    text=message_text,
                    source=source_name,
                    timestamp=event_time,
                    translated_text=translated_text,
                    language=language,
                    translation_confidence=translation_confidence,
                    keywords_hit=keywords_hit,
                    historical_reference=(
                        {
                            "entries": historical_reference_entries,
                            "enabled": True,
                        }
                        if self.config.MEMORY_ENABLED
                        else {}
                    ),
                    media=media_payload,
                )
                signal_result = await self.ai_engine.analyse(payload)
                if signal_result:
                    self._update_ai_stats(signal_result)

            should_skip_forward = False
            if signal_result and signal_result.status != "error":
                low_confidence_skip = signal_result.confidence < 0.4
                neutral_skip = (
                    self.config.AI_SKIP_NEUTRAL_FORWARD
                    and signal_result.status == "skip"
                    and signal_result.summary != "AI disabled"
                )
                # äºŒæ¬¡è¿‡æ»¤ï¼šè§‚æœ›ç±»ä¿¡å·ä¸”ç½®ä¿¡åº¦ < 0.85 ä¸è½¬å‘ï¼ˆé™ä½å™ªéŸ³ï¼‰
                low_value_observe = (
                    signal_result.action == "observe"
                    and signal_result.confidence < 0.85
                )
                if low_confidence_skip or neutral_skip or low_value_observe:
                    should_skip_forward = True
                    self.stats["ai_skipped"] += 1
                    reason = "ä½ä»·å€¼è§‚æœ›ä¿¡å·" if low_value_observe else "ä½ä¼˜å…ˆçº§"
                    logger.info(
                        "ğŸ¤– AI è¯„ä¼°ä¸º%sï¼Œè·³è¿‡è½¬å‘: source=%s action=%s confidence=%.2f",
                        reason,
                        source_name,
                        signal_result.action,
                        signal_result.confidence,
                    )

            if should_skip_forward:
                await self._persist_event(
                    source_name,
                    message_text,
                    translated_text,
                    signal_result,
                    False,
                    source_message_id=source_message_id,
                    source_url=source_url,
                    published_at=published_at,
                    processed_at=event_time,
                    language=language,
                    keywords_hit=keywords_hit,
                    translation_confidence=translation_confidence,
                    media_refs=media_payload,
                    hash_raw=hash_raw,
                    hash_canonical=hash_canonical,
                    embedding=embedding_vector,
                )
                return

            ai_kwargs = self._build_ai_kwargs(signal_result, source_name)
            if not ai_kwargs:
                # AI åˆ†ææœªè¿”å›æˆåŠŸç»“æœï¼ˆå¯èƒ½æ˜¯ asset=NONE æˆ–å…¶ä»–åŸå› ï¼‰
                self.stats["ai_skipped"] += 1
                reason = (
                    f"status={signal_result.status}"
                    if signal_result and signal_result.status != "success"
                    else "ç¼ºå°‘ AI æ‘˜è¦"
                )
                logger.info(
                    "ğŸ¤– AI åˆ†ææœªé€šè¿‡ï¼Œè·³è¿‡è½¬å‘: source=%s reason=%s",
                    source_name,
                    reason,
                )
                await self._persist_event(
                    source_name,
                    message_text,
                    translated_text,
                    signal_result,
                    False,
                    source_message_id=source_message_id,
                    source_url=source_url,
                    published_at=published_at,
                    processed_at=event_time,
                    language=language,
                    keywords_hit=keywords_hit,
                    translation_confidence=translation_confidence,
                    media_refs=media_payload,
                    hash_raw=hash_raw,
                    hash_canonical=hash_canonical,
                    embedding=embedding_vector,
                )
                return

            show_original = self._should_include_original(
                original_text=message_text,
                translated_text=translated_text,
                signal_result=signal_result,
            )

            formatted_message = format_forwarded_message(
                source_channel=source_name,
                timestamp=event_time,
                translated_text=translated_text,
                original_text=message_text,
                show_original=show_original,
                show_translation=self.config.FORWARD_INCLUDE_TRANSLATION,
                **ai_kwargs,
            )
            links: list[str] = []
            if signal_result:
                links = self._collect_links(
                    signal_result,
                    formatted_message,
                    translated_text,
                    message_text,
                )
            if links:
                formatted_message = self._append_links(
                    formatted_message,
                    links,
                )

            success = await self.forwarder.forward_message(
                formatted_message,
                link_preview=False,
            )
            if success:
                self.stats["forwarded"] += 1
                logger.info("ğŸ“¤ å·²è½¬å‘æ¥è‡ª %s çš„æ¶ˆæ¯", source_name)
            else:
                self.stats["errors"] += 1
                logger.error("âŒ æ¶ˆæ¯è½¬å‘å¤±è´¥")

            await self._persist_event(
                source_name,
                message_text,
                translated_text,
                signal_result,
                success,
                source_message_id=source_message_id,
                source_url=source_url,
                published_at=published_at,
                processed_at=event_time,
                language=language,
                keywords_hit=keywords_hit,
                translation_confidence=translation_confidence,
                media_refs=media_payload,
                hash_raw=hash_raw,
                hash_canonical=hash_canonical,
                embedding=embedding_vector,
            )
        except Exception as exc:  # pylint: disable=broad-except
            self.stats["errors"] += 1
            logger.error(f"âŒ å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {exc}")

    def _append_links(self, message: str, links: list[str]) -> str:
        if not links:
            return message
        normalized = [link for link in links if link]
        if not normalized:
            return message
        link_lines = "\n".join(f"source: {link}" for link in normalized)
        return f"{message}\n{link_lines}"

    def _collect_links(
        self,
        signal_result: SignalResult,
        formatted_message: str,
        translated_text: str | None,
        original_text: str | None,
    ) -> list[str]:
        candidate_sources = [
            "\n".join(signal_result.links),
            signal_result.summary,
            signal_result.notes,
            translated_text,
            original_text,
            formatted_message,
        ]
        seen: set[str] = set()
        collected: list[str] = []
        for source in candidate_sources:
            for link in self._extract_links(source):
                if link not in seen:
                    seen.add(link)
                    collected.append(link)
        return collected

    @staticmethod
    def _extract_links(text: str | None) -> list[str]:
        if not text:
            return []
        # Match URLs, excluding common Markdown artifacts
        pattern = re.compile(r"https?://[\w\-._~:/?#\[\]@!$&'()*+,;=%]+", re.IGNORECASE)
        matches = pattern.findall(text)
        normalized = []
        for match in matches:
            # Strip Markdown link syntax residue: ](url) or [text](url)
            cleaned = match.lstrip('](').rstrip(r'.,!?)\]"\'')
            # Skip if still contains invalid prefixes
            if cleaned and not cleaned.startswith((']', '[')):
                normalized.append(cleaned)
        return normalized

    def _build_ai_kwargs(
        self,
        signal_result: SignalResult | None,
        source: str,
    ) -> dict[str, object]:
        if not signal_result:
            logger.debug("AI ç»“æœä¸ºç©ºï¼Œsource=%s", source)
            return {}

        if signal_result.status == "error":
            logger.warning(
                "AI åˆ†æè¿”å›é”™è¯¯çŠ¶æ€ï¼Œsource=%s error=%s",
                source,
                signal_result.error or "unknown",
            )
            return {}

        if signal_result.status != "success":
            logger.debug(
                "AI çŠ¶æ€ä¸º %sï¼ŒéæˆåŠŸç»“æœï¼Œsource=%s",
                signal_result.status,
                source,
            )
            return {}

        if not signal_result.summary:
            raw_preview = (signal_result.raw_response or "").strip()
            if len(raw_preview) > 160:
                raw_preview = raw_preview[:157] + "..."
            logger.info(
                "AI è¿”å›ç¼ºå°‘æ‘˜è¦ï¼Œsource=%s action=%s raw=%s",
                source,
                signal_result.action,
                raw_preview,
            )
            return {}

        return {
            "ai_summary": signal_result.summary,
            "ai_action": signal_result.action,
            "ai_direction": signal_result.direction,
            "ai_event_type": signal_result.event_type,
            "ai_asset": signal_result.asset,
            "ai_asset_names": signal_result.asset_names,
            "ai_confidence": signal_result.confidence,
            "ai_strength": signal_result.strength,
            "ai_timeframe": signal_result.timeframe,
            "ai_risk_flags": signal_result.risk_flags,
            "ai_notes": signal_result.notes,
            "ai_alert": signal_result.alert or None,
            "ai_severity": signal_result.severity or None,
        }

    def _should_include_original(
        self,
        *,
        original_text: str | None,
        translated_text: str | None,
        signal_result: SignalResult | None,
    ) -> bool:
        if not original_text or not original_text.strip():
            return False

        # æ—  AI ç»“æœæˆ–çŠ¶æ€å¼‚å¸¸ï¼Œä¿ç•™åŸæ–‡ä¾¿äºäººå·¥åˆ¤æ–­
        if not signal_result or signal_result.status != "success":
            return True

        # èµ„äº§æœªè¯†åˆ«æˆ–ä¿¡å·ç½®ä¿¡åº¦åä½ï¼Œå±•ç¤ºåŸæ–‡ä¾›å¤æ ¸
        asset_code = (signal_result.asset or "").strip().upper()
        if not asset_code or asset_code == "NONE":
            return True
        if signal_result.confidence < 0.4:
            return True

        normalized_flags = {flag for flag in signal_result.risk_flags}
        if {"data_incomplete", "confidence_low"} & normalized_flags:
            return True

        notes = (signal_result.notes or "").strip()
        if notes and "åŸæ–‡" in notes:
            return True

        # è‹¥ç¼ºå°‘è¯‘æ–‡ï¼Œåªåœ¨æ‘˜è¦ä¸­å±•ç¤ºåŸæ–‡å³å¯ï¼Œæ— éœ€é‡å¤
        if not translated_text or translated_text.strip() == "":
            return False

        return False

    def _update_ai_stats(self, signal_result: SignalResult) -> None:
        if signal_result.status == "error":
            self.stats["ai_errors"] += 1
            return
        if signal_result.status == "skip" and signal_result.summary == "AI disabled":
            return
        self.stats["ai_processed"] += 1
        if signal_result.status == "success" and signal_result.should_execute_hot_path:
            self.stats["ai_actions"] += 1

    async def _persist_event(
        self,
        source_name: str,
        original_text: str,
        translated_text: str | None,
        signal_result: SignalResult | None,
        forwarded: bool,
        *,
        source_message_id: str,
        source_url: str | None,
        published_at: datetime,
        processed_at: datetime,
        language: str,
        keywords_hit: list[str],
        translation_confidence: float,
        media_refs: list[dict[str, Any]],
        hash_raw: str | None = None,
        hash_canonical: str | None = None,
        embedding: list[float] | None = None,
    ) -> None:
        if not self.db_enabled or not self.news_repository:
            return

        if not original_text.strip():
            return

        try:
            hash_raw = hash_raw or compute_sha256(original_text)
            hash_canonical = hash_canonical or compute_canonical_hash(original_text)

            embedding_vector = embedding
            if embedding_vector is None and self.config.OPENAI_API_KEY:
                embedding_vector = await compute_embedding(
                    original_text,
                    api_key=self.config.OPENAI_API_KEY,
                    model=self.config.OPENAI_EMBEDDING_MODEL,
                )

            ingest_status = "forwarded" if forwarded else "processed"
            metadata = {
                "forwarded": forwarded,
                "source": source_name,
                "language_detected": language,
                "translation_confidence": translation_confidence,
                "processed_at": processed_at.replace(microsecond=0).isoformat(),
            }
            if embedding_vector:
                metadata["embedding_model"] = self.config.OPENAI_EMBEDDING_MODEL
                metadata["embedding_generated_at"] = datetime.now().isoformat()
            if signal_result:
                metadata.update(
                    {
                        "ai_status": signal_result.status,
                        "ai_confidence": signal_result.confidence,
                        "ai_strength": signal_result.strength,
                        "ai_direction": signal_result.direction,
                        "ai_alert": signal_result.alert,
                        "ai_severity": signal_result.severity,
                    }
                )
                if signal_result.error:
                    metadata["ai_error"] = signal_result.error

            # Level 1: Exact hash dedup
            news_event_id = await self.news_repository.check_duplicate(hash_raw)
            if news_event_id:
                logger.debug("ç²¾ç¡®å»é‡å‘½ä¸­: event_id=%s", news_event_id)
                return

            # Level 2: Semantic embedding dedup
            if embedding_vector:
                intensity = analyze_event_intensity(
                    original_text,
                    translated_text or "",
                )
                threshold = self.config.EMBEDDING_SIMILARITY_THRESHOLD
                time_window_hours = self.config.EMBEDDING_TIME_WINDOW_HOURS
                if intensity["has_high_impact"]:
                    threshold = max(threshold, 0.95)
                    time_window_hours = min(time_window_hours, 3)
                    logger.info(
                        "âš ï¸ é«˜å½±å“äº‹ä»¶å¯ç”¨å®½æ¾è¯­ä¹‰å»é‡: threshold=%.2f window=%sh",
                        threshold,
                        time_window_hours,
                    )
                threshold = max(0.0, min(1.0, threshold))
                time_window_hours = max(1, int(time_window_hours))
                similar = await self.news_repository.check_duplicate_by_embedding(
                    embedding=embedding_vector,
                    threshold=threshold,
                    time_window_hours=time_window_hours,
                )
                if similar:
                    logger.info(
                        "è¯­ä¹‰å»é‡å‘½ä¸­: event_id=%s similarity=%.3f content_preview=%s",
                        similar["id"],
                        similar["similarity"],
                        similar.get("content_text", "")[:50],
                    )
                    return

            if not news_event_id:
                payload = NewsEventPayload(
                    source=source_name,
                    source_message_id=source_message_id,
                    source_url=source_url,
                    published_at=published_at,
                    content_text=original_text,
                    translated_text=translated_text,
                    summary=signal_result.summary if signal_result else None,
                    language=language or "unknown",
                    media_refs=media_refs,
                    hash_raw=hash_raw,
                    hash_canonical=hash_canonical,
                    embedding=embedding_vector,  # Add embedding vector
                    keywords_hit=list(dict.fromkeys(keywords_hit or [])),
                    ingest_status=ingest_status,
                    metadata=metadata,
                )
                news_event_id = await self.news_repository.insert_event(payload)

            if not news_event_id:
                logger.warning("âš ï¸ æœªèƒ½å†™å…¥æ–°é—»äº‹ä»¶åˆ°æ•°æ®åº“")
                return

            if not signal_result or signal_result.status != "success" or not self.signal_repository:
                return

            model_name = self.config.AI_MODEL_NAME or "unknown"
            published_at_aware = (
                published_at if published_at.tzinfo else published_at.replace(tzinfo=timezone.utc)
            )
            processed_at_aware = (
                processed_at if processed_at.tzinfo else processed_at.replace(tzinfo=timezone.utc)
            )
            latency_delta = processed_at_aware - published_at_aware
            latency_ms = max(
                0,
                int(latency_delta.total_seconds() * 1000),
            )

            try:
                confidence_value = float(signal_result.confidence)
            except (TypeError, ValueError):
                logger.warning(
                    "AI ç½®ä¿¡åº¦å€¼æ— æ³•è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œä½¿ç”¨é»˜è®¤ 0.0: %s",
                    signal_result.confidence,
                )
                confidence_value = 0.0

            signal_payload = AiSignalPayload(
                news_event_id=news_event_id,
                model_name=model_name,
                summary_cn=signal_result.summary,
                event_type=signal_result.event_type,
                assets=signal_result.asset,
                asset_names=signal_result.asset_names or None,
                action=signal_result.action,
                direction=signal_result.direction,
                confidence=confidence_value,
                strength=signal_result.strength,
                risk_flags=signal_result.risk_flags,
                notes=signal_result.notes or None,
                links=signal_result.links,
                execution_path="hot" if signal_result.should_execute_hot_path else "cold",
                should_alert=forwarded,
                latency_ms=latency_ms,
                raw_response=signal_result.raw_response or None,
            )
            await self.signal_repository.insert_signal(signal_payload)
        except SupabaseError as exc:
            logger.warning("Supabase å†™å…¥å¤±è´¥: %s", exc)
        except Exception as exc:  # pylint: disable=broad-except
            logger.warning("æŒä¹…åŒ–æµç¨‹å¼‚å¸¸: %s", exc)

    def _collect_keywords(self, *texts: str) -> list[str]:
        hits: list[str] = []
        available = [text for text in texts if text]
        if not available:
            return hits
        for keyword in self.config.FILTER_KEYWORDS:
            if not keyword:
                continue
            lower_kw = keyword.lower()
            for text in available:
                if lower_kw in text.lower():
                    hits.append(keyword)
                    break
        return hits

    async def _extract_media(self, message) -> list[dict[str, Any]]:
        """Download image-like media as base64 for AI prompt consumption."""
        media_payload: list[dict[str, Any]] = []
        if not message or not getattr(message, "media", None):
            return media_payload

        try:
            if getattr(message, "photo", None):
                media_bytes = await message.download_media(bytes)
                if media_bytes and len(media_bytes) <= MAX_INLINE_MEDIA_BYTES:
                    media_payload.append(
                        {
                            "type": "photo",
                            "mime_type": "image/jpeg",
                            "size_bytes": len(media_bytes),
                            "base64": base64.b64encode(media_bytes).decode("ascii"),
                        }
                    )
                elif media_bytes:
                    media_payload.append(
                        {
                            "type": "photo_reference",
                            "mime_type": "image/jpeg",
                            "size_bytes": len(media_bytes),
                            "note": "image too large to inline",
                        }
                    )

            document = getattr(message, "document", None)
            if document:
                mime_type = getattr(document, "mime_type", "") or ""
                if mime_type.startswith("image/"):
                    media_bytes = await message.download_media(bytes)
                    if media_bytes and len(media_bytes) <= MAX_INLINE_MEDIA_BYTES:
                        file_name = None
                        for attribute in getattr(document, "attributes", []) or []:
                            file_name = getattr(attribute, "file_name", None) or file_name
                        media_payload.append(
                            {
                                "type": "image_document",
                                "mime_type": mime_type,
                                "size_bytes": len(media_bytes),
                                "file_name": file_name,
                                "base64": base64.b64encode(media_bytes).decode("ascii"),
                            }
                        )
                    elif media_bytes:
                        media_payload.append(
                            {
                                "type": "image_document_reference",
                                "mime_type": mime_type,
                                "size_bytes": len(media_bytes),
                                "note": "image too large to inline",
                            }
                        )
                else:
                    media_payload.append(
                        {
                            "type": "document_reference",
                            "mime_type": mime_type,
                            "size_bytes": getattr(document, "size", None),
                        }
                    )
        except Exception as exc:  # pragma: no cover - network/file issues
            logger.warning("åª’ä½“æå–å¤±è´¥ï¼Œå°†è·³è¿‡é™„ä»¶: %s", exc)

        return media_payload

    async def _stats_reporter(self) -> None:
        while self.running:
            await asyncio.sleep(300)
            runtime = datetime.now() - self.stats["start_time"]
            logger.info(
                "\nğŸ“Š **è¿è¡Œç»Ÿè®¡** (è¿è¡Œæ—¶é—´: %s)\n"
                "   â€¢ æ€»æ¥æ”¶: %s\n"
                "   â€¢ å·²è½¬å‘: %s\n"
                "   â€¢ å…³é”®è¯è¿‡æ»¤: %s\n"
                "   â€¢ é‡å¤æ¶ˆæ¯: %s (å†…å­˜: %s / å“ˆå¸Œ: %s / è¯­ä¹‰: %s)\n"
                "   â€¢ é”™è¯¯æ¬¡æ•°: %s\n"
                "   â€¢ ç¿»è¯‘æˆåŠŸ: %s\n"
                "   â€¢ ç¿»è¯‘é”™è¯¯: %s\n"
                "   â€¢ AI å·²å¤„ç†: %s\n"
                "   â€¢ AI è¡ŒåŠ¨: %s\n"
                "   â€¢ AI è·³è¿‡: %s\n"
                "   â€¢ AI é”™è¯¯: %s\n",
                str(runtime).split(".")[0],
                self.stats["total_received"],
                self.stats["forwarded"],
                self.stats["filtered_out"],
                self.stats["duplicates"],
                self.stats["dup_memory"],
                self.stats["dup_hash"],
                self.stats["dup_semantic"],
                self.stats["errors"],
                self.stats["translations"],
                self.stats["translation_errors"],
                self.stats["ai_processed"],
                self.stats["ai_actions"],
                self.stats["ai_skipped"],
                self.stats["ai_errors"],
            )

    async def _cleanup(self) -> None:
        logger.info("ğŸ§¹ æ­£åœ¨æ¸…ç†èµ„æº...")
        if self.client:
            await self.client.disconnect()
        logger.info("âœ… æ¸…ç†å®Œæˆ")


async def main() -> None:
    try:
        listener = TelegramListener()
        await listener.initialize()
        await listener.start_listening()
    except SessionPasswordNeededError:
        logger.error("âŒ è´¦å·éœ€è¦ä¸¤æ­¥éªŒè¯å¯†ç ï¼Œè¯·æ‰‹åŠ¨ç™»å½•ä¸€æ¬¡")
        sys.exit(1)
    except PhoneCodeInvalidError:
        logger.error("âŒ æ‰‹æœºéªŒè¯ç æ— æ•ˆï¼Œè¯·é‡è¯•")
        sys.exit(1)
    except Exception as exc:  # pylint: disable=broad-except
        logger.error(f"âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {exc}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
