"""LangGraph node implementations for tool-enhanced deep analysis.

This module contains all LangGraph node methods that will be added to
GeminiDeepAnalysisEngine. These methods are separated for clarity during development
and will be integrated into the main gemini.py file.

Usage: Copy these methods into GeminiDeepAnalysisEngine class in gemini.py
"""

# ==================== LangGraph Node Methods ====================

async def _node_context_gather(self, state):
    """Node 1: Gather historical memory context (async)."""
    logger.info("ğŸ§  Context Gather: è·å–å†å²è®°å¿†")

    entries = await self._fetch_memory_entries(
        payload=state["payload"],
        preliminary=state["preliminary"],
    )

    memory_text = self._format_memory_evidence(entries)
    logger.info("ğŸ§  Context Gather: æ‰¾åˆ° %d æ¡å†å²äº‹ä»¶", len(entries))

    return {
        "memory_evidence": {
            "entries": entries,
            "formatted": memory_text,
            "count": len(entries),
        }
    }


async def _node_tool_planner(self, state):
    """Node 2: AI decides which tools to call and generates search keywords (async)."""
    logger.info("ğŸ¤– Tool Planner: å†³ç­–ä¸‹ä¸€æ­¥å·¥å…·")

    preliminary = state["preliminary"]

    # Blacklist: Skip search for certain event types (ä½ä»·å€¼ã€å™ªéŸ³å¤šçš„äº‹ä»¶ç±»å‹)
    # airdrop: ç©ºæŠ•ç±»æ´»åŠ¨é€šå¸¸å¸‚å€¼å°ã€æŠ•æœºæ€§å¼ºï¼Œä¸é€‚åˆæ·±åº¦æœç´¢
    NEVER_SEARCH_EVENT_TYPES = {"macro", "governance", "airdrop", "celebrity"}
    if preliminary.event_type in NEVER_SEARCH_EVENT_TYPES:
        logger.info("ğŸ¤– Tool Planner: äº‹ä»¶ç±»å‹ '%s' åœ¨é»‘åå•ï¼Œè·³è¿‡æœç´¢", preliminary.event_type)
        return {"next_tools": []}

    # Whitelist: Force search for high-priority events (first turn only)
    FORCE_SEARCH_EVENT_TYPES = {"hack", "regulation", "partnership"}
    if preliminary.event_type in FORCE_SEARCH_EVENT_TYPES and state["tool_call_count"] == 0:
        logger.info("ğŸ¤– Tool Planner: äº‹ä»¶ç±»å‹ '%s' åœ¨ç™½åå•ï¼Œå¼ºåˆ¶æœç´¢", preliminary.event_type)
        # Generate keywords using AI
        keyword = await self._generate_search_keywords(state)
        return {"next_tools": ["search"], "search_keywords": keyword}

    # Already have search results: No need to search again
    if state.get("search_evidence"):
        logger.info("ğŸ¤– Tool Planner: å·²æœ‰æœç´¢ç»“æœï¼Œæ— éœ€å†æœç´¢")
        return {"next_tools": []}

    # Use Function Calling for AI decision
    prompt = self._build_planner_prompt(state)

    # Tool definition for decision making + keyword generation
    tool_definition = {
        "name": "decide_next_tools",
        "description": "æ ¹æ®å·²æœ‰è¯æ®å†³å®šä¸‹ä¸€æ­¥éœ€è¦è°ƒç”¨çš„å·¥å…·ï¼Œå¹¶ä¸ºæœç´¢ç”Ÿæˆæœ€ä¼˜å…³é”®è¯",
        "parameters": {
            "type": "OBJECT",
            "properties": {
                "tools": {
                    "type": "ARRAY",
                    "items": {"type": "STRING"},
                    "description": "éœ€è¦è°ƒç”¨çš„å·¥å…·åˆ—è¡¨,å¯é€‰å€¼: search",
                },
                "search_keywords": {
                    "type": "STRING",
                    "description": "å¦‚æœéœ€è¦æœç´¢ï¼Œç”Ÿæˆæœ€ä¼˜æœç´¢å…³é”®è¯ï¼ˆä¸­è‹±æ–‡æ··åˆï¼ŒåŒ…å«å…³é”®å®ä½“ã€å®˜æ–¹æ¥æºæ ‡è¯†ï¼‰ã€‚ç¤ºä¾‹ï¼š'USDC Circle depeg official statement è„±é”š å®˜æ–¹å£°æ˜'",
                },
                "reason": {
                    "type": "STRING",
                    "description": "å†³ç­–ç†ç”±",
                },
            },
            "required": ["tools", "reason"],
        },
    }

    try:
        response = await self._client.generate_content_with_tools(
            messages=[{"role": "user", "content": prompt}],
            tools=[tool_definition],
        )

        # Parse Function Calling result
        if response and response.function_calls:
            decision = response.function_calls[0].args
            tools = decision.get("tools", [])
            search_keywords = decision.get("search_keywords", "")
            reason = decision.get("reason", "")

            logger.info(
                "ğŸ¤– Tool Planner å†³ç­–: tools=%s, keywords='%s', ç†ç”±: %s",
                tools,
                search_keywords,
                reason,
            )

            return {
                "next_tools": tools,
                "search_keywords": search_keywords,  # AI-generated keywords
            }
        else:
            logger.warning("Tool Planner æœªè¿”å›å·¥å…·è°ƒç”¨")
            return {"next_tools": []}

    except Exception as exc:
        logger.error("Tool Planner æ‰§è¡Œå¤±è´¥: %s", exc)
        return {"next_tools": []}


async def _node_tool_executor(self, state):
    """Node 3: Execute tools decided by planner (async, Phase 1: search only)."""
    tools_to_call = state.get("next_tools", [])
    logger.info("ğŸ”§ Tool Executor: è°ƒç”¨å·¥å…·: %s", tools_to_call)

    # Check daily quota
    if not self._check_tool_quota():
        logger.warning("âš ï¸ è¶…å‡ºæ¯æ—¥é…é¢ï¼Œè·³è¿‡å·¥å…·è°ƒç”¨")
        return {"tool_call_count": state["tool_call_count"] + 1}

    updates = {"tool_call_count": state["tool_call_count"] + 1}

    for tool_name in tools_to_call:
        if tool_name != "search":
            logger.warning("æœªçŸ¥å·¥å…·: %s", tool_name)
            continue

        if not self._search_tool:
            logger.warning("æœç´¢å·¥å…·æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æ‰§è¡Œ")
            continue

        result = await self._execute_search_tool(state)
        if result:
            updates["search_evidence"] = result

    return updates


async def _node_synthesis(self, state):
    """Node 4: Synthesize all evidence to generate final signal (async)."""
    logger.info("ğŸ“Š Synthesis: ç”Ÿæˆæœ€ç»ˆåˆ†æ")

    prompt = self._build_synthesis_prompt(state)
    final_json = await self._invoke_text_model(prompt)

    try:
        import json
        import re

        # Try to extract JSON from markdown code blocks if present
        json_text = final_json
        if "```json" in final_json:
            match = re.search(r'```json\s*\n(.*?)\n```', final_json, re.DOTALL)
            if match:
                json_text = match.group(1)
        elif "```" in final_json:
            match = re.search(r'```\s*\n(.*?)\n```', final_json, re.DOTALL)
            if match:
                json_text = match.group(1)

        parsed = json.loads(json_text.strip())
        final_conf = parsed.get("confidence", 0.0)
        prelim_conf = state["preliminary"].confidence
        logger.info("ğŸ“Š Synthesis: æœ€ç»ˆç½®ä¿¡åº¦ %.2f (åˆæ­¥ %.2f)", final_conf, prelim_conf)
    except json.JSONDecodeError as exc:
        logger.error("ğŸ“Š Synthesis: JSON è§£æå¤±è´¥ - %s", exc)
        logger.error("ğŸ“Š åŸå§‹å“åº” (å‰500å­—ç¬¦): %s", final_json[:500])
    except Exception as exc:  # pragma: no cover - tolerate parsing failures
        logger.error("ğŸ“Š Synthesis: å…¶ä»–é”™è¯¯ - %s", exc)

    return {"final_response": final_json}


# ==================== Router Methods ====================


def _route_after_planner(self, state):
    """Router after Tool Planner: synthesis if no tools, else executor."""
    if not state.get("next_tools"):
        return "synthesis"
    return "executor"


def _route_after_executor(self, state):
    """Router after Tool Executor: synthesis if max turns, else planner."""
    if state["tool_call_count"] >= state["max_tool_calls"]:
        logger.info("è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•° (3)ï¼Œè¿›å…¥æœ€ç»ˆæ¨ç†")
        return "synthesis"
    return "planner"


# ==================== Graph Builder ====================


def _build_deep_graph(self):
    """Build LangGraph for tool-enhanced deep analysis."""
    from langgraph.graph import END, StateGraph

    graph = StateGraph(DeepAnalysisState)

    # Add nodes
    graph.add_node("context_gather", self._node_context_gather)
    graph.add_node("planner", self._node_tool_planner)
    graph.add_node("executor", self._node_tool_executor)
    graph.add_node("synthesis", self._node_synthesis)

    # Define edges
    graph.set_entry_point("context_gather")
    graph.add_edge("context_gather", "planner")

    # Conditional routing
    graph.add_conditional_edges(
        "planner",
        self._route_after_planner,
        {
            "executor": "executor",
            "synthesis": "synthesis",
        },
    )

    graph.add_conditional_edges(
        "executor",
        self._route_after_executor,
        {
            "planner": "planner",
            "synthesis": "synthesis",
        },
    )

    graph.add_edge("synthesis", END)

    return graph.compile()


# ==================== Helper Methods ====================


async def _fetch_memory_entries(
    self,
    *,
    payload,
    preliminary,
    limit=None,
):
    """Independent memory retrieval helper, reused in both Context Gather node and Function Calling tool."""
    if not self._memory or not self._memory.enabled:
        return []

    limit = limit or self._memory_limit
    keywords = list(payload.keywords_hit or [])
    asset_codes = _normalise_asset_codes(preliminary.asset)

    repo = self._memory.repository
    if repo is None:
        return []

    entries = []

    if hasattr(repo, "fetch_memories") and inspect.iscoroutinefunction(repo.fetch_memories):
        kwargs = {"embedding": None, "asset_codes": asset_codes}
        parameters = inspect.signature(repo.fetch_memories).parameters
        if "keywords" in parameters:
            kwargs["keywords"] = keywords
        try:
            context = await repo.fetch_memories(**kwargs)
        except Exception as exc:
            logger.warning("è®°å¿†æ£€ç´¢å¤±è´¥: %s", exc)
            return []
        if isinstance(context, MemoryContext):
            entries = list(context.entries)
        elif isinstance(context, Iterable):
            entries = list(context)
    elif hasattr(repo, "fetch_memories"):
        kwargs = {"embedding": None, "asset_codes": asset_codes}
        parameters = inspect.signature(repo.fetch_memories).parameters
        if "keywords" in parameters:
            kwargs["keywords"] = keywords
        try:
            result = repo.fetch_memories(**kwargs)
        except Exception as exc:
            logger.warning("è®°å¿†æ£€ç´¢å¤±è´¥: %s", exc)
            return []
        if inspect.isawaitable(result):
            result = await result
        if isinstance(result, MemoryContext):
            entries = list(result.entries)
        elif isinstance(result, Iterable):
            entries = list(result)
    elif hasattr(repo, "load_entries"):
        try:
            entries = repo.load_entries(
                keywords=keywords,
                limit=limit,
                min_confidence=self._memory_min_confidence,
            )
        except Exception as exc:
            logger.warning("æœ¬åœ°è®°å¿†æ£€ç´¢å¤±è´¥: %s", exc)
            return []

    prompt_entries = _memory_entries_to_prompt(entries)[:limit] if entries else []
    return prompt_entries


def _format_memory_evidence(self, entries):
    """Format memory entries for AI consumption."""
    if not entries:
        return "æ— å†å²ç›¸ä¼¼äº‹ä»¶"

    lines = []
    for i, entry in enumerate(entries, 1):
        confidence = entry.get("confidence", "N/A")
        similarity = entry.get("similarity", "N/A")
        summary = entry.get("summary", "N/A")
        lines.append(f"{i}. {summary} (ç½®ä¿¡åº¦: {confidence}, ç›¸ä¼¼åº¦: {similarity})")

    return "\\n".join(lines)


async def _generate_search_keywords(self, state):
    """Generate search keywords using AI for whitelist events."""
    payload = state["payload"]
    preliminary = state["preliminary"]

    prompt = f"""ä½ æ˜¯å…³é”®è¯ç”Ÿæˆä¸“å®¶ï¼Œæ ¹æ®æ¶ˆæ¯å†…å®¹ç”Ÿæˆæœ€ä¼˜æœç´¢å…³é”®è¯ã€‚

ã€æ¶ˆæ¯å†…å®¹ã€‘{payload.text}
ã€äº‹ä»¶ç±»å‹ã€‘{preliminary.event_type}
ã€èµ„äº§ã€‘{preliminary.asset}

ã€å…³é”®è¯ç”Ÿæˆè§„åˆ™ã€‘
1. **ä¸­è‹±æ–‡æ··åˆ**: ä¸­æ–‡æ¶ˆæ¯ç”Ÿæˆä¸­è‹±æ–‡æ··åˆå…³é”®è¯
2. **åŒ…å«å…³é”®å®ä½“**: æå–å…¬å¸åã€åè®®åã€é‡‘é¢ç­‰
3. **å®˜æ–¹æ¥æºæ ‡è¯†**: æ·»åŠ  "official statement å®˜æ–¹å£°æ˜"
4. **äº‹ä»¶ç±»å‹å…³é”®è¯**: hack â†’ "hack exploit æ”»å‡»", regulation â†’ "regulation ç›‘ç®¡", listing â†’ "listing ä¸Šçº¿"
5. **é¿å…æ³›åŒ–è¯**: ä¸ä½¿ç”¨ "æ–°é—»" "æ¶ˆæ¯" ç­‰

ç›´æ¥è¿”å›å…³é”®è¯å­—ç¬¦ä¸²ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

    try:
        text = await self._invoke_text_model(prompt)
        keyword = text.strip()
        logger.info("ğŸ¤– AI ç”Ÿæˆå…³é”®è¯: '%s'", keyword)
        return keyword
    except Exception as exc:
        logger.warning("AI å…³é”®è¯ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ: %s", exc)
        return f"{preliminary.asset} {preliminary.event_type}"


def _build_planner_prompt(self, state):
    """Build prompt for Tool Planner with keyword generation rules."""
    payload = state["payload"]
    preliminary = state["preliminary"]
    memory_ev = state.get("memory_evidence", {})
    search_ev = state.get("search_evidence", {})

    return f"""ä½ æ˜¯å·¥å…·è°ƒåº¦ä¸“å®¶,åˆ¤æ–­æ˜¯å¦éœ€è¦æœç´¢æ–°é—»éªŒè¯,å¹¶ç”Ÿæˆæœ€ä¼˜æœç´¢å…³é”®è¯ã€‚

ã€æ¶ˆæ¯å†…å®¹ã€‘{payload.text}
ã€æ¶ˆæ¯è¯­è¨€ã€‘{getattr(payload, 'language', 'æœªçŸ¥')}
ã€äº‹ä»¶ç±»å‹ã€‘{preliminary.event_type}
ã€èµ„äº§ã€‘{preliminary.asset}
ã€åˆæ­¥ç½®ä¿¡åº¦ã€‘{preliminary.confidence}

ã€å·²æœ‰è¯æ®ã€‘
- å†å²è®°å¿†: {memory_ev.get('formatted', 'æ— ')}
- æœç´¢ç»“æœ: {self._format_search_evidence(search_ev)}

ã€å†³ç­–è§„åˆ™ã€‘
0. âš ï¸ æˆæœ¬æ„è¯†ï¼šæ¯æ¬¡æœç´¢æ¶ˆè€—é…é¢ï¼Œè¯·è°¨æ…å†³ç­–
1. å¦‚æœå·²æœ‰æœç´¢ç»“æœä¸” multi_source=true â†’ è¯æ®å……åˆ†,æ— éœ€å†æœç´¢
2. å¦‚æœäº‹ä»¶ç±»å‹æ˜¯ hack/regulation/partnership â†’ éœ€è¦æœç´¢éªŒè¯
3. å¦‚æœ tool_call_count >= 2 â†’ è¯æ®å……åˆ†,æ— éœ€å†æœç´¢
4. å¦‚æœæ˜¯æ•°å€¼ç±»äº‹ä»¶ (depeg/liquidation) â†’ æš‚ä¸éœ€è¦æœç´¢ï¼ˆç¬¬ä¸€é˜¶æ®µé™åˆ¶ï¼‰
5. å¦‚æœè®°å¿†ä¸­å·²æœ‰é«˜ç›¸ä¼¼åº¦æ¡ˆä¾‹ (similarity > 0.8) â†’ ä¼˜å…ˆä½¿ç”¨è®°å¿†ï¼Œå‡å°‘æœç´¢

ã€å…³é”®è¯ç”Ÿæˆè§„åˆ™ã€‘ï¼ˆä»…å½“å†³å®šæœç´¢æ—¶ï¼‰
1. **ä¸­è‹±æ–‡æ··åˆ**: å¦‚æœæ¶ˆæ¯æ˜¯ä¸­æ–‡,ç”Ÿæˆä¸­è‹±æ–‡æ··åˆå…³é”®è¯,æé«˜æœç´¢è¦†ç›–ç‡
   ç¤ºä¾‹: "æ¯”ç‰¹å¸ Bitcoin ETF æ‰¹å‡† approval"

2. **åŒ…å«å…³é”®å®ä½“**: æå–æ¶ˆæ¯ä¸­çš„å…·ä½“å…¬å¸åã€åè®®åã€é‡‘é¢ç­‰
   ç¤ºä¾‹: "Circle USDC $3B depeg"

3. **å®˜æ–¹æ¥æºæ ‡è¯†**: å¯¹ hack/regulation/partnership äº‹ä»¶,æ·»åŠ å®˜æ–¹å…³é”®è¯
   - ä¸­æ–‡: "å®˜æ–¹å£°æ˜ å®˜æ–¹å…¬å‘Š"
   - è‹±æ–‡: "official statement announcement"

4. **äº‹ä»¶ç±»å‹å…³é”®è¯**:
   - hack â†’ "é»‘å®¢æ”»å‡» hack exploit breach"
   - regulation â†’ "ç›‘ç®¡æ”¿ç­– regulation SEC CFTC"
   - listing â†’ "ä¸Šçº¿ listing announce"
   - partnership â†’ "åˆä½œ partnership collaboration"

5. **é¿å…æ³›åŒ–è¯**: ä¸è¦ä½¿ç”¨ "æ–°é—»" "æ¶ˆæ¯" "æŠ¥é“" ç­‰ä½ä»·å€¼è¯

ã€ç¤ºä¾‹ã€‘
- æ¶ˆæ¯: "Circle ç¡®è®¤ USDC å‚¨å¤‡å®‰å…¨,è„±é”šå·²æ¢å¤"
  â†’ å…³é”®è¯: "USDC Circle depeg official statement è„±é”š å®˜æ–¹å£°æ˜"

- æ¶ˆæ¯: "XXX DeFi åè®®é­å—é—ªç”µè´·æ”»å‡»,æŸå¤± $50M"
  â†’ å…³é”®è¯: "XXX protocol flash loan hack exploit $50M æ”»å‡»"

- æ¶ˆæ¯: "SEC æ‰¹å‡†æ¯”ç‰¹å¸ç°è´§ ETF,å°†äºä¸‹å‘¨å¼€å§‹äº¤æ˜“"
  â†’ å…³é”®è¯: "Bitcoin spot ETF SEC approval æ¯”ç‰¹å¸ ç°è´§ æ‰¹å‡†"

ã€å½“å‰çŠ¶æ€ã€‘
- å·²è°ƒç”¨å·¥å…·æ¬¡æ•°: {state['tool_call_count']}
- æœ€å¤§è°ƒç”¨æ¬¡æ•°: {state['max_tool_calls']}

è¯·è°ƒç”¨ decide_next_tools å‡½æ•°è¿”å›å†³ç­–å’Œå…³é”®è¯ã€‚"""


def _build_synthesis_prompt(self, state):
    """Build prompt for Synthesis node with quantified confidence adjustment rules."""
    payload = state["payload"]
    preliminary = state["preliminary"]
    memory_ev = state.get("memory_evidence", {})
    search_ev = state.get("search_evidence", {})

    return f"""ä½ æ˜¯åŠ å¯†äº¤æ˜“å°èµ„æ·±åˆ†æå¸ˆ,å·²æŒæ¡å®Œæ•´è¯æ®,è¯·ç»™å‡ºæœ€ç»ˆåˆ¤æ–­ã€‚

ã€åŸå§‹æ¶ˆæ¯ã€‘
{payload.text}

ã€Gemini Flash åˆæ­¥åˆ¤æ–­ã€‘
- äº‹ä»¶ç±»å‹: {preliminary.event_type}
- èµ„äº§: {preliminary.asset}
- æ“ä½œ: {preliminary.action}
- ç½®ä¿¡åº¦: {preliminary.confidence}
- æ‘˜è¦: {preliminary.summary}

ã€å†å²è®°å¿†ã€‘
{memory_ev.get('formatted', 'æ— å†å²ç›¸ä¼¼äº‹ä»¶')}

ã€æœç´¢éªŒè¯ã€‘
{self._format_search_detail(search_ev)}

ã€ç½®ä¿¡åº¦è°ƒæ•´è§„åˆ™ã€‘
- åŸºå‡†: Gemini Flash åˆåˆ¤ç½®ä¿¡åº¦ = {preliminary.confidence}

- æœç´¢å¤šæºç¡®è®¤ (multi_source=true) AND å®˜æ–¹ç¡®è®¤ (official_confirmed=true):
  â†’ æå‡ +0.15 to +0.20

- æœç´¢å¤šæºç¡®è®¤ä½†æ— å®˜æ–¹ç¡®è®¤:
  â†’ æå‡ +0.05 to +0.10

- æœç´¢ç»“æœ < 3 æ¡æˆ–æ— å®˜æ–¹ç¡®è®¤:
  â†’ é™ä½ -0.10 to -0.20

- æœç´¢ç»“æœå†²çª (ä¸åŒæ¥æºè¯´æ³•çŸ›ç›¾):
  â†’ é™ä½ -0.20 å¹¶æ ‡è®° data_incomplete

- å†å²è®°å¿†å­˜åœ¨é«˜ç›¸ä¼¼åº¦æ¡ˆä¾‹ (similarity > 0.8):
  â†’ å‚è€ƒå†å²æ¡ˆä¾‹æœ€ç»ˆç½®ä¿¡åº¦,è°ƒæ•´ Â±0.10

ã€æœ€ç»ˆçº¦æŸã€‘
- ç½®ä¿¡åº¦èŒƒå›´: 0.0 - 1.0
- å¦‚æœæœ€ç»ˆç½®ä¿¡åº¦ < 0.4, å¿…é¡»æ·»åŠ  confidence_low é£é™©æ ‡å¿—
- åœ¨ notes ä¸­è¯´æ˜: "åˆåˆ¤ {preliminary.confidence:.2f} â†’ æœ€ç»ˆ {{final_confidence:.2f}}, ä¾æ®: [æœç´¢/è®°å¿†/å†²çª]"

è¿”å› JSONï¼ˆä¸ SignalResult æ ¼å¼ä¸€è‡´ï¼‰:
{{
  "summary": "ä¸­æ–‡æ‘˜è¦",
  "event_type": "{preliminary.event_type}",
  "asset": "{preliminary.asset}",
  "asset_name": "{getattr(preliminary, 'asset_name', '')}",
  "action": "buy|sell|observe",
  "direction": "long|short|neutral",
  "confidence": 0.0-1.0,
  "strength": "low|medium|high",
  "timeframe": "short|medium|long",
  "risk_flags": [],
  "notes": "æ¨ç†ä¾æ®,å¼•ç”¨æœç´¢æ¥æºå’Œå…³é”®è¯æ®",
  "links": []
}}

åªè¿”å› JSON,ä¸è¦å…¶ä»–æ–‡å­—ã€‚"""


async def _execute_search_tool(self, state):
    """Execute SearchTool and convert to LangGraph state format."""
    preliminary = state["preliminary"]

    # Prioritize AI-generated keywords
    keyword = state.get("search_keywords", "").strip()
    keyword_source = "AIç”Ÿæˆ"

    # Fallback: If AI didn't generate keywords, use basic concatenation
    if not keyword:
        keyword = f"{preliminary.asset} {preliminary.event_type}"
        if preliminary.event_type in ["hack", "regulation"]:
            keyword += " news official"
        keyword_source = "ç¡¬ç¼–ç é™çº§"

    # Get domain whitelist for high-priority events
    include_domains = None
    event_type = preliminary.event_type
    if hasattr(self._config, "HIGH_PRIORITY_EVENT_DOMAINS"):
        include_domains = self._config.HIGH_PRIORITY_EVENT_DOMAINS.get(event_type)

    logger.info(
        "ğŸ”§ è°ƒç”¨æœç´¢å·¥å…·: keyword='%s' (æ¥æº: %s), domains=%s",
        keyword,
        keyword_source,
        include_domains,
    )

    try:
        result = await self._search_tool.fetch(
            keyword=keyword,
            max_results=5,
            include_domains=include_domains,
        )
    except Exception as exc:  # pragma: no cover - defensive
        logger.error("æœç´¢å·¥å…·æ‰§è¡Œå¤±è´¥: %s", exc)
        return None

    if not result.success:
        logger.warning("ğŸ”§ æœç´¢å·¥å…·è°ƒç”¨å¤±è´¥: %s", result.error)
        return None

    logger.info(
        "ğŸ”§ æœç´¢è¿”å› %d æ¡ç»“æœ (multi_source=%s, official=%s)",
        result.data.get("source_count", 0),
        result.data.get("multi_source"),
        result.data.get("official_confirmed"),
    )

    return {
        "success": True,
        "data": result.data,
        "triggered": result.triggered,
        "confidence": result.confidence,
    }


async def _invoke_text_model(self, prompt):
    """Unified text generation call, reusing Function Calling client."""
    messages = [{"role": "user", "content": prompt}]
    response = await self._client.generate_content_with_tools(messages, tools=None)

    if not response or not response.text:
        raise DeepAnalysisError("Gemini è¿”å›ç©ºå“åº”")

    return response.text.strip()


def _format_search_evidence(self, search_ev):
    """Format search evidence for display."""
    if not search_ev:
        return "æ— "

    data = search_ev.get("data", {})
    return f"æ‰¾åˆ° {data.get('source_count', 0)} æ¡ç»“æœ, å¤šæºç¡®è®¤={data.get('multi_source', False)}, å®˜æ–¹ç¡®è®¤={data.get('official_confirmed', False)}"


def _format_search_detail(self, search_ev):
    """Format search evidence in detail for Synthesis."""
    if not search_ev or not search_ev.get("success"):
        return "æ— æœç´¢ç»“æœæˆ–æœç´¢å¤±è´¥"

    data = search_ev.get("data", {})
    results = data.get("results", [])

    lines = [
        f"å…³é”®è¯: {data.get('keyword', 'N/A')}",
        f"ç»“æœæ•°: {data.get('source_count', 0)}",
        f"å¤šæºç¡®è®¤: {data.get('multi_source', False)}",
        f"å®˜æ–¹ç¡®è®¤: {data.get('official_confirmed', False)}",
        f"æƒ…ç»ªåˆ†æ: {data.get('sentiment', {})}",
        "",
        "æœç´¢ç»“æœ:",
    ]

    for i, result in enumerate(results[:3], 1):  # Show first 3 results
        lines.append(
            f"{i}. {result.get('title', 'N/A')} (æ¥æº: {result.get('source', 'N/A')}, è¯„åˆ†: {result.get('score', 0.0)})"
        )

    return "\\n".join(lines)


def _check_tool_quota(self):
    """Check if daily tool quota is exceeded."""
    from datetime import datetime, timezone

    today = datetime.now(timezone.utc).date()

    # Reset counter on new day
    if today != self._tool_call_reset_date:
        self._tool_call_count_today = 0
        self._tool_call_reset_date = today

    # Check quota
    if self._tool_call_count_today >= self._tool_call_daily_limit:
        logger.warning(
            "âš ï¸ ä»Šæ—¥å·¥å…·è°ƒç”¨é…é¢å·²ç”¨å°½ (%d/%d)",
            self._tool_call_count_today,
            self._tool_call_daily_limit,
        )
        return False

    self._tool_call_count_today += 1
    return True
