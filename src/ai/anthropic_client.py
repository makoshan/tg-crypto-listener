"""
Anthropic Claude API client with Memory Tool support

Features:
- Memory Tool å¾ªç¯ï¼ˆTool Use â†’ Execute â†’ Feed backï¼‰
- Context Editingï¼ˆè‡ªåŠ¨æ¸…ç†æ—§ Tool Use ç»“æœï¼‰
- å…¼å®¹ç°æœ‰ SignalResult ç»“æ„
"""

import asyncio
import json
import logging
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

try:
    from anthropic import Anthropic
    from anthropic.types import Message, TextBlock, ToolUseBlock
except ImportError:
    Anthropic = None  # type: ignore
    Message = None  # type: ignore
    TextBlock = None  # type: ignore
    ToolUseBlock = None  # type: ignore

from src.memory.memory_tool_handler import MemoryToolHandler

logger = logging.getLogger(__name__)


class AiServiceError(RuntimeError):
    """Raised when the AI service call fails."""

    def __init__(self, message: str, *, temporary: bool = False) -> None:
        super().__init__(message)
        self.temporary = temporary


@dataclass
class AnthropicResponse:
    """Structured response returned by the Anthropic client."""

    text: str
    usage: Optional[Dict[str, int]] = None  # token ä½¿ç”¨ç»Ÿè®¡
    stop_reason: Optional[str] = None


class AnthropicClient:
    """
    Claude API å®¢æˆ·ç«¯ï¼ˆæ”¯æŒ Memory Toolï¼‰

    æ ¸å¿ƒç‰¹æ€§:
    - Memory Tool å¾ªç¯ï¼šè‡ªåŠ¨æ‰§è¡Œå·¥å…·è°ƒç”¨å¹¶å›å¡«ç»“æœ
    - Context Editingï¼šè‡ªåŠ¨æ¸…ç†æ—§ Tool Use ç»“æœï¼ˆèŠ‚çœ tokenï¼‰
    - é‡è¯•æœºåˆ¶ï¼šç½‘ç»œé”™è¯¯å’Œæš‚æ—¶æ€§æ•…éšœè‡ªåŠ¨é‡è¯•
    """

    def __init__(
        self,
        api_key: str,
        model_name: str,
        timeout: float,
        memory_handler: MemoryToolHandler,
        context_management: Optional[Dict[str, Any]] = None,
        max_retries: int = 1,
        retry_backoff_seconds: float = 1.5,
        max_tool_turns: int = 5,  # é˜²æ­¢ Tool Use æ­»å¾ªç¯
        *,
        context_trigger_tokens: Optional[int] = None,
        context_keep_tools: Optional[int] = None,
        context_clear_at_least: Optional[int] = None
    ) -> None:
        """
        åˆå§‹åŒ– Anthropic å®¢æˆ·ç«¯

        Args:
            api_key: Anthropic API key
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ claude-sonnet-4-5-20250929ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            memory_handler: Memory Tool å¤„ç†å™¨
            context_management: Context Editing é…ç½®
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_backoff_seconds: é‡è¯•é€€é¿æ—¶é—´ï¼ˆç§’ï¼‰
            max_tool_turns: æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°
        """
        if not api_key:
            raise AiServiceError("Anthropic API key is required")

        if Anthropic is None:
            raise AiServiceError(
                "anthropic æœªå®‰è£…ï¼Œè¯·å…ˆåœ¨ç¯å¢ƒä¸­å®‰è£…è¯¥ä¾èµ–: pip install anthropic"
            )

        self._client = Anthropic(api_key=api_key)
        self._model_name = model_name
        self._timeout = timeout
        self._memory_handler = memory_handler
        if context_management is None:
            self._context_management = self._default_context_config(
                trigger_tokens=context_trigger_tokens,
                keep_tools=context_keep_tools,
                clear_at_least=context_clear_at_least,
            )
        else:
            self._context_management = context_management
        self._max_retries = max(0, int(max_retries))
        self._retry_backoff = max(0.0, float(retry_backoff_seconds))
        self._max_tool_turns = max_tool_turns

        logger.info(
            f"AnthropicClient åˆå§‹åŒ–: {model_name} "
            f"(timeout={timeout}s, max_tool_turns={max_tool_turns})"
        )

    def _default_context_config(
        self,
        *,
        trigger_tokens: Optional[int] = None,
        keep_tools: Optional[int] = None,
        clear_at_least: Optional[int] = None,
    ) -> Dict[str, Any]:
        """é»˜è®¤ Context Editing é…ç½®"""
        return {
            "edits": [
                {
                    "type": "clear_tool_uses_20250919",
                    "trigger": {
                        "type": "input_tokens",
                        "value": int(trigger_tokens) if trigger_tokens is not None else 10000,
                    },
                    "keep": {
                        "type": "tool_uses",
                        "value": int(keep_tools) if keep_tools is not None else 2,
                    },
                    "clear_at_least": {
                        "type": "input_tokens",
                        "value": int(clear_at_least) if clear_at_least is not None else 500,
                    },
                }
            ]
        }

    async def generate_signal(
        self,
        prompt: str | List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        max_tokens: int = 4096
    ) -> AnthropicResponse:
        """
        æ‰§è¡Œä¿¡å·åˆ†æï¼ˆæ”¯æŒ Memory Tool å¾ªç¯ï¼‰

        Args:
            prompt: æç¤ºè¯ï¼ˆå­—ç¬¦ä¸²æˆ– OpenAI é£æ ¼çš„ messagesï¼‰
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°

        Returns:
            AnthropicResponse
        """
        # è½¬æ¢ä¸º Anthropic messages æ ¼å¼
        messages = self._convert_to_anthropic_messages(prompt)

        # æ‰§è¡Œ Memory Tool å¾ªç¯
        last_exc: Exception | None = None
        last_error_message = "Claude è°ƒç”¨å¤±è´¥"
        last_error_temporary = False

        for attempt in range(self._max_retries + 1):
            try:
                response = await asyncio.wait_for(
                    asyncio.to_thread(
                        self._run_tool_loop,
                        messages,
                        system_prompt,
                        max_tokens
                    ),
                    timeout=self._timeout
                )
                return response

            except asyncio.TimeoutError as exc:
                last_exc = exc
                last_error_message = "Claude è¯·æ±‚è¶…æ—¶"
                last_error_temporary = True
                logger.warning(
                    f"Claude è¯·æ±‚è¶…æ—¶ (attempt {attempt + 1}/{self._max_retries + 1})"
                )

            except Exception as exc:
                last_exc = exc
                last_error_message, last_error_temporary = self._normalize_exception(exc)
                logger.warning(
                    f"Claude è°ƒç”¨å¼‚å¸¸ (attempt {attempt + 1}/{self._max_retries + 1}): {last_error_message}"
                )

            # é‡è¯•é€€é¿
            if attempt < self._max_retries and self._retry_backoff > 0:
                backoff = self._retry_backoff * (2 ** attempt)
                logger.debug(f"Claude å°†åœ¨ {backoff:.2f} ç§’åé‡è¯•")
                await asyncio.sleep(backoff)

        raise AiServiceError(last_error_message, temporary=last_error_temporary) from last_exc

    def _run_tool_loop(
        self,
        messages: List[Dict[str, Any]],
        system_prompt: Optional[str],
        max_tokens: int
    ) -> AnthropicResponse:
        """
        æ‰§è¡Œ Memory Tool å¾ªç¯

        æµç¨‹:
        1. è°ƒç”¨ Claude APIï¼ˆå¸¦ Memory Toolï¼‰
        2. æ£€æŸ¥ response.content ä¸­æ˜¯å¦æœ‰ tool_use
        3. å¦‚æœæœ‰ï¼Œæ‰§è¡Œ MemoryToolHandler
        4. å°† tool_result å›å¡«åˆ° messages
        5. ç»§ç»­è°ƒç”¨ APIï¼ˆé‡å¤ 1-4ï¼‰
        6. ç›´åˆ°æ²¡æœ‰ tool_use æˆ–è¾¾åˆ°æœ€å¤§è½®æ•°

        Returns:
            AnthropicResponse
        """
        tool_turn_count = 0

        # å®šä¹‰ Memory Tool çš„å®Œæ•´ schema
        memory_tool = {
            "type": "memory_20250818",
            "name": "memory",
            "description": "Memory management tool for storing, retrieving, and modifying information. Supports viewing, creating, editing, and deleting files in the memory storage.",
            "input_schema": {
                "type": "object",
                "properties": {
                    "command": {
                        "type": "string",
                        "enum": ["view", "create", "str_replace", "insert", "delete", "rename"],
                        "description": "The command to execute: view (read file/dir), create (write file), str_replace (replace text), insert (insert at line), delete (remove file/dir), rename (move/rename)"
                    },
                    "path": {
                        "type": "string",
                        "description": "File or directory path (relative to memory root)"
                    },
                    "file_text": {
                        "type": "string",
                        "description": "Content to write when using 'create' command"
                    },
                    "old_str": {
                        "type": "string",
                        "description": "Text to replace when using 'str_replace' command"
                    },
                    "new_str": {
                        "type": "string",
                        "description": "Replacement text when using 'str_replace' command"
                    },
                    "insert_line": {
                        "type": "integer",
                        "description": "Line number to insert at when using 'insert' command (1-indexed)"
                    },
                    "insert_text": {
                        "type": "string",
                        "description": "Text to insert when using 'insert' command"
                    },
                    "old_path": {
                        "type": "string",
                        "description": "Source path when using 'rename' command"
                    },
                    "new_path": {
                        "type": "string",
                        "description": "Destination path when using 'rename' command"
                    }
                },
                "required": ["command"]
            }
        }

        while tool_turn_count < self._max_tool_turns:
            # è°ƒç”¨ Claude API
            logger.info(f"ğŸ¤– Claude API è°ƒç”¨å¼€å§‹ (è½®æ¬¡: {tool_turn_count + 1}, model: {self._model_name})")
            response: Message = self._client.messages.create(
                model=self._model_name,
                max_tokens=max_tokens,
                system=system_prompt or "You are a helpful AI assistant.",
                messages=messages,
                tools=[memory_tool],
                context_management=self._context_management
            )
            logger.info(
                f"âœ… Claude API å“åº”å®Œæˆ (input_tokens: {response.usage.input_tokens}, "
                f"output_tokens: {response.usage.output_tokens}, stop_reason: {response.stop_reason})"
            )

            # æå– text blocks å’Œ tool use blocks
            text_blocks = []
            tool_uses = []

            for block in response.content:
                if isinstance(block, TextBlock):
                    text_blocks.append(block.text)
                elif isinstance(block, ToolUseBlock):
                    tool_uses.append(block)

            # å¦‚æœæ²¡æœ‰ tool useï¼Œè¯´æ˜å¯¹è¯å®Œæˆ
            if not tool_uses:
                final_text = "\n".join(text_blocks).strip()

                return AnthropicResponse(
                    text=final_text,
                    usage={
                        "input_tokens": response.usage.input_tokens,
                        "output_tokens": response.usage.output_tokens
                    },
                    stop_reason=response.stop_reason
                )

            # æ‰§è¡Œ tool uses
            tool_turn_count += 1
            logger.debug(
                f"Tool Use è½®æ¬¡ {tool_turn_count}/{self._max_tool_turns}: "
                f"{len(tool_uses)} ä¸ªå·¥å…·è°ƒç”¨"
            )

            # å°† assistant çš„å“åº”ï¼ˆå« tool_useï¼‰æ·»åŠ åˆ° messages
            messages.append({
                "role": "assistant",
                "content": response.content  # ä¿æŒåŸå§‹æ ¼å¼ï¼ˆåŒ…å« TextBlock å’Œ ToolUseBlockï¼‰
            })

            # æ‰§è¡Œå·¥å…·å¹¶æ„é€  tool_result
            tool_results = []
            for tool_use in tool_uses:
                try:
                    result = self._memory_handler.execute_tool_use(tool_use.input)

                    tool_results.append({
                        "type": "tool_result",
                        "tool_use_id": tool_use.id,
                        "content": json.dumps(result, ensure_ascii=False)
                    })

                    logger.debug(
                        f"Memory Tool æ‰§è¡Œ: {tool_use.input.get('command')} "
                        f"{tool_use.input.get('path', '')[:50]}"
                    )

                except Exception as e:
                    logger.error(f"Memory Tool æ‰§è¡Œå¤±è´¥: {e}")
                    tool_results.append({
                        "type": "tool_result",
                        "tool_use_id": tool_use.id,
                        "content": json.dumps({
                            "success": False,
                            "error": str(e)
                        }, ensure_ascii=False),
                        "is_error": True
                    })

            # å°† tool_result æ·»åŠ åˆ° messages
            messages.append({
                "role": "user",
                "content": tool_results
            })

        # è¾¾åˆ°æœ€å¤§è½®æ•°ï¼Œè¿”å›è­¦å‘Š
        logger.warning(
            f"Tool Use å¾ªç¯è¾¾åˆ°æœ€å¤§è½®æ•° {self._max_tool_turns}ï¼Œå¼ºåˆ¶ç»ˆæ­¢"
        )

        return AnthropicResponse(
            text="[Error: Tool Use å¾ªç¯è¶…è¿‡æœ€å¤§è½®æ•°]",
            stop_reason="max_tool_turns_exceeded"
        )

    def _convert_to_anthropic_messages(
        self,
        prompt: str | List[Dict[str, str]]
    ) -> List[Dict[str, Any]]:
        """
        è½¬æ¢ä¸º Anthropic messages æ ¼å¼

        Args:
            prompt: å­—ç¬¦ä¸²æˆ– OpenAI é£æ ¼çš„ messages

        Returns:
            Anthropic messages
        """
        if isinstance(prompt, str):
            return [{"role": "user", "content": prompt}]

        if isinstance(prompt, list) and prompt and isinstance(prompt[0], dict):
            # OpenAI é£æ ¼: [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]
            # Anthropic é£æ ¼: system æå–ä¸ºç‹¬ç«‹å‚æ•°ï¼Œmessages åªä¿ç•™ user/assistant

            messages = []
            for msg in prompt:
                role = msg.get("role")
                content = msg.get("content")

                # è·³è¿‡ systemï¼ˆåœ¨ generate_signal ä¸­å•ç‹¬å¤„ç†ï¼‰
                if role == "system":
                    continue

                # è½¬æ¢ user/assistant
                if role in ("user", "assistant"):
                    messages.append({
                        "role": role,
                        "content": content
                    })

            return messages

        # é»˜è®¤ï¼šåŒ…è£…ä¸º user message
        return [{"role": "user", "content": str(prompt)}]

    def _normalize_exception(self, exc: Exception) -> tuple[str, bool]:
        """è¿”å›äººç±»å¯è¯»çš„é”™è¯¯æ¶ˆæ¯å’Œæ˜¯å¦ä¸ºæš‚æ—¶æ€§é”™è¯¯"""

        message = str(exc).strip() or "Claude è°ƒç”¨å¤±è´¥"
        temporary = False

        # Anthropic SDK å¼‚å¸¸å¤„ç†
        # å‚è€ƒ: https://github.com/anthropics/anthropic-sdk-python
        exc_type = type(exc).__name__

        if "RateLimitError" in exc_type or "429" in message:
            return ("Claude è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•", True)

        if "APIConnectionError" in exc_type or "InternalServerError" in exc_type:
            return ("Claude æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•", True)

        if "APITimeoutError" in exc_type:
            return ("Claude è¯·æ±‚è¶…æ—¶", True)

        if "503" in message or "UNAVAILABLE" in message.upper():
            return ("Claude æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•", True)

        if isinstance(exc, (ConnectionError, OSError)):
            return ("Claude ç½‘ç»œè¿æ¥å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥ç½‘ç»œåé‡è¯•", True)

        return (message, temporary)
