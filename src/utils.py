"""Utility helpers for logging, dedupe, and formatting."""

from __future__ import annotations

import logging
import hashlib
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Set

import os
import colorlog


def setup_logger(name: str, level: str = None) -> logging.Logger:
    """Configure a color logger that also writes to file."""
    # ä»ç¯å¢ƒå˜é‡è¯»å–æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ä¸º INFO
    if level is None:
        level = os.getenv("LOG_LEVEL", "INFO")

    logger = logging.getLogger(name)
    logger.setLevel(getattr(logging, level.upper(), logging.INFO))

    if logger.handlers:
        return logger

    console_handler = colorlog.StreamHandler()
    console_handler.setFormatter(
        colorlog.ColoredFormatter(
            "%(log_color)s%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
            log_colors={
                "DEBUG": "cyan",
                "INFO": "green",
                "WARNING": "yellow",
                "ERROR": "red",
                "CRITICAL": "red,bg_white",
            },
        )
    )
    logger.addHandler(console_handler)

    Path("./logs").mkdir(exist_ok=True)
    file_handler = logging.FileHandler("./logs/app.log", encoding="utf-8")
    file_handler.setFormatter(
        logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    )
    logger.addHandler(file_handler)

    return logger


class MessageDeduplicator:
    """Deduplicate messages by hash within a time window."""

    def __init__(self, window_hours: int = 24):
        self.seen_hashes: Dict[str, datetime] = {}
        self.window_hours = window_hours

    def is_duplicate(self, text: str) -> bool:
        """Return True if the message text appeared recently."""
        self._cleanup_expired()

        message_hash = hashlib.md5(text.encode("utf-8")).hexdigest()
        if message_hash in self.seen_hashes:
            return True

        self.seen_hashes[message_hash] = datetime.now()
        return False

    def _cleanup_expired(self) -> None:
        cutoff = datetime.now() - timedelta(hours=self.window_hours)
        expired = [key for key, timestamp in self.seen_hashes.items() if timestamp < cutoff]
        for key in expired:
            del self.seen_hashes[key]


def contains_keywords(text: str, keywords: Set[str]) -> bool:
    """Check if text contains any keyword (case-insensitive)."""
    if not keywords:
        return True

    text_lower = text.lower()
    return any(keyword in text_lower for keyword in keywords)


def compute_sha256(text: str) -> str:
    """Return SHA256 hash for raw text."""
    if not text:
        return ""
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def compute_canonical_hash(text: str) -> str:
    """Return SHA256 hash after stripping whitespace and URLs."""
    if not text:
        return ""
    normalized = re.sub(r"https?://\S+", "", text, flags=re.IGNORECASE)
    normalized = re.sub(r"\s+", "", normalized)
    return hashlib.sha256(normalized.encode("utf-8")).hexdigest()


async def compute_embedding(text: str, api_key: str, model: str = "text-embedding-3-small") -> list[float] | None:
    """Generate OpenAI embedding vector for text.

    Args:
        text: Input text to embed (will be truncated to 8000 chars)
        api_key: OpenAI API key
        model: Embedding model name

    Returns:
        List of floats (1536 dimensions for text-embedding-3-small) or None on error
    """
    if not text or not text.strip():
        return None

    if not api_key:
        return None

    try:
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=api_key)

        # Truncate text to avoid token limits
        truncated_text = text[:8000]

        response = await client.embeddings.create(
            model=model,
            input=truncated_text
        )

        return response.data[0].embedding

    except ImportError:
        logger = setup_logger(__name__)
        logger.warning("OpenAI SDK not installed, skipping embedding generation")
        return None
    except Exception as exc:
        logger = setup_logger(__name__)
        logger.warning("Embedding generation failed: %s", exc)
        return None


ACTION_LABELS = {
    "buy": "ä¹°å…¥",
    "sell": "å–å‡º",
    "observe": "è§‚æœ›",
}

STRENGTH_LABELS = {
    "low": "ä½",
    "medium": "ä¸­",
    "high": "é«˜",
}

DIRECTION_LABELS = {
    "long": "åšå¤š",
    "short": "åšç©º",
    "neutral": "ä¸­æ€§",
}

EVENT_TYPE_LABELS = {
    "listing": "ä¸Šçº¿/æŒ‚ç‰Œ",
    "delisting": "ä¸‹æ¶/é€€å¸‚",
    "hack": "å®‰å…¨/æ”»å‡»",
    "regulation": "ç›‘ç®¡/æ”¿ç­–",
    "funding": "èèµ„/å‹Ÿèµ„",
    "whale": "å·¨é²¸åŠ¨å‘",
    "liquidation": "æ¸…ç®—/çˆ†ä»“",
    "partnership": "åˆä½œ/é›†æˆ",
    "product_launch": "äº§å“å‘å¸ƒ/ä¸»ç½‘",
    "governance": "æ²»ç†ææ¡ˆ",
    "macro": "å®è§‚åŠ¨å‘",
    "celebrity": "åäººè¨€è®º",
    "airdrop": "ç©ºæŠ•æ¿€åŠ±",
    "other": "å…¶ä»–",
}

RISK_FLAG_LABELS = {
    "price_volatility": "ä»·æ ¼æ³¢åŠ¨",
    "liquidity_risk": "æµåŠ¨æ€§é£é™©",
    "regulation_risk": "åˆè§„é£é™©",
    "confidence_low": "ç½®ä¿¡åº¦ä½",
    "data_incomplete": "ä¿¡æ¯ä¸å®Œæ•´",
}


def format_forwarded_message(
    *,
    source_channel: str,
    timestamp: datetime,
    translated_text: str | None = None,
    original_text: str | None = None,
    show_original: bool = False,  # ä¿ç•™ç­¾åä½†ä¸å†å±•ç¤ºåŸæ–‡
    show_translation: bool = True,
    ai_summary: str | None = None,
    ai_action: str | None = None,
    ai_direction: str | None = None,
    ai_event_type: str | None = None,
    ai_asset: str | None = None,
    ai_asset_names: str | None = None,
    ai_confidence: float | None = None,
    ai_strength: str | None = None,
    ai_risk_flags: list[str] | None = None,
    ai_notes: str | None = None,
    context_source: str | None = None,
) -> str:
    """Compose a compact forwarding message emphasising actionable insights."""

    ai_risk_flags = ai_risk_flags or []
    ai_notes = (ai_notes or "").strip()
    ai_asset = (ai_asset or "").strip()
    ai_asset_names = (ai_asset_names or "").strip()
    translated_text = (translated_text or "").strip()
    original_text = (original_text or "").strip()

    if not show_translation:
        translated_text = ""

    parts: list[str] = ["ğŸ”” åŠ å¯†æ–°é—»ç›‘å¬\n"]

    # ä¿¡å·æ‘˜è¦ï¼šç¿»è¯‘æ–‡æœ¬ä¸ AI æ‘˜è¦åˆ†åˆ«åˆ—å‡ºï¼Œæ¸…æ™°ç´§å‡‘
    def _normalize_for_compare(text: str) -> str:
        stripped = re.sub(r"\s+", "", text)
        stripped = re.sub(r'[ï¼Œ,ã€‚\\.!ï¼Ÿ?ï¼š:ï¼›;"\'â€œâ€â€˜â€™`Â·â€¢\-]', "", stripped)
        return stripped.lower()

    if translated_text and original_text:
        if _normalize_for_compare(translated_text) == _normalize_for_compare(original_text):
            translated_text = ""

    summary_text = (ai_summary or "").strip()
    if not summary_text:
        summary_text = translated_text or original_text
    summary_text = (summary_text or "æš‚æ— æ‘˜è¦").replace("\n", " ").strip()

    parts: list[str] = []

    parts.append("âš¡ ä¿¡å·")
    if context_source:
        source_display = context_source
    else:
        source_display = source_channel
    parts.append(f" {source_display}ï¼š{summary_text}")
    if ai_notes:
        parts.append("")
        parts.append(f"å¤‡æ³¨: {ai_notes}")
        parts.append("")

    # æ“ä½œè¦ç‚¹ï¼Œä»…å½“æœ‰ AI ç»“æœæ—¶å±•ç¤º
    if ai_summary:
        action_key = (ai_action or "observe").lower()
        action_value = ACTION_LABELS.get(action_key, ai_action or "observe")
        confidence_text = (
            f"{ai_confidence:.2f}" if ai_confidence is not None else "æœªçŸ¥"
        )
        parts.append("ğŸ¯ æ“ä½œ")

        asset_line = ""
        if ai_asset or ai_asset_names:
            asset_line = ai_asset
            if ai_asset_names and ai_asset:
                asset_line = f"{ai_asset_names} ({ai_asset})"
            elif ai_asset_names:
                asset_line = ai_asset_names
            elif ai_asset:
                asset_line = ai_asset

        direction_key = (ai_direction or "").lower()
        direction_cn = DIRECTION_LABELS.get(direction_key, ai_direction) if ai_direction else None
        strength_key = (ai_strength or "").lower()
        strength_cn = STRENGTH_LABELS.get(strength_key, ai_strength) if ai_strength else None

        line_parts: list[str] = []
        if asset_line:
            line_parts.append(asset_line)

        if action_key == "observe":
            line_parts.append(f"çŠ¶æ€: {action_value}")
        else:
            line_parts.append(action_value)
            if direction_cn:
                line_parts.append(f"æ–¹å‘: {direction_cn}")

        line_parts.append(f"ç½®ä¿¡åº¦: {confidence_text}")
        if strength_cn:
            line_parts.append(f"å¼ºåº¦: {strength_cn}")

        parts.append("- " + "ï¼Œ".join(line_parts))

        event_type_label = EVENT_TYPE_LABELS.get(ai_event_type or "", None)
        if event_type_label:
            parts.append(f"- äº‹ä»¶ç±»å‹: {event_type_label}")

        localized_flags = [
            RISK_FLAG_LABELS.get(flag, flag) for flag in ai_risk_flags if flag
        ]
        if localized_flags:
            parts.append(f"- é£é™©: {'ã€'.join(localized_flags)}")

        parts.append("")

    # æ—¶é—´
    parts.append("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
    parts.append(timestamp.strftime("%Y-%m-%d %H:%M:%S"))
    if context_source and context_source != source_channel:
        parts[-1] += f" | æ¥æºé¢‘é“: {source_channel}"

    return "\n".join(parts)
