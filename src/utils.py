"""Utility helpers for logging, dedupe, and formatting."""

from __future__ import annotations

import hashlib
import logging
import os
import re
import sys
import unicodedata
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Set

import colorlog


class _MaxLevelFilter(logging.Filter):
    """Filter that only allows records up to a specific level."""

    def __init__(self, max_level: int) -> None:
        super().__init__()
        self._max_level = max_level

    def filter(self, record: logging.LogRecord) -> bool:  # pragma: no cover - logging helper
        return record.levelno <= self._max_level


def setup_logger(name: str, level: str = None) -> logging.Logger:
    """Configure a color logger that also writes to file."""
    # ä»ç¯å¢ƒå˜é‡è¯»å–æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ä¸º INFO
    if level is None:
        level = os.getenv("LOG_LEVEL", "INFO")

    logger = logging.getLogger(name)
    logger.setLevel(getattr(logging, level.upper(), logging.INFO))

    if logger.handlers:
        return logger

    color_formatter = colorlog.ColoredFormatter(
        "%(log_color)s%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        log_colors={
            "DEBUG": "cyan",
            "INFO": "green",
            "WARNING": "yellow",
            "ERROR": "red",
            "CRITICAL": "red,bg_white",
        },
    )

    console_handler = colorlog.StreamHandler(stream=sys.stdout)
    console_handler.setLevel(logging.DEBUG)
    console_handler.addFilter(_MaxLevelFilter(logging.INFO))
    console_handler.setFormatter(color_formatter)
    logger.addHandler(console_handler)

    stderr_handler = logging.StreamHandler(stream=sys.stderr)
    stderr_handler.setLevel(logging.WARNING)
    stderr_handler.setFormatter(
        logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    )
    logger.addHandler(stderr_handler)

    Path("./logs").mkdir(exist_ok=True)
    file_handler = logging.FileHandler("./logs/app.log", encoding="utf-8")
    file_handler.setFormatter(
        logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    )
    logger.addHandler(file_handler)

    return logger


class MessageDeduplicator:
    """Deduplicate messages by hash within a time window."""

    def __init__(self, window_hours: int = 24):
        self.seen_hashes: Dict[str, datetime] = {}
        self.window_hours = window_hours

    def is_duplicate(self, text: str) -> bool:
        """Return True if the message text appeared recently."""
        self._cleanup_expired()

        message_hash = hashlib.md5(text.encode("utf-8")).hexdigest()
        if message_hash in self.seen_hashes:
            return True

        self.seen_hashes[message_hash] = datetime.now()
        return False

    def _cleanup_expired(self) -> None:
        cutoff = datetime.now() - timedelta(hours=self.window_hours)
        expired = [key for key, timestamp in self.seen_hashes.items() if timestamp < cutoff]
        for key in expired:
            del self.seen_hashes[key]


def _normalize_text(text: str) -> str:
    if not text:
        return ""
    normalized = unicodedata.normalize("NFKC", text)
    return normalized.lower()


def contains_keywords(text: str, keywords: Set[str]) -> bool:
    """Check if text contains any keyword (case-insensitive, unicode-normalized)."""
    if not keywords:
        return True

    normalized_text = _normalize_text(text)
    return any(keyword in normalized_text for keyword in keywords)


HIGH_IMPACT_TERMS: Set[str] = {
    "è„±é”š",
    "depeg",
    "æš´è·Œ",
    "å¤§å¹…ä¸‹è·Œ",
    "æ¸…ç®—",
    "å¼ºåˆ¶å¹³ä»“",
    "å¼ºåˆ¶å¹³å€‰",
    "å¼ºåˆ¶æ¸…ç®—",
    "é—ªå´©",
    "flash crash",
    "è·Œè‡³",
    "è·Œç ´",
    "è·Œåˆ°",
    "ä½äº",
    "è·Œç©¿",
    "æº¢ä»·",
    "æŠ˜ä»·",
    "liquidation",
    "liquidations",
}

CRITICAL_ASSET_TOKENS: Set[str] = {
    "usde",
    "wbeth",
    "wbtc",
    "wbsol",
    "stablecoin",
    "ç¨³å®šå¸",
    "åŒ…è£¹",
    "wrapped beacon eth",
    "wrapped btc",
}

DROP_CONTEXT_TERMS: Set[str] = {
    "è·Œè‡³",
    "è·Œç ´",
    "è·Œåˆ°",
    "ä½äº",
    "è·Œç©¿",
    "è·Œè½",
    "è·Œå¹…",
    "è·Œå»äº†",
    "plunged to",
    "dropped to",
    "trading at",
    "crashed to",
}

PERCENT_CHANGE_PATTERN = re.compile(r"\d{1,3}(?:\.\d+)?\s*%")
PRICE_LEVEL_PATTERN = re.compile(
    r"(?:è·Œè‡³|è·Œç ´|è·Œåˆ°|ä½äº|è·Œç©¿|plunged to|dropped to|trading at)\s*[\d,]+(?:\.\d+)?"
)


def analyze_event_intensity(*texts: str) -> Dict[str, bool]:
    """Inspect free-form texts and return high-impact risk signals for downstream heuristics."""
    normalized_segments = [_normalize_text(text) for text in texts if text]
    if not normalized_segments:
        return {
            "has_high_impact": False,
            "mentions_critical_asset": False,
            "has_percent_change": False,
            "has_price_level_change": False,
            "has_drop_keyword": False,
        }

    combined = " ".join(segment for segment in normalized_segments if segment)
    has_high_impact = any(term in combined for term in HIGH_IMPACT_TERMS)
    mentions_critical_asset = any(token in combined for token in CRITICAL_ASSET_TOKENS)
    has_percent_change = bool(PERCENT_CHANGE_PATTERN.search(combined))
    has_price_level_change = bool(PRICE_LEVEL_PATTERN.search(combined))
    has_drop_keyword = any(term in combined for term in DROP_CONTEXT_TERMS)

    return {
        "has_high_impact": has_high_impact,
        "mentions_critical_asset": mentions_critical_asset,
        "has_percent_change": has_percent_change,
        "has_price_level_change": has_price_level_change,
        "has_drop_keyword": has_drop_keyword,
    }


def compute_sha256(text: str) -> str:
    """Return SHA256 hash for raw text."""
    if not text:
        return ""
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def compute_canonical_hash(text: str) -> str:
    """Return SHA256 hash after stripping whitespace and URLs."""
    if not text:
        return ""
    normalized = re.sub(r"https?://\S+", "", text, flags=re.IGNORECASE)
    normalized = re.sub(r"\s+", "", normalized)
    return hashlib.sha256(normalized.encode("utf-8")).hexdigest()


async def compute_embedding(text: str, api_key: str, model: str = "text-embedding-3-small") -> list[float] | None:
    """Generate OpenAI embedding vector for text.

    Args:
        text: Input text to embed (will be truncated to 8000 chars)
        api_key: OpenAI API key
        model: Embedding model name

    Returns:
        List of floats (1536 dimensions for text-embedding-3-small) or None on error
    """
    if not text or not text.strip():
        return None

    if not api_key:
        return None

    try:
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=api_key)

        # Truncate text to avoid token limits
        truncated_text = text[:8000]

        response = await client.embeddings.create(
            model=model,
            input=truncated_text
        )

        return response.data[0].embedding

    except ImportError:
        logger = setup_logger(__name__)
        logger.warning("OpenAI SDK not installed, skipping embedding generation")
        return None
    except Exception as exc:
        logger = setup_logger(__name__)
        logger.warning("Embedding generation failed: %s", exc)
        return None


ACTION_LABELS = {
    "buy": "ä¹°å…¥",
    "sell": "å–å‡º",
    "observe": "è§‚æœ›",
}

STRENGTH_LABELS = {
    "low": "ä½",
    "medium": "ä¸­",
    "high": "é«˜",
}

TIMEFRAME_LABELS = {
    "short": "çŸ­æœŸ",
    "medium": "ä¸­æœŸ",
    "long": "é•¿æœŸ",
}

DIRECTION_LABELS = {
    "long": "åšå¤š",
    "short": "åšç©º",
    "neutral": "ä¸­æ€§",
}

ALERT_LABELS = {
    "extreme_market_move": "æç«¯è¡Œæƒ…",
}

SEVERITY_LABELS = {
    "high": "é«˜",
    "medium": "ä¸­",
    "low": "ä½",
}

EVENT_TYPE_LABELS = {
    "listing": "ä¸Šçº¿/æŒ‚ç‰Œ",
    "delisting": "ä¸‹æ¶/é€€å¸‚",
    "hack": "å®‰å…¨/æ”»å‡»",
    "regulation": "ç›‘ç®¡/æ”¿ç­–",
    "funding": "èèµ„/å‹Ÿèµ„",
    "whale": "å·¨é²¸åŠ¨å‘",
    "liquidation": "æ¸…ç®—/çˆ†ä»“",
    "partnership": "åˆä½œ/é›†æˆ",
    "product_launch": "äº§å“å‘å¸ƒ/ä¸»ç½‘",
    "governance": "æ²»ç†ææ¡ˆ",
    "macro": "å®è§‚åŠ¨å‘",
    "celebrity": "åäººè¨€è®º",
    "airdrop": "ç©ºæŠ•æ¿€åŠ±",
    "scam_alert": "âš ï¸ é«˜é£é™©è­¦å‘Š",
    "other": "å…¶ä»–",
}

RISK_FLAG_LABELS = {
    "price_volatility": "ä»·æ ¼æ³¢åŠ¨",
    "liquidity_risk": "æµåŠ¨æ€§é£é™©",
    "regulation_risk": "åˆè§„é£é™©",
    "confidence_low": "ç½®ä¿¡åº¦ä½",
    "data_incomplete": "ä¿¡æ¯ä¸å®Œæ•´",
    "vague_timeline": "æ—¶é—´çº¿æ¨¡ç³Š",
    "speculative": "æŠ•æœºæ€§å†…å®¹",
    "unverifiable": "æ— æ³•éªŒè¯",
}


def format_forwarded_message(
    *,
    source_channel: str,
    timestamp: datetime,
    translated_text: str | None = None,
    original_text: str | None = None,
    show_original: bool = False,  # ä¿ç•™ç­¾åä½†ä¸å†å±•ç¤ºåŸæ–‡
    show_translation: bool = True,
    ai_summary: str | None = None,
    ai_action: str | None = None,
    ai_direction: str | None = None,
    ai_event_type: str | None = None,
    ai_asset: str | None = None,
    ai_asset_names: str | None = None,
    ai_confidence: float | None = None,
    ai_strength: str | None = None,
    ai_timeframe: str | None = None,
    ai_risk_flags: list[str] | None = None,
    ai_notes: str | None = None,
    context_source: str | None = None,
    ai_alert: str | None = None,
    ai_severity: str | None = None,
) -> str:
    """Compose a compact forwarding message emphasising actionable insights."""

    ai_risk_flags = ai_risk_flags or []
    ai_notes = (ai_notes or "").strip()
    ai_asset = (ai_asset or "").strip()
    ai_asset_names = (ai_asset_names or "").strip()
    ai_alert = (ai_alert or "").strip()
    ai_severity = (ai_severity or "").strip()
    translated_text = (translated_text or "").strip()
    original_text = (original_text or "").strip()

    if not show_translation:
        translated_text = ""

    parts: list[str] = ["ğŸ”” åŠ å¯†æ–°é—»ç›‘å¬\n"]

    # ä¿¡å·æ‘˜è¦ï¼šç¿»è¯‘æ–‡æœ¬ä¸ AI æ‘˜è¦åˆ†åˆ«åˆ—å‡ºï¼Œæ¸…æ™°ç´§å‡‘
    def _normalize_for_compare(text: str) -> str:
        stripped = re.sub(r"\s+", "", text)
        stripped = re.sub(r'[ï¼Œ,ã€‚\\.!ï¼Ÿ?ï¼š:ï¼›;"\'â€œâ€â€˜â€™`Â·â€¢\-]', "", stripped)
        return stripped.lower()

    if translated_text and original_text:
        if _normalize_for_compare(translated_text) == _normalize_for_compare(original_text):
            translated_text = ""

    summary_text = (ai_summary or "").strip()
    if not summary_text:
        summary_text = translated_text or original_text
    summary_text = (summary_text or "æš‚æ— æ‘˜è¦").replace("\n", " ").strip()

    parts: list[str] = []

    parts.append("âš¡ ä¿¡å·")
    if context_source:
        source_display = context_source
    else:
        source_display = source_channel
    parts.append(f" {source_display}ï¼š{summary_text}")
    if ai_notes:
        parts.append("")
        parts.append(f"å¤‡æ³¨: {ai_notes}")
        parts.append("")

    if ai_alert:
        alert_key = ai_alert.lower()
        alert_label = ALERT_LABELS.get(alert_key, ai_alert)
        severity_label = ""
        if ai_severity:
            severity_label = SEVERITY_LABELS.get(ai_severity.lower(), ai_severity)
        if severity_label:
            parts.append(f"ğŸš¨ {alert_label} | ç­‰çº§: {severity_label}")
        else:
            parts.append(f"ğŸš¨ {alert_label}")
        parts.append("")

    # æ“ä½œè¦ç‚¹ï¼Œä»…å½“æœ‰ AI ç»“æœæ—¶å±•ç¤º
    if ai_summary:
        action_key = (ai_action or "observe").lower()
        action_value = ACTION_LABELS.get(action_key, ai_action or "observe")
        confidence_text = (
            f"{ai_confidence:.2f}" if ai_confidence is not None else "æœªçŸ¥"
        )
        parts.append("ğŸ¯ æ“ä½œ")

        asset_line = ""
        if ai_asset or ai_asset_names:
            asset_line = ai_asset
            if ai_asset_names and ai_asset:
                asset_line = f"{ai_asset_names} ({ai_asset})"
            elif ai_asset_names:
                asset_line = ai_asset_names
            elif ai_asset:
                asset_line = ai_asset

        direction_key = (ai_direction or "").lower()
        direction_cn = DIRECTION_LABELS.get(direction_key, ai_direction) if ai_direction else None
        strength_key = (ai_strength or "").lower()
        strength_cn = STRENGTH_LABELS.get(strength_key, ai_strength) if ai_strength else None
        timeframe_key = (ai_timeframe or "").lower()
        timeframe_cn = TIMEFRAME_LABELS.get(timeframe_key, ai_timeframe) if ai_timeframe else None

        line_parts: list[str] = []
        if asset_line:
            line_parts.append(asset_line)

        if action_key == "observe":
            line_parts.append(f"çŠ¶æ€: {action_value}")
        else:
            line_parts.append(action_value)
            if direction_cn:
                line_parts.append(f"æ–¹å‘: {direction_cn}")

        line_parts.append(f"ç½®ä¿¡åº¦: {confidence_text}")
        if strength_cn:
            line_parts.append(f"å¼ºåº¦: {strength_cn}")
        if timeframe_cn:
            line_parts.append(f"å‘¨æœŸ: {timeframe_cn}")

        parts.append("- " + "ï¼Œ".join(line_parts))

        event_type_label = EVENT_TYPE_LABELS.get(ai_event_type or "", None)
        if event_type_label:
            parts.append(f"- äº‹ä»¶ç±»å‹: {event_type_label}")

        localized_flags = [
            RISK_FLAG_LABELS.get(flag, flag) for flag in ai_risk_flags if flag
        ]
        if localized_flags:
            parts.append(f"- é£é™©: {'ã€'.join(localized_flags)}")

        parts.append("")

    # æ—¶é—´
    parts.append("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
    parts.append(timestamp.strftime("%Y-%m-%d %H:%M:%S"))
    if context_source and context_source != source_channel:
        parts[-1] += f" | æ¥æºé¢‘é“: {source_channel}"

    return "\n".join(parts)
