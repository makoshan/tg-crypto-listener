"""Utility helpers for logging, dedupe, and formatting."""

from __future__ import annotations

import hashlib
import logging
import os
import re
import sys
import unicodedata
from collections import deque
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from difflib import SequenceMatcher
from pathlib import Path
from typing import Any, Callable, Deque, Dict, Optional, Set, Tuple

try:
    import colorlog
except ImportError:  # pragma: no cover - optional dependency
    colorlog = None  # type: ignore[assignment]


# åŒ—äº¬æ—¶åŒº UTC+8
BEIJING_TZ = timezone(timedelta(hours=8))


class BeijingTimeFormatter(logging.Formatter):
    """Formatter that uses Beijing time (UTC+8) instead of local time."""

    def converter(self, timestamp):
        """Convert timestamp to Beijing time."""
        dt = datetime.fromtimestamp(timestamp, tz=timezone.utc)
        return dt.astimezone(BEIJING_TZ).timetuple()

    def formatTime(self, record, datefmt=None):
        """Format time in Beijing timezone with milliseconds."""
        dt = datetime.fromtimestamp(record.created, tz=timezone.utc)
        beijing_time = dt.astimezone(BEIJING_TZ)
        if datefmt:
            s = beijing_time.strftime(datefmt)
        else:
            s = beijing_time.strftime("%Y-%m-%d %H:%M:%S")
        # Add milliseconds to match the format: 2025-10-24 10:10:24,047
        s = f"{s},{int(record.msecs):03d}"
        return s


class BeijingColoredFormatter(colorlog.ColoredFormatter if colorlog else logging.Formatter):
    """Colored formatter with Beijing time."""

    def formatTime(self, record, datefmt=None):
        """Format time in Beijing timezone."""
        dt = datetime.fromtimestamp(record.created, tz=timezone.utc)
        beijing_time = dt.astimezone(BEIJING_TZ)
        if datefmt:
            s = beijing_time.strftime(datefmt)
        else:
            s = beijing_time.strftime("%Y-%m-%d %H:%M:%S")
        # Add milliseconds
        s = f"{s},{int(record.msecs):03d}"
        return s


class _MaxLevelFilter(logging.Filter):
    """Filter that only allows records up to a specific level."""

    def __init__(self, max_level: int) -> None:
        super().__init__()
        self._max_level = max_level

    def filter(self, record: logging.LogRecord) -> bool:  # pragma: no cover - logging helper
        return record.levelno <= self._max_level


def setup_logger(name: str, level: str = None) -> logging.Logger:
    """Configure a color logger that also writes to file with Beijing time."""
    # ä»ç¯å¢ƒå˜é‡è¯»å–æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ä¸º INFO
    if level is None:
        level = os.getenv("LOG_LEVEL", "INFO")

    logger = logging.getLogger(name)
    logger.setLevel(getattr(logging, level.upper(), logging.INFO))

    if logger.handlers:
        return logger

    if colorlog is not None:
        color_formatter = BeijingColoredFormatter(
            "%(log_color)s%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
            log_colors={
                "DEBUG": "cyan",
                "INFO": "green",
                "WARNING": "yellow",
                "ERROR": "red",
                "CRITICAL": "red,bg_white",
            },
        )

        console_handler = colorlog.StreamHandler(stream=sys.stdout)
        console_handler.setLevel(logging.DEBUG)
        console_handler.addFilter(_MaxLevelFilter(logging.INFO))
        console_handler.setFormatter(color_formatter)
    else:
        plain_formatter = BeijingTimeFormatter(
            "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S"
        )
        console_handler = logging.StreamHandler(stream=sys.stdout)
        console_handler.setLevel(logging.DEBUG)
        console_handler.addFilter(_MaxLevelFilter(logging.INFO))
        console_handler.setFormatter(plain_formatter)
    logger.addHandler(console_handler)

    stderr_handler = logging.StreamHandler(stream=sys.stderr)
    stderr_handler.setLevel(logging.WARNING)
    stderr_formatter = BeijingTimeFormatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    stderr_handler.setFormatter(stderr_formatter)
    logger.addHandler(stderr_handler)

    Path("./logs").mkdir(exist_ok=True)
    file_handler = logging.FileHandler("./logs/app.log", encoding="utf-8")
    file_formatter = BeijingTimeFormatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)

    return logger


class MessageDeduplicator:
    """Deduplicate messages by hash within a time window."""

    def __init__(self, window_hours: int = 24, normalizer: Callable[[str], str] | None = None):
        self.seen_hashes: Dict[str, datetime] = {}
        self.window_hours = window_hours
        self._normalizer = normalizer

    def is_duplicate(self, text: str) -> bool:
        """Return True if the message text appeared recently."""
        self._cleanup_expired()

        processed_text = text
        if self._normalizer:
            processed_text = self._normalizer(text)
        message_hash = hashlib.md5(processed_text.encode("utf-8")).hexdigest()
        if message_hash in self.seen_hashes:
            return True

        self.seen_hashes[message_hash] = datetime.now()
        return False

    def _cleanup_expired(self) -> None:
        cutoff = datetime.now() - timedelta(hours=self.window_hours)
        expired = [key for key, timestamp in self.seen_hashes.items() if timestamp < cutoff]
        for key in expired:
            del self.seen_hashes[key]


@dataclass
class SignalDedupEntry:
    """Record of a recently forwarded AI signal."""

    normalized_summary: str
    char_set: Set[str]
    metadata: Tuple[str, str, str, str, str]
    timestamp: datetime


class SignalMessageDeduplicator:
    """Detect near-duplicate AI signals within a sliding time window."""

    def __init__(
        self,
        *,
        window_minutes: int = 240,
        similarity_threshold: float = 0.68,
        min_common_chars: int = 10,
    ) -> None:
        self.window = timedelta(minutes=max(window_minutes, 1))
        self.similarity_threshold = max(0.0, min(similarity_threshold, 1.0))
        self.min_common_chars = max(0, min_common_chars)
        self.entries: Deque[SignalDedupEntry] = deque()

    def is_duplicate(
        self,
        *,
        summary: str,
        action: str = "",
        direction: str = "",
        event_type: str = "",
        asset: str = "",
        asset_names: str = "",
    ) -> bool:
        """Return True if the signal is similar to a recent one."""
        normalized_summary = self._normalize_text(summary)
        if not normalized_summary:
            return False

        metadata = self._normalize_metadata(
            action=action,
            direction=direction,
            event_type=event_type,
            asset=asset,
            asset_names=asset_names,
        )
        char_set = set(normalized_summary)
        now = datetime.now()
        self._cleanup(now)

        for entry in self.entries:
            if entry.metadata != metadata:
                continue

            ratio = SequenceMatcher(None, normalized_summary, entry.normalized_summary).ratio()
            if ratio < self.similarity_threshold:
                continue

            common_chars = len(char_set & entry.char_set)
            if common_chars < self.min_common_chars:
                continue

            # Update timestamp to extend lifetime of matched entry
            entry.timestamp = now
            return True

        self.entries.append(
            SignalDedupEntry(
                normalized_summary=normalized_summary,
                char_set=char_set,
                metadata=metadata,
                timestamp=now,
            )
        )
        return False

    def _cleanup(self, now: datetime) -> None:
        cutoff = now - self.window
        while self.entries and self.entries[0].timestamp < cutoff:
            self.entries.popleft()

    @staticmethod
    def _normalize_text(text: str) -> str:
        normalized = unicodedata.normalize("NFKC", text or "")
        normalized = normalized.lower()
        normalized = re.sub(r"https?://\S+", "", normalized)
        normalized = re.sub(r"[0-9]+(?:\.[0-9]+)?", "", normalized)
        normalized = re.sub(r"[ï¼Œ,ã€‚.!ï¼Ÿ?ï¼š:ï¼›;\"'""''()ï¼ˆï¼‰\[\]{}<>ã€Šã€‹â€¢â€”\-Â·â€¦~`_]+", "", normalized)
        normalized = re.sub(r"\s+", "", normalized)
        return normalized

    @staticmethod
    def _normalize_metadata(
        *,
        action: str,
        direction: str,
        event_type: str,
        asset: str,
        asset_names: str,
    ) -> Tuple[str, str, str, str, str]:
        def _norm(value: str) -> str:
            normalized = unicodedata.normalize("NFKC", (value or "").strip())
            return normalized.lower()

        return (
            _norm(action),
            _norm(direction),
            _norm(event_type),
            _norm(asset),
            _norm(asset_names),
        )


def _normalize_text(text: str) -> str:
    if not text:
        return ""
    normalized = unicodedata.normalize("NFKC", text)
    return normalized.lower()


def contains_keywords(text: str, keywords: Set[str]) -> bool:
    """Check if text contains any keyword (case-insensitive, unicode-normalized)."""
    if not keywords:
        return True

    normalized_text = _normalize_text(text)
    return any(keyword in normalized_text for keyword in keywords)


HIGH_IMPACT_TERMS: Set[str] = {
    "è„±é”š",
    "depeg",
    "æš´è·Œ",
    "å¤§å¹…ä¸‹è·Œ",
    "æ¸…ç®—",
    "å¼ºåˆ¶å¹³ä»“",
    "å¼ºåˆ¶å¹³å€‰",
    "å¼ºåˆ¶æ¸…ç®—",
    "é—ªå´©",
    "flash crash",
    "è·Œè‡³",
    "è·Œç ´",
    "è·Œåˆ°",
    "ä½äº",
    "è·Œç©¿",
    "æº¢ä»·",
    "æŠ˜ä»·",
    "liquidation",
    "liquidations",
}

CRITICAL_ASSET_TOKENS: Set[str] = {
    "usde",
    "wbeth",
    "wbtc",
    "wbsol",
    "stablecoin",
    "ç¨³å®šå¸",
    "åŒ…è£¹",
    "wrapped beacon eth",
    "wrapped btc",
}

DROP_CONTEXT_TERMS: Set[str] = {
    "è·Œè‡³",
    "è·Œç ´",
    "è·Œåˆ°",
    "ä½äº",
    "è·Œç©¿",
    "è·Œè½",
    "è·Œå¹…",
    "è·Œå»äº†",
    "plunged to",
    "dropped to",
    "trading at",
    "crashed to",
}

PERCENT_CHANGE_PATTERN = re.compile(r"\d{1,3}(?:\.\d+)?\s*%")
PRICE_LEVEL_PATTERN = re.compile(
    r"(?:è·Œè‡³|è·Œç ´|è·Œåˆ°|ä½äº|è·Œç©¿|plunged to|dropped to|trading at)\s*[\d,]+(?:\.\d+)?"
)


def analyze_event_intensity(*texts: str) -> Dict[str, bool]:
    """Inspect free-form texts and return high-impact risk signals for downstream heuristics."""
    normalized_segments = [_normalize_text(text) for text in texts if text]
    if not normalized_segments:
        return {
            "has_high_impact": False,
            "mentions_critical_asset": False,
            "has_percent_change": False,
            "has_price_level_change": False,
            "has_drop_keyword": False,
        }

    combined = " ".join(segment for segment in normalized_segments if segment)
    has_high_impact = any(term in combined for term in HIGH_IMPACT_TERMS)
    mentions_critical_asset = any(token in combined for token in CRITICAL_ASSET_TOKENS)
    has_percent_change = bool(PERCENT_CHANGE_PATTERN.search(combined))
    has_price_level_change = bool(PRICE_LEVEL_PATTERN.search(combined))
    has_drop_keyword = any(term in combined for term in DROP_CONTEXT_TERMS)

    return {
        "has_high_impact": has_high_impact,
        "mentions_critical_asset": mentions_critical_asset,
        "has_percent_change": has_percent_change,
        "has_price_level_change": has_price_level_change,
        "has_drop_keyword": has_drop_keyword,
    }


def compute_sha256(text: str) -> str:
    """Return SHA256 hash for raw text."""
    if not text:
        return ""
    return hashlib.sha256(text.encode("utf-8")).hexdigest()


def compute_canonical_hash(text: str) -> str:
    """Return SHA256 hash after stripping whitespace and URLs."""
    if not text:
        return ""
    normalized = re.sub(r"https?://\S+", "", text, flags=re.IGNORECASE)
    normalized = re.sub(r"\s+", "", normalized)
    return hashlib.sha256(normalized.encode("utf-8")).hexdigest()


async def compute_embedding(text: str, api_key: str, model: str = "text-embedding-3-small") -> list[float] | None:
    """Generate OpenAI embedding vector for text.

    Args:
        text: Input text to embed (will be truncated to 8000 chars)
        api_key: OpenAI API key
        model: Embedding model name

    Returns:
        List of floats (1536 dimensions for text-embedding-3-small) or None on error
    """
    if not text or not text.strip():
        return None

    if not api_key:
        return None

    try:
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=api_key)

        # Truncate text to avoid token limits
        truncated_text = text[:8000]

        response = await client.embeddings.create(
            model=model,
            input=truncated_text
        )

        return response.data[0].embedding

    except ImportError:
        logger = setup_logger(__name__)
        logger.warning("OpenAI SDK not installed, skipping embedding generation")
        return None
    except Exception as exc:
        logger = setup_logger(__name__)
        logger.warning("Embedding generation failed: %s", exc)
        return None


ACTION_LABELS = {
    "buy": "ä¹°å…¥",
    "sell": "å–å‡º",
    "observe": "è§‚æœ›",
}

STRENGTH_LABELS = {
    "low": "ä½",
    "medium": "ä¸­",
    "high": "é«˜",
}

TIMEFRAME_LABELS = {
    "short": "çŸ­æœŸ",
    "medium": "ä¸­æœŸ",
    "long": "é•¿æœŸ",
}

DIRECTION_LABELS = {
    "long": "åšå¤š",
    "short": "åšç©º",
    "neutral": "ä¸­æ€§",
}

ALERT_LABELS = {
    "extreme_market_move": "æç«¯è¡Œæƒ…",
}

SEVERITY_LABELS = {
    "high": "é«˜",
    "medium": "ä¸­",
    "low": "ä½",
}

EVENT_TYPE_LABELS = {
    "listing": "ä¸Šçº¿/æŒ‚ç‰Œ",
    "delisting": "ä¸‹æ¶/é€€å¸‚",
    "hack": "å®‰å…¨/æ”»å‡»",
    "regulation": "ç›‘ç®¡/æ”¿ç­–",
    "funding": "èèµ„/å‹Ÿèµ„",
    "whale": "å·¨é²¸åŠ¨å‘",
    "liquidation": "æ¸…ç®—/çˆ†ä»“",
    "partnership": "åˆä½œ/é›†æˆ",
    "product_launch": "äº§å“å‘å¸ƒ/ä¸»ç½‘",
    "governance": "æ²»ç†ææ¡ˆ",
    "macro": "å®è§‚åŠ¨å‘",
    "celebrity": "åäººè¨€è®º",
    "airdrop": "ç©ºæŠ•æ¿€åŠ±",
    "scam_alert": "âš ï¸ é«˜é£é™©è­¦å‘Š",
    "other": "å…¶ä»–",
}

RISK_FLAG_LABELS = {
    "price_volatility": "ä»·æ ¼æ³¢åŠ¨",
    "liquidity_risk": "æµåŠ¨æ€§é£é™©",
    "regulation_risk": "åˆè§„é£é™©",
    "confidence_low": "ç½®ä¿¡åº¦ä½",
    "data_incomplete": "ä¿¡æ¯ä¸å®Œæ•´",
    "vague_timeline": "æ—¶é—´çº¿æ¨¡ç³Š",
    "speculative": "æŠ•æœºæ€§å†…å®¹",
    "unverifiable": "æ— æ³•éªŒè¯",
}


def format_forwarded_message(
    *,
    source_channel: str,
    timestamp: datetime,
    translated_text: str | None = None,
    original_text: str | None = None,
    show_original: bool = False,  # ä¿ç•™ç­¾åä½†ä¸å†å±•ç¤ºåŸæ–‡
    show_translation: bool = True,
    ai_summary: str | None = None,
    ai_action: str | None = None,
    ai_direction: str | None = None,
    ai_event_type: str | None = None,
    ai_asset: str | None = None,
    ai_asset_names: str | None = None,
    ai_confidence: float | None = None,
    ai_strength: str | None = None,
    ai_timeframe: str | None = None,
    ai_risk_flags: list[str] | None = None,
    ai_notes: str | None = None,
    context_source: str | None = None,
    ai_alert: str | None = None,
    ai_severity: str | None = None,
    price_snapshot: Dict[str, Any] | None = None,
) -> str:
    """Compose a compact forwarding message emphasising actionable insights."""

    ai_risk_flags = ai_risk_flags or []
    ai_notes = (ai_notes or "").strip()
    ai_asset = (ai_asset or "").strip()
    ai_asset_names = (ai_asset_names or "").strip()
    ai_alert = (ai_alert or "").strip()
    ai_severity = (ai_severity or "").strip()
    translated_text = (translated_text or "").strip()
    original_text = (original_text or "").strip()

    if not show_translation:
        translated_text = ""

    parts: list[str] = ["ğŸ”” åŠ å¯†æ–°é—»ç›‘å¬\n"]

    # ä¿¡å·æ‘˜è¦ï¼šç¿»è¯‘æ–‡æœ¬ä¸ AI æ‘˜è¦åˆ†åˆ«åˆ—å‡ºï¼Œæ¸…æ™°ç´§å‡‘
    def _normalize_for_compare(text: str) -> str:
        stripped = re.sub(r"\s+", "", text)
        stripped = re.sub(r'[ï¼Œ,ã€‚\\.!ï¼Ÿ?ï¼š:ï¼›;"\'â€œâ€â€˜â€™`Â·â€¢\-]', "", stripped)
        return stripped.lower()

    if translated_text and original_text:
        if _normalize_for_compare(translated_text) == _normalize_for_compare(original_text):
            translated_text = ""

    summary_text = (ai_summary or "").strip()
    if not summary_text:
        summary_text = translated_text or original_text
    summary_text = (summary_text or "æš‚æ— æ‘˜è¦").replace("\n", " ").strip()

    parts: list[str] = []

    parts.append("âš¡ ä¿¡å·")
    if context_source:
        source_display = context_source
    else:
        source_display = source_channel
    parts.append(f" {source_display}ï¼š{summary_text}")
    if ai_notes:
        parts.append("")
        parts.append(f"å¤‡æ³¨: {ai_notes}")
        parts.append("")

    if ai_alert:
        alert_key = ai_alert.lower()
        alert_label = ALERT_LABELS.get(alert_key, ai_alert)
        severity_label = ""
        if ai_severity:
            severity_label = SEVERITY_LABELS.get(ai_severity.lower(), ai_severity)
        if severity_label:
            parts.append(f"ğŸš¨ {alert_label} | ç­‰çº§: {severity_label}")
        else:
            parts.append(f"ğŸš¨ {alert_label}")
        parts.append("")

    # æ“ä½œè¦ç‚¹ï¼Œä»…å½“æœ‰ AI ç»“æœæ—¶å±•ç¤º
    if ai_summary:
        action_key = (ai_action or "observe").lower()
        action_value = ACTION_LABELS.get(action_key, ai_action or "observe")
        confidence_text = (
            f"{ai_confidence:.2f}" if ai_confidence is not None else "æœªçŸ¥"
        )
        parts.append("ğŸ¯ æ“ä½œ")

        asset_line = ""
        if ai_asset or ai_asset_names:
            asset_line = ai_asset
            if ai_asset_names and ai_asset:
                asset_line = f"{ai_asset_names} ({ai_asset})"
            elif ai_asset_names:
                asset_line = ai_asset_names
            elif ai_asset:
                asset_line = ai_asset

        direction_key = (ai_direction or "").lower()
        direction_cn = DIRECTION_LABELS.get(direction_key, ai_direction) if ai_direction else None
        strength_key = (ai_strength or "").lower()
        strength_cn = STRENGTH_LABELS.get(strength_key, ai_strength) if ai_strength else None
        timeframe_key = (ai_timeframe or "").lower()
        timeframe_cn = TIMEFRAME_LABELS.get(timeframe_key, ai_timeframe) if ai_timeframe else None

        line_parts: list[str] = []
        if asset_line:
            line_parts.append(asset_line)

        if action_key == "observe":
            line_parts.append(f"çŠ¶æ€: {action_value}")
        else:
            line_parts.append(action_value)
            if direction_cn:
                line_parts.append(f"æ–¹å‘: {direction_cn}")

        line_parts.append(f"ç½®ä¿¡åº¦: {confidence_text}")
        if strength_cn:
            line_parts.append(f"å¼ºåº¦: {strength_cn}")
        if timeframe_cn:
            line_parts.append(f"å‘¨æœŸ: {timeframe_cn}")

        parts.append("- " + "ï¼Œ".join(line_parts))

        event_type_label = EVENT_TYPE_LABELS.get(ai_event_type or "", None)
        if event_type_label:
            parts.append(f"- äº‹ä»¶ç±»å‹: {event_type_label}")

        # Display price information if available
        if price_snapshot:
            if price_snapshot.get("multiple"):
                # Multiple assets - display all prices
                snapshots = price_snapshot.get("snapshots", [])
                for snap in snapshots:
                    asset = snap.get("asset", "")
                    data = snap.get("data", {})
                    metrics = data.get("metrics", {})
                    price_usd = metrics.get("price_usd")
                    price_change_24h_pct = metrics.get("price_change_24h_pct")

                    if price_usd is not None:
                        price_parts = [f"{asset}: ${price_usd:,.4f}"]
                        if price_change_24h_pct is not None:
                            change_sign = "+" if price_change_24h_pct >= 0 else ""
                            price_parts.append(f"24h {change_sign}{price_change_24h_pct:.2f}%")
                        parts.append(f"- å½“å‰ä»·æ ¼ ({asset}): {' '.join(price_parts)}")
            else:
                # Single asset - keep existing behavior
                metrics = price_snapshot.get("metrics", {})
                price_usd = metrics.get("price_usd")
                price_change_24h_pct = metrics.get("price_change_24h_pct")

                if price_usd is not None:
                    price_parts = [f"${price_usd:,.4f}"]
                    if price_change_24h_pct is not None:
                        change_sign = "+" if price_change_24h_pct >= 0 else ""
                        price_parts.append(f"24h {change_sign}{price_change_24h_pct:.2f}%")
                    parts.append(f"- å½“å‰ä»·æ ¼: {' '.join(price_parts)}")

        localized_flags = [
            RISK_FLAG_LABELS.get(flag, flag) for flag in ai_risk_flags if flag
        ]
        if localized_flags:
            parts.append(f"- é£é™©: {'ã€'.join(localized_flags)}")

        parts.append("")

    # æ—¶é—´
    parts.append("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”")
    parts.append(timestamp.strftime("%Y-%m-%d %H:%M:%S"))
    if context_source and context_source != source_channel:
        parts[-1] += f" | æ¥æºé¢‘é“: {source_channel}"

    return "\n".join(parts)
