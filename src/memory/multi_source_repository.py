"""Memory repository wrapper that merges multiple Supabase sources."""

from __future__ import annotations

import hashlib
import json
from datetime import datetime, timezone
from typing import Any, Iterable, Sequence

from src.db.supabase_client import SupabaseClient, SupabaseError
from src.utils import setup_logger

from .repository import MemoryRepositoryConfig, SupabaseMemoryRepository
from .types import MemoryContext, MemoryEntry


class MultiSourceMemoryRepository:
    """Aggregate memories from primary (news events) and secondary (docs) sources."""

    def __init__(
        self,
        primary: SupabaseMemoryRepository,
        *,
        secondary_client: SupabaseClient | None,
        secondary_table: str,
        config: MemoryRepositoryConfig,
        secondary_similarity_threshold: float,
        secondary_max_results: int,
    ) -> None:
        self._primary = primary
        self._secondary_client = secondary_client
        self._secondary_table = secondary_table or "docs"
        self._config = config
        self._secondary_similarity_threshold = float(secondary_similarity_threshold)
        self._secondary_max_results = int(secondary_max_results)
        self._logger = setup_logger(__name__)

    async def fetch_memories(
        self,
        *,
        embedding: Sequence[float] | None,
        asset_codes: Iterable[str] | None = None,
    ) -> MemoryContext:
        """Fetch memories from multiple sources and merge them."""

        self._logger.info(
            "ðŸ”„ MultiSource å¼€å§‹æ£€ç´¢: ä¸»åº“ + å‰¯åº“ (embeddingç»´åº¦=%d, asset_codes=%s)",
            len(embedding) if embedding else 0,
            list(asset_codes) if asset_codes else []
        )

        primary_context = await self._primary.fetch_memories(
            embedding=embedding,
            asset_codes=asset_codes,
        )

        entries: list[MemoryEntry] = list(primary_context.entries)
        self._logger.info("ðŸ“Š ä¸»åº“è¿”å›ž %d æ¡è®°å¿†", len(entries))

        if self._secondary_client and embedding:
            self._logger.info("ðŸ” å‡†å¤‡è°ƒç”¨å‰¯åº“æ£€ç´¢ (embeddingç»´åº¦=%d)", len(embedding))
            try:
                secondary_entries = await self._fetch_from_secondary(
                    embedding=embedding,
                    asset_codes=asset_codes,
                )
            except SupabaseError as exc:
                self._logger.warning("âŒ å‰¯åº“è®°å¿†æ£€ç´¢å¤±è´¥ (SupabaseError): %s", exc)
            except Exception as exc:  # pragma: no cover - defensive logging
                self._logger.warning("âŒ å‰¯åº“è®°å¿†æ£€ç´¢å‡ºçŽ°å¼‚å¸¸: %s", exc, exc_info=True)
            else:
                self._logger.info("âœ… å‰¯åº“è¿”å›ž %d æ¡è®°å¿†ï¼Œåˆå¹¶ä¸­...", len(secondary_entries))
                entries.extend(secondary_entries)
        elif not self._secondary_client:
            self._logger.debug("âš ï¸  å‰¯åº“å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡å‰¯åº“æ£€ç´¢")
        elif not embedding:
            self._logger.debug("âš ï¸  æ—  embeddingï¼Œè·³è¿‡å‰¯åº“æ£€ç´¢")

        merged = self._merge_and_rank(entries)
        self._logger.info(
            "ðŸŽ¯ MultiSource åˆå¹¶å®Œæˆ: ä¸»åº“(%d) + å‰¯åº“ â†’ åŽ»é‡æŽ’åºåŽ(%d) â†’ æˆªå–å‰%dæ¡",
            len(primary_context.entries),
            len(merged),
            self._config.max_notes
        )

        context = MemoryContext()
        context.extend(merged[: self._config.max_notes])
        return context

    async def _fetch_from_secondary(
        self,
        *,
        embedding: Sequence[float],
        asset_codes: Iterable[str] | None,
    ) -> list[MemoryEntry]:
        """Retrieve documents from the secondary Supabase source."""

        params: dict[str, Any] = {
            "query_embedding": list(embedding),
            "match_threshold": float(self._secondary_similarity_threshold),
            "match_count": int(self._secondary_max_results),
        }

        self._logger.info(
            "ðŸ”Ž è°ƒç”¨å‰¯åº“ RPC match_documents: é˜ˆå€¼=%.3f, æœ€å¤§è¿”å›ž=%d, ç»´åº¦=%d, table=%s",
            params["match_threshold"],
            params["match_count"],
            len(params["query_embedding"]),
            self._secondary_table,
        )

        response = await self._secondary_client.rpc("match_documents", params)

        self._logger.info(
            "ðŸ“¥ å‰¯åº“ RPC è¿”å›ž: type=%s, åŽŸå§‹ç»“æžœæ•°=%d",
            type(response).__name__,
            len(response) if isinstance(response, list) else 0
        )

        if not isinstance(response, list):
            self._logger.warning("âš ï¸  å‰¯åº“ RPC è¿”å›žéžåˆ—è¡¨ç±»åž‹: %s", type(response).__name__)
            return []

        if not response:
            self._logger.info("â„¹ï¸  å‰¯åº“è¿”å›žç©ºåˆ—è¡¨ï¼ˆå¯èƒ½æ— åŒ¹é…æ–‡æ¡£ï¼‰")
            return []

        target_assets = {
            str(code).strip().upper()
            for code in (asset_codes or [])
            if str(code).strip()
        }

        entries: list[MemoryEntry] = []
        filtered_count = 0
        invalid_count = 0

        for idx, row in enumerate(response):
            if not isinstance(row, dict):
                invalid_count += 1
                continue

            entry = self._build_entry_from_secondary(row)
            if entry is None:
                invalid_count += 1
                self._logger.debug("å‰¯åº“ç¬¬ %d è¡Œ: æž„å»º entry å¤±è´¥ï¼Œè·³è¿‡", idx)
                continue

            if target_assets and entry.assets:
                if not target_assets.intersection(entry.assets):
                    filtered_count += 1
                    self._logger.debug(
                        "å‰¯åº“è®°å¿†è¿‡æ»¤ [%d]: æœªå‘½ä¸­èµ„äº§è¿‡æ»¤ %s -> %s",
                        idx,
                        target_assets,
                        entry.assets,
                    )
                    continue

            entries.append(entry)
            self._logger.debug(
                "âœ… å‰¯åº“ç¬¬ %d è¡Œ: æ·»åŠ è®°å¿† id=%s, similarity=%.3f, assets=%s",
                idx, entry.id, entry.similarity, entry.assets
            )

        self._logger.info(
            "ðŸ“Š å‰¯åº“å¤„ç†å®Œæˆ: åŽŸå§‹%dæ¡ â†’ æœ‰æ•ˆ%dæ¡ (è¿‡æ»¤%d, æ— æ•ˆ%d)",
            len(response), len(entries), filtered_count, invalid_count
        )
        return entries

    def _build_entry_from_secondary(self, row: dict[str, Any]) -> MemoryEntry | None:
        """Convert secondary RPC row into MemoryEntry."""

        summary = self._format_summary(row)
        if not summary:
            return None

        created_at = self._parse_timestamp(
            row.get("published_at") or row.get("created_at")
        )

        assets = self._extract_assets(row)
        similarity = float(row.get("similarity", 0.0))

        prefix = self._secondary_table or "docs"
        entry = MemoryEntry(
            id=f"{prefix}:{row.get('id', '')}",
            created_at=created_at,
            assets=assets,
            action="inform",
            confidence=similarity,
            summary=summary,
            similarity=similarity,
        )
        return entry

    def _parse_timestamp(self, value: Any) -> datetime:
        if isinstance(value, str):
            try:
                return datetime.fromisoformat(value.replace("Z", "+00:00")).astimezone(timezone.utc)
            except ValueError:
                self._logger.debug("å‰¯åº“æ—¶é—´è§£æžå¤±è´¥: %s", value)
        return datetime.now(tz=timezone.utc)

    def _extract_assets(self, row: dict[str, Any]) -> list[str]:
        tags = row.get("tags")

        if isinstance(tags, str):
            try:
                tags = json.loads(tags)
            except json.JSONDecodeError:
                self._logger.debug("å‰¯åº“ tags å­—æ®µè§£æžå¤±è´¥ï¼ˆéž JSONï¼‰: %s", tags)
                tags = None

        assets: set[str] = set()
        if isinstance(tags, dict):
            for key in ("entities", "tickers", "assets"):
                values = tags.get(key)
                if isinstance(values, (list, tuple, set)):
                    for token in values:
                        normalized = str(token).strip().upper()
                        if normalized:
                            assets.add(normalized)

        return sorted(assets)

    def _format_summary(self, row: dict[str, Any]) -> str:
        """Build a concise summary string for secondary entries."""

        summary = str(
            row.get("ai_summary_cn")
            or row.get("summary")
            or row.get("content_text")
            or ""
        ).strip()

        if not summary:
            return ""

        max_length = 360
        if len(summary) > max_length:
            summary = summary[: max_length - 3].rstrip() + "..."

        metadata_bits: list[str] = []
        source = row.get("source")
        if source:
            metadata_bits.append(f"æ¥æº: {source}")

        author = row.get("source_author")
        if author:
            metadata_bits.append(f"ä½œè€…: {author}")

        url = row.get("canonical_url")
        if url:
            metadata_bits.append(f"é“¾æŽ¥: {url}")

        if metadata_bits:
            summary = f"{summary}ï¼ˆ{', '.join(metadata_bits)}ï¼‰"

        return summary

    def _merge_and_rank(self, entries: list[MemoryEntry]) -> list[MemoryEntry]:
        self._logger.debug("ðŸ”€ å¼€å§‹åˆå¹¶æŽ’åº: è¾“å…¥ %d æ¡è®°å¿†", len(entries))

        deduped = self._deduplicate(entries)
        self._logger.debug("ðŸ”€ åŽ»é‡åŽ: %d æ¡è®°å¿†", len(deduped))

        secondary_prefix = f"{self._secondary_table}:"
        primary_count = sum(1 for e in deduped if not e.id.startswith(secondary_prefix))
        secondary_count = len(deduped) - primary_count

        self._logger.debug(
            "ðŸ”€ æ¥æºåˆ†å¸ƒ: ä¸»åº“ %d æ¡, å‰¯åº“ %d æ¡",
            primary_count, secondary_count
        )

        sorted_entries = sorted(
            deduped,
            key=lambda item: (
                round(item.similarity, 6),
                item.created_at.timestamp(),
                0 if item.id and not item.id.startswith(secondary_prefix) else 1,
            ),
            reverse=True,
        )

        if self._logger.isEnabledFor(10):  # DEBUG level
            self._logger.debug("ðŸ”€ æŽ’åºåŽå‰5æ¡:")
            for i, entry in enumerate(sorted_entries[:5], 1):
                source = "ä¸»åº“" if not entry.id.startswith(secondary_prefix) else "å‰¯åº“"
                self._logger.debug(
                    "  [%d] %s sim=%.3f %s %s",
                    i, source, entry.similarity, entry.assets, entry.id[:20]
                )

        return sorted_entries

    def _deduplicate(self, entries: list[MemoryEntry]) -> list[MemoryEntry]:
        unique: dict[str, MemoryEntry] = {}
        seen_summaries: set[str] = set()

        for entry in entries:
            if not entry.summary:
                continue

            summary_hash = hashlib.sha256(entry.summary.encode("utf-8")).hexdigest()
            if entry.id in unique or summary_hash in seen_summaries:
                continue
            unique[entry.id] = entry
            seen_summaries.add(summary_hash)

        return list(unique.values())
