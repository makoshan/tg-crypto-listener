"""
Local Memory Store - æœ¬åœ°è®°å¿†å¿«é€Ÿè¯»å–ï¼ˆä¾› Gemini ä½¿ç”¨ï¼‰

æ”¯æŒ:
- å…³é”®è¯åŒ¹é…æ£€ç´¢
- æ—¶é—´çª—å£è¿‡æ»¤
- è¿”å›ä¸ SupabaseMemoryRepository ä¸€è‡´çš„ç»“æ„
"""

import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List
from uuid import uuid4

from src.memory.types import MemoryEntry, MemoryContext
from src.utils import setup_logger

logger = setup_logger(__name__)


class LocalMemoryStore:
    """
    æœ¬åœ°è®°å¿†å­˜å‚¨ï¼ˆJSON æ–‡ä»¶ï¼‰

    ç›®å½•ç»“æ„:
    memories/
      patterns/
        listing.json      # ä¸Šå¸æ¶ˆæ¯æ¨¡å¼
        hack.json         # é»‘å®¢äº‹ä»¶æ¨¡å¼
        regulation.json   # ç›‘ç®¡æ¶ˆæ¯æ¨¡å¼
        core.json         # é€šç”¨æ¨¡å¼
    """

    def __init__(self, base_path: str = "./memories", lookback_hours: int = 168):
        """
        åˆå§‹åŒ–æœ¬åœ°è®°å¿†å­˜å‚¨

        Args:
            base_path: è®°å¿†å­˜å‚¨æ ¹ç›®å½•
            lookback_hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤ 168h = 7å¤©
        """
        self.base_path = Path(base_path)
        self.lookback_hours = lookback_hours
        self.pattern_dir = self.base_path / "patterns"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.pattern_dir.mkdir(parents=True, exist_ok=True)

        logger.info(
            f"LocalMemoryStore åˆå§‹åŒ–: {self.base_path} "
            f"(æ—¶é—´çª—å£: {lookback_hours}h)"
        )

    def load_entries(
        self,
        keywords: List[str],
        limit: int = 3,
        min_confidence: float = 0.6
    ) -> List[MemoryEntry]:
        """
        åŠ è½½è®°å¿†æ¡ç›®ï¼ˆè¿”å›ä¸ SupabaseMemoryRepository.fetch_memories ä¸€è‡´çš„ç»“æ„ï¼‰

        Args:
            keywords: å…³é”®è¯åˆ—è¡¨ï¼ˆç”¨äºåŒ¹é…æ–‡ä»¶åï¼‰
            limit: æœ€å¤§è¿”å›æ•°é‡
            min_confidence: æœ€å°ç½®ä¿¡åº¦é˜ˆå€¼

        Returns:
            MemoryEntry åˆ—è¡¨ï¼ˆæŒ‰ç›¸ä¼¼åº¦é™åºï¼‰
        """
        if not keywords:
            return []

        patterns: List[Dict] = []

        # åŠ è½½å…³é”®è¯å¯¹åº”çš„æ¨¡å¼æ–‡ä»¶
        loaded_files = []
        for keyword in keywords:
            file_path = self.pattern_dir / f"{keyword.lower()}.json"
            keyword_patterns = self._load_pattern_file(file_path)
            if keyword_patterns:
                loaded_files.append(f"{keyword.lower()}.json({len(keyword_patterns)})")
                patterns.extend(keyword_patterns)

        # åŠ è½½é€šç”¨æ¨¡å¼
        common_path = self.pattern_dir / "core.json"
        core_patterns = self._load_pattern_file(common_path)
        if core_patterns:
            loaded_files.append(f"core.json({len(core_patterns)})")
            patterns.extend(core_patterns)

        logger.debug(f"ğŸ“‚ åŠ è½½æ¨¡å¼æ–‡ä»¶: {', '.join(loaded_files) if loaded_files else 'æ— '}")

        if not patterns:
            logger.info("æœªæ£€ç´¢åˆ°ç›¸ä¼¼å†å²è®°å¿†")
            return []

        # æ ‡å‡†åŒ–ä¸º MemoryEntry
        entries = self._normalize_patterns(patterns)

        # æ—¶é—´çª—å£è¿‡æ»¤ï¼ˆä½¿ç”¨ UTC aware datetimeï¼‰
        from datetime import timezone
        cutoff_time = datetime.now(timezone.utc) - timedelta(hours=self.lookback_hours)

        # ç¡®ä¿ created_at æ˜¯ aware datetime
        aware_entries = []
        for e in entries:
            if e.created_at.tzinfo is None:
                # å¦‚æœæ˜¯ naiveï¼Œå‡è®¾ä¸º UTC
                aware_created_at = e.created_at.replace(tzinfo=timezone.utc)
                e.created_at = aware_created_at
            aware_entries.append(e)

        entries = [e for e in aware_entries if e.created_at >= cutoff_time]

        # ç½®ä¿¡åº¦è¿‡æ»¤
        entries = [e for e in entries if e.confidence >= min_confidence]

        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        entries.sort(key=lambda x: x.similarity, reverse=True)

        # é™åˆ¶æ•°é‡
        entries = entries[:limit]

        logger.info(f"æ£€ç´¢åˆ° {len(entries)} æ¡å†å²è®°å¿†")

        # è¯¦ç»†è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºæ£€ç´¢åˆ°çš„è®°å¿†å†…å®¹
        if entries:
            logger.debug("ğŸ“š Local Memory æ£€ç´¢è¯¦æƒ…:")
            for i, entry in enumerate(entries, 1):
                logger.debug(
                    f"  [{i}] ID={entry.id[:8]}... asset={entry.assets} "
                    f"action={entry.action} confidence={entry.confidence:.2f} "
                    f"similarity={entry.similarity:.2f}"
                )
                logger.debug(f"      æ‘˜è¦: {entry.summary[:80]}..." if len(entry.summary) > 80 else f"      æ‘˜è¦: {entry.summary}")

        return entries

    def _load_pattern_file(self, file_path: Path) -> List[Dict]:
        """
        åŠ è½½å•ä¸ªæ¨¡å¼æ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            æ¨¡å¼åˆ—è¡¨
        """
        if not file_path.exists():
            return []

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            patterns = data.get("patterns", [])
            logger.debug(f"åŠ è½½ {len(patterns)} æ¡æ¨¡å¼ä» {file_path.name}")
            return patterns
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å¼æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return []

    def _normalize_patterns(self, patterns: List[Dict]) -> List[MemoryEntry]:
        """
        å°† JSON æ¨¡å¼æ ‡å‡†åŒ–ä¸º MemoryEntry

        Args:
            patterns: åŸå§‹æ¨¡å¼åˆ—è¡¨

        Returns:
            MemoryEntry åˆ—è¡¨
        """
        entries: List[MemoryEntry] = []

        for item in patterns:
            try:
                # è§£ææ—¶é—´æˆ³ï¼ˆç¡®ä¿ä¸º UTC aware datetimeï¼‰
                from datetime import timezone

                timestamp_str = item.get("timestamp") or item.get("created_at")
                if timestamp_str:
                    created_at = datetime.fromisoformat(
                        str(timestamp_str).replace("Z", "+00:00")
                    )
                else:
                    created_at = datetime.now(timezone.utc)

                # è§£æèµ„äº§åˆ—è¡¨
                assets = item.get("assets") or item.get("asset") or []
                if isinstance(assets, str):
                    assets_list = [
                        part.strip()
                        for part in assets.split(",")
                        if part.strip()
                    ]
                else:
                    assets_list = [str(part).strip() for part in assets if str(part).strip()]

                if not assets_list:
                    assets_list = ["NONE"]

                # æ„é€  MemoryEntry
                entry = MemoryEntry(
                    id=item.get("id") or str(uuid4()),
                    created_at=created_at,
                    assets=assets_list,
                    action=item.get("action", "observe"),
                    confidence=float(item.get("confidence", 0.0)),
                    similarity=float(item.get("similarity", 1.0)),  # Local æ¨¡å¼æ— çœŸå®ç›¸ä¼¼åº¦ï¼Œé»˜è®¤ 1.0
                    summary=item.get("summary") or item.get("notes", ""),
                )

                entries.append(entry)
            except Exception as e:
                logger.error(f"æ ‡å‡†åŒ–æ¨¡å¼å¤±è´¥: {e}, æ•°æ®: {item}")
                continue

        return entries

    def save_pattern(self, category: str, pattern: Dict) -> None:
        """
        ä¿å­˜æ¨¡å¼ï¼ˆå¯é€‰ï¼Œä»…ç”¨äºå®šæœŸå½’çº³ä»»åŠ¡ï¼‰

        Args:
            category: æ¨¡å¼åˆ†ç±»ï¼ˆå¦‚ listing, hack, regulationï¼‰
            pattern: æ¨¡å¼æ•°æ®
        """
        file_path = self.pattern_dir / f"{category.lower()}.json"

        # åŠ è½½ç°æœ‰æ¨¡å¼
        existing = []
        if file_path.exists():
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    existing = json.load(f).get("patterns", [])
            except Exception as e:
                logger.error(f"åŠ è½½ç°æœ‰æ¨¡å¼å¤±è´¥ {file_path}: {e}")

        # è¿½åŠ æ–°æ¨¡å¼
        existing.append(pattern)

        # å»é‡ï¼ˆåŸºäº summaryï¼‰
        unique = {p.get("summary", str(uuid4())): p for p in existing}.values()

        # é™åˆ¶æ•°é‡ï¼ˆä¿ç•™æœ€è¿‘ 50 æ¡ï¼‰
        limited = sorted(
            unique,
            key=lambda x: x.get("timestamp", ""),
            reverse=True
        )[:50]

        # ä¿å­˜
        try:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(
                    {"patterns": list(limited)},
                    f,
                    ensure_ascii=False,
                    indent=2
                )

            logger.info(f"æ¨¡å¼å·²ä¿å­˜: {file_path.name} ({len(limited)} æ¡)")
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å¼å¤±è´¥ {file_path}: {e}")

    def get_stats(self) -> Dict[str, any]:
        """
        è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡æ•°æ®ï¼ˆæ–‡ä»¶æ•°ã€æ€»æ¨¡å¼æ•°ã€æœ€è€è®°å½•æ—¶é—´ç­‰ï¼‰
        """
        if not self.pattern_dir.exists():
            return {
                "total_files": 0,
                "total_patterns": 0,
                "oldest_record": None,
            }

        files = list(self.pattern_dir.glob("*.json"))
        total_patterns = 0
        oldest_time = datetime.utcnow()

        for file_path in files:
            patterns = self._load_pattern_file(file_path)
            total_patterns += len(patterns)

            for p in patterns:
                timestamp_str = p.get("timestamp") or p.get("created_at")
                if timestamp_str:
                    try:
                        ts = datetime.fromisoformat(str(timestamp_str).replace("Z", "+00:00"))
                        if ts < oldest_time:
                            oldest_time = ts
                    except Exception:
                        pass

        return {
            "total_files": len(files),
            "total_patterns": total_patterns,
            "oldest_record": oldest_time.isoformat() if total_patterns > 0 else None,
        }
